{
  "extraction_info": {
    "date": "2025-10-01T15:16:34.160778",
    "total_toolsets": 1,
    "total_tools": 96,
    "toolbox": "image-analyst"
  },
  "tools": [
    {
      "toolset": "image-analyst",
      "tool_name": "Multidimensional Analysis",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/an-overview-of-the-multidimensional-analysis-toolset-in-image-analyst.htm",
      "parameters": [],
      "summary": "",
      "extraction_date": "2025-10-01T15:11:44.000932"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Analyze Changes Using CCDC",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/analyze-changes-using-ccdc.htm",
      "parameters": [
        {
          "name": "in_multidimensional_raster",
          "explanation": "The input multidimensional raster dataset.",
          "datatype": "Raster Dataset; Raster Layer; Mosaic Dataset; Mosaic Layer; Image Service"
        },
        {
          "name": "bands[bands,...](Optional)",
          "explanation": "The band IDs to use for change detection. If no band IDs are provided, all the bands from the input raster dataset will be used.",
          "datatype": "Long"
        },
        {
          "name": "tmask_bands[tmask_bands,...](Optional)",
          "explanation": "The band IDs to be used in the temporal mask (Tmask). It is recommended that you use the green band and the SWIR band. If no band IDs are provided, no masking will occur.",
          "datatype": "Long"
        },
        {
          "name": "chi_squared_threshold(Optional)",
          "explanation": "The chi-square statistic change probability threshold. If an observation has a calculated change probability that is above this threshold, \r\nit is flagged as an anomaly, which is a potential change event. The default value is 0.99.",
          "datatype": "Double"
        },
        {
          "name": "min_anomaly_observations(Optional)",
          "explanation": "The minimum number of consecutive anomaly observations that must occur before an event is considered a change. A pixel must be flagged as an anomaly for the specified number of consecutive time slices before it is considered a true change. The default value is 6.",
          "datatype": "Long"
        },
        {
          "name": "update_frequency(Optional)",
          "explanation": "The frequency, in years, at which to update the time series model with new observations. \r\nThe default value is 1.",
          "datatype": "Double"
        }
      ],
      "summary": "Evaluates changes in pixel values over time using the Continuous Change Detection and Classification (CCDC) method and generates a change analysis raster containing the model results. Learn more about how Analyze Changes Using CCDC works",
      "extraction_date": "2025-10-01T15:11:46.681118"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Analyze Changes Using LandTrendr",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/analyze-changes-using-landtrendr.htm",
      "parameters": [
        {
          "name": "in_multidimensional_raster",
          "explanation": "The input multidimensional raster dataset.",
          "datatype": "Raster Dataset; Raster Layer; Mosaic Dataset; Mosaic Layer; Image Service; File"
        },
        {
          "name": "processing_band(Optional)",
          "explanation": "The image band name to use for segmenting the pixel value trajectories over time. Choose the band name that will best capture the changes in the feature you want to observe.If no band value is specified and the input is multiband imagery, the first band in the multiband image will be used.",
          "datatype": "String"
        },
        {
          "name": "snapping_date(Optional)",
          "explanation": "The date used to identify a slice for each year in the input multidimensional dataset. The slice with the date closest to the snapping date will be used. This parameter is required if the input dataset contains sub-yearly data. The default is 06-30, or June 30, which is approximately midway through a calendar year.",
          "datatype": "String"
        },
        {
          "name": "max_num_segments(Optional)",
          "explanation": "The maximum number of segments to be fitted to the time series for each pixel. The default is 5.",
          "datatype": "Long"
        },
        {
          "name": "vertex_count_overshoot(Optional)",
          "explanation": "The number of additional vertices beyond  max_num_segments + 1 that can be used to fit the model during the initial stage of identifying vertices. Later in the modeling process, the number of additional vertices will be reduced to max_num_segments + 1. The default is 2.",
          "datatype": "Long"
        },
        {
          "name": "spike_threshold(Optional)",
          "explanation": "The threshold to use for dampening spikes or anomalies in the pixel value trajectory. The value must  range between 0 and 1 in which 1 means no dampening. The default is 0.9.",
          "datatype": "Double"
        },
        {
          "name": "recovery_threshold(Optional)",
          "explanation": "The recovery threshold value in years. If a segment has a recovery rate that is faster than 1/recovery threshold, the segment is discarded and not included in the time series model. The value must range between 0 and 1. The default is 0.25.",
          "datatype": "Double"
        },
        {
          "name": "prevent_one_year_recovery(Optional)",
          "explanation": "Specifies whether segments that exhibit a one year recovery will be excluded.ALLOW_ONE_YEAR_RECOVERY—Segments that exhibit a one year recovery will not be excluded.PREVENT_ONE_YEAR_RECOVERY—Segments that exhibit a one year recovery will be excluded. This is the default.",
          "datatype": "Boolean"
        },
        {
          "name": "recovery_trend(Optional)",
          "explanation": "Specifies whether the recovery has an increasing (positive) trend.INCREASING_TREND—The recovery has an increasing trend. This is the default.DECREASING_TREND—The recovery has a decreasing trend.",
          "datatype": "Boolean"
        },
        {
          "name": "min_num_observations(Optional)",
          "explanation": "The   minimum number of valid observations required to perform fitting. The number of years in the input multidimensional dataset must be equal to or greater than this value. The default is 6.",
          "datatype": "Long"
        },
        {
          "name": "best_model_proportion(Optional)",
          "explanation": "The best model proportion value. During the model selection process, the tool will calculate the p-value for each model and identify a model that has the most vertices while maintaining the smallest (most significant) p-value based on this proportion value. A value of 1 means the model has the lowest p-value but may not have a high number of vertices. The default is 1.25.",
          "datatype": "Double"
        },
        {
          "name": "pvalue_threshold",
          "explanation": "The p-value threshold\r\nfor a model to be selected. After the vertices are detected in the initial stage of the model fitting, the tool will fit each segment and calculate the p-value to determine the significance of the model. On the next iteration, the model will decrease the number of segments by one and recalculate the p-value. This will continue and, if the p-value is smaller than the value specified in this parameter, the model will be selected and the tool will stop searching for a better model. If no such model is selected, the tool will select a model with a p-value smaller than the lowest p-value × best model proportion value. The default is 0.01.",
          "datatype": "Double"
        },
        {
          "name": "output_other_bands(Optional)",
          "explanation": "Specifies whether other bands will be included in the segmentation process.INCLUDE_OTHER_BANDS—Other bands will be included. The segmentation and vertices information from the initial segmentation band specified in the processing_band parameter will also be fitted to the remaining bands in the multiband images. The model results will include the segmentation band first, then the remaining bands.EXCLUDE_OTHER_BANDS—Other bands will not be included. This is the default.",
          "datatype": "Boolean"
        }
      ],
      "summary": "Evaluates changes in pixel values over time using the Landsat-based detection of trends in disturbance and recovery (LandTrendr) method and generates a change analysis raster containing the model results. Learn more about how LandTrendr works",
      "extraction_date": "2025-10-01T15:11:49.430695"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Compute Change Raster",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/compute-change-raster.htm",
      "parameters": [
        {
          "name": "from_raster",
          "explanation": "The \r\ninitial or earlier raster to be analyzed.",
          "datatype": "Raster Dataset; Raster Layer; Mosaic Dataset; Mosaic Layer; Image Service; String"
        },
        {
          "name": "to_raster",
          "explanation": "The \r\nfinal or later raster to be analyzed. This is the raster that will be compared to the initial raster.",
          "datatype": "Raster Dataset; Raster Layer; Mosaic Dataset; Mosaic Layer; Image Service; String"
        },
        {
          "name": "compute_change_method(Optional)",
          "explanation": "Specifies the type of calculation that will be performed between the two rasters. DIFFERENCE—The mathematical difference, or subtraction, between the pixel values in the rasters will be calculated. This is the default.RELATIVE_DIFFERENCE—The difference in pixel values,  accounting for the quantities of the values being compared,  will be calculated.CATEGORICAL_DIFFERENCE—The difference between two categorical or thematic rasters will be calculated.  The output will contain class transitions that occurred between the two rasters.SPECTRAL_EUCLIDEAN_DISTANCE—The Euclidean  distance between the pixel values of two multiband rasters will be calculated.SPECTRAL_ANGLE_DIFFERENCE—The spectral angle between the pixel values of two multiband rasters will be calculated. The output is in radians.BAND_WITH_MOST_CHANGE—The band that accounts for the most change in each pixel between two multiband rasters will be calculated.",
          "datatype": "String"
        },
        {
          "name": "from_classes[from_classes,...](Optional)",
          "explanation": "The list of class names from the from_raster parameter that will be included in the computation. If no classes are provided, all classes will be included. This parameter is enabled when the compute_change_method parameter is set to CATEGORICAL_DIFFERENCE.",
          "datatype": "String"
        },
        {
          "name": "to_classes[to_classes,...](Optional)",
          "explanation": "The list of class names from the to_raster parameter that will be included in the computation. If no classes are provided, all classes will be included. This parameter is enabled when the compute_change_method parameter is set to CATEGORICAL_DIFFERENCE.",
          "datatype": "String"
        },
        {
          "name": "filter_method(Optional)",
          "explanation": "Specifies the pixels that will be categorized in the output raster. This parameter is enabled when the compute_change_method parameter is set to CATEGORICAL_DIFFERENCE.CHANGED_PIXELS_ONLY—Only the pixels that changed categories will be categorized in the output. Pixels that did not change categories will be grouped in a class called Other.UNCHANGED_PIXELS_ONLY—Only the pixels that did not change categories will be categorized in the output. Pixels that changed categories will be grouped in a class called Other.ALL—All pixels will be categorized in the output. This is the default.",
          "datatype": "String"
        },
        {
          "name": "define_transition_colors(Optional)",
          "explanation": "Specifies the color that will be used to symbolize the output classes.  When a pixel changes from one class type to another, the output pixel color represents the initial class type, the final class type, or a blend of the two.This parameter is enabled when the compute_change_method parameter is set to CATEGORICAL_DIFFERENCE.AVERAGE—The color  of the output class will be the average of the from (initial) and to (final) class colors. This is the default.FROM_COLOR—The color of the output class will match the color of the from (initial) class.TO_COLOR—The color of the output class will match the color of the to (final) class.",
          "datatype": "String"
        }
      ],
      "summary": "Calculates the absolute, relative, categorical, or spectral difference between two raster datasets.",
      "extraction_date": "2025-10-01T15:11:52.071218"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Detect Change Using Change Analysis Raster",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/detect-change-using-change-analysis-raster.htm",
      "parameters": [
        {
          "name": "in_change_analysis_raster",
          "explanation": "The change analysis raster generated from the Analyze Changes Using CCDCtool or the Analyze Changes Using LandTrendr tool.",
          "datatype": "Raster Dataset; Raster Layer; Image Service"
        },
        {
          "name": "change_type(Optional)",
          "explanation": "Specifies the change information that will be calculated for each pixel.TIME_OF_LATEST_CHANGE—Each pixel will contain the date of its most recent change in the time series. This is the default.TIME_OF_EARLIEST_CHANGE—Each pixel will contain the date of its earliest change in the time series.TIME_OF_LARGEST_CHANGE—Each pixel will contain the date of its most significant change in the time series.NUM_OF_CHANGES—Each pixel will contain the total number of times it changed in the time series.TIME_OF_LONGEST_CHANGE—Each pixel will contain the date of change at the beginning or end of the longest transition segment in the time series.TIME_OF_SHORTEST_CHANGE—Each pixel will contain the date of change at the beginning or end of the shortest transition segment in the time series.TIME_OF_FASTEST_CHANGE—Each pixel will contain the date of change at the beginning or end of the transition that occurred most quickly.TIME_OF_SLOWEST_CHANGE—Each pixel will contain the date of change at the beginning or end of the transition that occurred most slowly.",
          "datatype": "String"
        },
        {
          "name": "max_number_changes(Optional)",
          "explanation": "The maximum number of changes per pixel that will be calculated. This number corresponds to the number of bands in the output raster. The default is 1, meaning only one change date will be calculated, and the output raster will contain only one band.\r\nThis parameter is not enabled when the change_type parameter is set to NUM_OF_CHANGES.",
          "datatype": "Long"
        },
        {
          "name": "segment_date(Optional)",
          "explanation": "Specifies whether the date at the beginning of a change segment will be extracted or at the end.This parameter is available only when the input change analysis raster is the output from the Analyze Changes Using LandTrendr tool.BEGINNING_OF_SEGMENT—The date at the beginning of a change segment will be extracted. This is the default.END_OF_SEGMENT—The date at the end of a change segment will be extracted.",
          "datatype": "String"
        },
        {
          "name": "change_direction(Optional)",
          "explanation": "Specifies the direction of change that will be included in the analysis.This parameter is available only when the input change analysis raster is the output from the Analyze Changes Using LandTrendr tool.ALL—All change directions will be included in the output. This is the default.INCREASE—Only change in the positive or increasing direction will be included in the output.DECREASE—Only change in the negative or decreasing direction will be included in the output.",
          "datatype": "String"
        },
        {
          "name": "filter_by_year(Optional)",
          "explanation": "Specifies whether the output will be filtered by a range of years. FILTER_BY_YEAR—Results will be filtered so that only changes that occurred within a specific range of years will be included in the output.NO_FILTER_BY_YEAR—Results will not be filtered by year. This is the default.",
          "datatype": "Boolean"
        },
        {
          "name": "min_year(Optional)",
          "explanation": "The earliest year that will be used to filter results. This parameter is required if the filter_by_year parameter is set to FILTER_BY_YEAR.",
          "datatype": "Long"
        },
        {
          "name": "max_year(Optional)",
          "explanation": "The latest year that will be used to filter results. This parameter is required if the filter_by_year parameter is set to FILTER_BY_YEAR.",
          "datatype": "Long"
        },
        {
          "name": "filter_by_duration(Optional)",
          "explanation": "Specifies whether results will be filtered by the change duration. This parameter is enabled only when the input change analysis raster is the output from the Analyze Changes Using LandTrendr tool.FILTER_BY_DURATION—Results will be filtered by duration so that only the changes that lasted a given amount of time will be included in the output.NO_FILTER_BY_DURATION—Results will not be filtered by duration. This is the default.",
          "datatype": "Boolean"
        },
        {
          "name": "min_duration(Optional)",
          "explanation": "The minimum number of consecutive years that will be included in the results. This parameter is required if the filter_by_duration parameter is set to FILTER_BY_DURATION.",
          "datatype": "Double"
        },
        {
          "name": "max_duration(Optional)",
          "explanation": "The maximum number of consecutive years that will be included in the results. This parameter is required if the filter_by_duration parameter is set to FILTER_BY_DURATION.",
          "datatype": "Double"
        },
        {
          "name": "filter_by_magnitude(Optional)",
          "explanation": "Specifies whether results will be filtered by change magnitude.Checked—Results will be filtered by magnitude so that only the changes of a given magnitude will be included in the output.Unchecked—Results will not be filtered by magnitude. This is the default.Specifies whether results will be filtered by change magnitude.FILTER_BY_MAGNITUDE—Results will be filtered by magnitude so that only the changes of a given magnitude will be included in the output.NO_FILTER_BY_MAGNITUDE—Results will not be filtered by magnitude. This is the default.",
          "datatype": "Boolean"
        },
        {
          "name": "min_magnitude(Optional)",
          "explanation": "The minimum magnitude that will be included in the results. This parameter is required if the filter_by_magnitude parameter is set to FILTER_BY_MAGNITUDE.",
          "datatype": "Double"
        },
        {
          "name": "max_magnitude(Optional)",
          "explanation": "The maximum magnitude that will be included in the results. This parameter is required if the filter_by_magnitude parameter is set to FILTER_BY_MAGNITUDE.",
          "datatype": "Double"
        },
        {
          "name": "filter_by_start_value(Optional)",
          "explanation": "Specifies whether results will be filtered by start value.This parameter is enabled only when the input change analysis raster is the output from the Analyze Changes Using LandTrendr tool.FILTER_BY_START_VALUE—Results will be filtered by start value so that only the changes of a given start value will be included in the output.NO_FILTER_BY_START_VALUE—Results will not be filtered by start value. This is the default.",
          "datatype": "Boolean"
        },
        {
          "name": "min_start_value(Optional)",
          "explanation": "The minimum start value that will be included in the results.This parameter is required if the filter_by_start_value parameter is set to FILTER_BY_START_VALUE.",
          "datatype": "Double"
        },
        {
          "name": "max_start_value(Optional)",
          "explanation": "The maximum start value that will be included in the results.This parameter is required if the filter_by_start_value parameter is set to FILTER_BY_START_VALUE.",
          "datatype": "Double"
        },
        {
          "name": "filter_by_end_value(Optional)",
          "explanation": "Specifies whether results will be filtered by end value.This parameter is enabled only when the input change analysis raster is the output from the Analyze Changes Using LandTrendr tool.FILTER_BY_END_VALUE—Results will be filtered by end value so that only the changes of a given end value will be included in the output.NO_FILTER_BY_END_VALUE—Results will not be filtered by end value. This is the default.",
          "datatype": "Boolean"
        },
        {
          "name": "min_end_value(Optional)",
          "explanation": "The minimum end value that will be included in the results.This parameter is required if the filter_by_end_value parameter is set to FILTER_BY_END_VALUE.",
          "datatype": "Double"
        },
        {
          "name": "max_end_value(Optional)",
          "explanation": "The maximum end value that will be included in the results.This parameter is required if the filter_by_end_value parameter is set to FILTER_BY_END_VALUE.",
          "datatype": "Double"
        }
      ],
      "summary": "Generates a raster containing pixel change information using the output change analysis raster from the Analyze Changes Using CCDC tool or the Analyze Changes Using LandTrendr tool.",
      "extraction_date": "2025-10-01T15:11:54.808627"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Classify Raster",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/classify-raster.htm",
      "parameters": [
        {
          "name": "in_raster",
          "explanation": "The raster dataset to classify.",
          "datatype": "Mosaic Layer; Raster Layer; Image Service; String; Raster Dataset; Mosaic Dataset"
        },
        {
          "name": "in_classifier_definition",
          "explanation": "The input Esri classifier definition file (.ecd) containing the statistics for the chosen attributes for the classifier.",
          "datatype": "File"
        },
        {
          "name": "in_additional_raster(Optional)",
          "explanation": "Ancillary raster datasets, such as a multispectral image or a DEM, will be incorporated to generate attributes and other required information for the classifier. This raster is necessary when calculating attributes such as mean or standard deviation. This parameter is optional.",
          "datatype": "Mosaic Layer; Raster Layer; Image Service; String; Raster Dataset; Mosaic Dataset"
        }
      ],
      "summary": "Classifies a raster dataset based on an Esri classifier definition file (.ecd) and raster dataset inputs. The .ecd file contains all the information needed to perform a specific type of Esri-supported classification. The inputs to this tool must match the inputs used to generate the required .ecd file. The .ecd file can be generated from any of the classifier training tools, such as Train Random Trees Classifier or Train Support Vector Machine Classifier.",
      "extraction_date": "2025-10-01T15:11:59.164495"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Classify Raster Using Spectra",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/classify-raster-using-spectra.htm",
      "parameters": [
        {
          "name": "in_raster",
          "explanation": "The input multiband raster.",
          "datatype": "Mosaic Layer; Raster Layer; Image Service; String; Raster Dataset; Mosaic Dataset"
        },
        {
          "name": "in_spectra_file",
          "explanation": "The  spectral information for different pixel classes.\r\nThe spectral information can be provided as point features, a training sample point feature class generated from the Training Samples Manager pane, or  a  .json file that contains the class spectral profiles.",
          "datatype": "Feature Layer; File; String"
        },
        {
          "name": "method",
          "explanation": "Specifies the spectral matching method that will be used.SAM—The  vector angle between the input multiband raster and the reference spectra will be calculated in which the spectra of each pixel is treated as a vector. Angle values are in radians.SID—The spectral information divergence between the input multiband raster and the reference spectra will be calculated. A score will be calculated for each pixel based on the divergence between the probability distributions of the pixel and reference spectra. Values are in radians.",
          "datatype": "String"
        },
        {
          "name": "thresholds(Optional)",
          "explanation": "The threshold for spectral matching. Pixel values that exceed this value will be classified as undefined. This can be a single value applied to all spectral classes or a space-delimited list of values for each class.",
          "datatype": "String"
        },
        {
          "name": "out_score_raster(Optional)",
          "explanation": "A multiband raster that stores the matching results for each end member. The band order follows the order of the classes in the in_spectra_file parameter value. If the input is a multidimensional raster, the output format must be CRF.",
          "datatype": "Raster Dataset"
        },
        {
          "name": "out_classifier_definition(Optional)",
          "explanation": "The output .ecd file.",
          "datatype": "File"
        }
      ],
      "summary": "Classifies a multiband raster dataset using spectral matching techniques. The input spectral data can be provided as a point feature class or a .json file.",
      "extraction_date": "2025-10-01T15:12:01.607212"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Compute Confusion Matrix",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/compute-confusion-matrix.htm",
      "parameters": [
        {
          "name": "in_accuracy_assessment_points",
          "explanation": "The accuracy assessment point feature class created from the Create Accuracy Assessment Points tool, containing the Classified and GrndTruth fields. These fields are both long integer field types.",
          "datatype": "Feature Layer"
        },
        {
          "name": "out_confusion_matrix",
          "explanation": "The output file name of the confusion matrix in table format.The format of the table is determined by the output location and path. By default, the output will be a geodatabase table. If the path is not in a geodatabase, specify the .dbf extension to save it in dBASE format.",
          "datatype": "Table"
        }
      ],
      "summary": "Computes a confusion matrix with errors of omission and commission and derives a kappa index of agreement, Intersection over Union (IoU), and overall accuracy between the classified map and the reference data. This tool uses the outputs from the Create Accuracy Assessment Points tool or the Update Accuracy Assessment Points tool.",
      "extraction_date": "2025-10-01T15:12:03.990547"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Compute Segment Attributes",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/compute-segment-attributes.htm",
      "parameters": [
        {
          "name": "in_segmented_raster",
          "explanation": "The input segmented raster dataset, where all the pixels belonging to a segment have the same converged RGB color. Usually, it is an 8-bit, 3-band RGB raster, but it can also be a 1-band grayscale raster.",
          "datatype": "Raster Layer; Mosaic Layer"
        },
        {
          "name": "in_additional_raster(Optional)",
          "explanation": "Ancillary raster datasets, such as a multispectral image or a DEM, will be incorporated to generate attributes and other required information for the classifier. This raster is necessary when calculating attributes such as mean or standard deviation. This parameter is optional.",
          "datatype": "Raster Layer; Mosaic Layer"
        },
        {
          "name": "used_attributes[used_attributes,...](Optional)",
          "explanation": "Specifies the attributes that will be included in the attribute table associated with the output raster.COLOR—The RGB color values will be derived from the input raster on a per-segment basis. This is also known as average chromaticity color.MEAN—The average digital number (DN) will be derived from the optional pixel image on a per-segment basis.STD—The standard deviation will be derived from the optional pixel image on a per-segment basis.COUNT—The number of pixels composing the segment, on a per-segment basis.COMPACTNESS—The degree to which a segment is compact or circular, on a per-segment basis. The values range from 0 to 1, in which 1 is a circle.RECTANGULARITY—The degree to which the segment is rectangular, on a per-segment basis. The values range from 0 to 1, in which 1 is a rectangle.\r\n If the only input into the tool is a segmented image, the default attributes are COLOR, COUNT, COMPACTNESS, and RECTANGULARITY. If an in_additional_raster is also included as an input along with a segmented image, then MEAN and STD are available as options.",
          "datatype": "String"
        }
      ],
      "summary": "Computes a set of attributes associated with the segmented image. The input raster can be a single-band or 3-band, 8-bit segmented image.",
      "extraction_date": "2025-10-01T15:12:06.429425"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Create Accuracy Assessment Points",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/create-accuracy-assessment-points.htm",
      "parameters": [
        {
          "name": "in_class_data",
          "explanation": "The input classification image or other thematic GIS reference data. The input can be a raster or feature class.Typical data is a classification image of a single band of integer data type.If using polygons as input, only use those that are not used as training samples. They can also be GIS land-cover data in shapefile or feature class format.",
          "datatype": "Raster Layer; Mosaic Layer; Feature Layer"
        },
        {
          "name": "out_points",
          "explanation": "The output point shapefile or feature class that contains the random points to be used for accuracy assessment.",
          "datatype": "Feature Class"
        },
        {
          "name": "target_field(Optional)",
          "explanation": "Specifies whether the input data is a classified image or ground truth data.A classified image is the image that was just classified. Ground truth data, or reference data, consists of identified features and will be compared to the classified results for quality assurance.\r\nCLASSIFIED—The input is a classified image. This is the default.GROUND_TRUTH—The input is reference data.",
          "datatype": "String"
        },
        {
          "name": "num_random_points(Optional)",
          "explanation": "The total number of random points that will be generated.The actual number may exceed but never fall below this number, depending on sampling strategy and number of classes. The default number of randomly generated points is 500.",
          "datatype": "Long"
        },
        {
          "name": "sampling(Optional)",
          "explanation": "Specifies the sampling scheme that will be used.STRATIFIED_RANDOM—Randomly distributed points will be created in each class in which each class has a number of points proportional to its relative area. This is the defaultEQUALIZED_STRATIFIED_RANDOM—Randomly distributed points will be created in each class in which each class has the same number of points.RANDOM—Randomly distributed points will be created throughout the image.",
          "datatype": "String"
        },
        {
          "name": "polygon_dimension_field(Optional)",
          "explanation": "A field that defines the dimension (time) of the features. This parameter is used only if the classification result is a multidimensional raster and you want to generate assessment points from a feature class, such as land classification polygons for multiple years.",
          "datatype": "Field"
        },
        {
          "name": "min_point_distance(Optional)",
          "explanation": "The minimum distance between the reference points. The default is 0.",
          "datatype": "Double"
        }
      ],
      "summary": "Creates randomly sampled points for postclassification accuracy assessment. A common practice is to randomly select hundreds of points and label their classification types by referencing reliable sources, such as field work or human interpretation of high-resolution imagery. The reference points are then compared with the classification results at the same locations.",
      "extraction_date": "2025-10-01T15:12:08.869461"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Generate Training Samples From Seed Points",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/generate-training-samples-from-seed-points.htm",
      "parameters": [
        {
          "name": "in_class_data",
          "explanation": "The data source that labels the training samples.",
          "datatype": "Mosaic Layer; Raster Layer; Feature Layer; Image Service; String"
        },
        {
          "name": "in_seed_points",
          "explanation": "A point shapefile or feature class to provide the centers of training sample polygons.",
          "datatype": "Feature Layer"
        },
        {
          "name": "out_training_feature_class",
          "explanation": "The output training sample feature class in the format that can be used in training tools, including shapefiles. The output feature class can be either a polygon feature class or a point feature class.",
          "datatype": "Feature Class"
        },
        {
          "name": "min_sample_area(Optional)",
          "explanation": "The minimum area needed for each training sample, in square meters. The minimum value must be greater than or equal to 0.",
          "datatype": "Double"
        },
        {
          "name": "max_sample_radius(Optional)",
          "explanation": "The longest distance (in meters) from any point within the training sample to its center seed point. If set to 0, the output training sample will be points instead of polygons. The minimum value must be greater than or equal to 0.",
          "datatype": "Double"
        }
      ],
      "summary": "Generates training samples from seed points, such as accuracy assessment points or training sample points. A typical use case is generating training samples from an existing source, such as a thematic raster or a feature class.",
      "extraction_date": "2025-10-01T15:12:11.213670"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Inspect Training Samples",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/inspect-training-samples.htm",
      "parameters": [
        {
          "name": "in_raster",
          "explanation": "The input raster to be classified.",
          "datatype": "Mosaic Layer; Raster Layer; Image Service; String"
        },
        {
          "name": "in_training_features",
          "explanation": "A training sample feature class created in the Training Samples Manager pane.",
          "datatype": "Feature Layer"
        },
        {
          "name": "in_classifier_definition",
          "explanation": "The .ecd output classifier file from any of the train classifier tools. The .ecd file is a JSON file that contains attribute information, statistics, or other information needed for the classifier.",
          "datatype": "File"
        },
        {
          "name": "out_training_feature_class",
          "explanation": "The output individual training samples saved as a feature class. The associated attribute table contains an addition field listing the accuracy score.",
          "datatype": "Feature Class"
        },
        {
          "name": "out_misclassified_raster",
          "explanation": "The output misclassified raster having NoData outside training samples. In training samples, correctly classified pixels are represented as NoData, and misclassified pixels are represented by their class value. The results is an index map of misclassified class values.",
          "datatype": "Raster Dataset"
        },
        {
          "name": "in_additional_raster(Optional)",
          "explanation": "Ancillary raster datasets, such as a multispectral image or a DEM, will be incorporated to generate attributes and other required information for the classifier. This raster is necessary when calculating attributes such as mean or standard deviation. This parameter is optional.",
          "datatype": "Mosaic Layer; Raster Layer; Image Service; String"
        }
      ],
      "summary": "Estimates the accuracy of individual training samples. The cross validation accuracy is computed using the previously generated classification training result in an .ecd file and the training samples. Outputs include a raster dataset containing the misclassified class values and a training sample dataset with the accuracy score for each training sample.",
      "extraction_date": "2025-10-01T15:12:13.713310"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Linear Spectral Unmixing",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/linear-spectral-unmixing.htm",
      "parameters": [
        {
          "name": "in_raster",
          "explanation": "The \r\ninput raster dataset.",
          "datatype": "Raster Dataset; Mosaic Dataset; Mosaic Layer; Raster Layer; File; Image Service"
        },
        {
          "name": "in_spectral_profile_file",
          "explanation": "The  spectral information for the different land-cover classes.This can be provided as polygon features, a classifier definition file (.ecd) generated from the Train Maximum Likelihood Classifier tool, or  a  JSON format file (.json) that contains the class spectral profiles.",
          "datatype": "File; Feature Layer; String"
        },
        {
          "name": "value_option[value_option,...](Optional)",
          "explanation": "Specifies how the output pixel values will be defined.SUM_TO_ONE—Class values for each pixel will be provided in decimal format with the sum of all classes equal to 1. For example, Class1 = 0.16; Class2 = 0.24; Class3 = 0.60.NON_NEGATIVE—There will be no negative output values.",
          "datatype": "String"
        }
      ],
      "summary": "Performs subpixel classification and calculates the fractional abundance of different land-cover types for individual pixels.",
      "extraction_date": "2025-10-01T15:12:16.175133"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Predict Using Regression Model",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/predict-using-regression-model.htm",
      "parameters": [
        {
          "name": "in_rasters[in_rasters,...]",
          "explanation": "The single-band, multidimensional, or multiband raster datasets, or mosaic datasets containing \r\nexplanatory variables.",
          "datatype": "Raster Dataset; Raster Layer; Mosaic Dataset; Mosaic Layer; Image Service; String"
        },
        {
          "name": "in_regression_definition",
          "explanation": "A JSON format file that contains attribute information, statistics, or other information from the regression model. The file has an .ecd extension. The file is the output of the Train Random Trees Regression Model tool.",
          "datatype": "File"
        },
        {
          "name": "out_raster_dataset",
          "explanation": "A raster of the predicted values.",
          "datatype": "Raster Dataset"
        }
      ],
      "summary": "Predicts data values using the output from the Train Random Trees Regression Model tool.",
      "extraction_date": "2025-10-01T15:12:18.682236"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Train Random Trees Regression Model",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/train-random-trees-regression-model.htm",
      "parameters": [
        {
          "name": "in_rasters[in_rasters,...]",
          "explanation": "The single-band, multidimensional, or multiband raster datasets, or mosaic datasets containing \r\nexplanatory variables.",
          "datatype": "Mosaic Dataset; Mosaic Layer; Raster Dataset; Raster Layer; Image Service; String"
        },
        {
          "name": "in_target_data",
          "explanation": "The raster or point feature class containing the target variable (dependant variable)  data.",
          "datatype": "Feature Class; Feature Layer; Raster Dataset; Raster Layer; Mosaic Layer; Image Service"
        },
        {
          "name": "out_regression_definition",
          "explanation": "A JSON format file with an .ecd extension that contains attribute information, statistics, or other information for the classifier.",
          "datatype": "File"
        },
        {
          "name": "target_value_field(Optional)",
          "explanation": "The field name of the information to model in the target point feature class or raster dataset.",
          "datatype": "Field"
        },
        {
          "name": "target_dimension_field(Optional)",
          "explanation": "A date field or numeric field in the input point feature class that defines the dimension values.",
          "datatype": "Field"
        },
        {
          "name": "raster_dimension(Optional)",
          "explanation": "The dimension name of the input multidimensional raster (explanatory variables) that  links to the dimension in the target data.",
          "datatype": "String"
        },
        {
          "name": "out_importance_table(Optional)",
          "explanation": "A table containing information describing the importance of each explanatory variable used in the model. A larger number indicates the corresponding variable is more correlated to the predicted variable and will contribute more in prediction. Values range between 0 and 1, and the sum of all the values equals 1.",
          "datatype": "Table"
        },
        {
          "name": "max_num_trees(Optional)",
          "explanation": "The maximum number of trees in the forest. Increasing the number of trees will lead to higher accuracy rates, although this improvement will level off. The number of trees increases the processing time linearly. The default is 50.",
          "datatype": "Long"
        },
        {
          "name": "max_tree_depth(Optional)",
          "explanation": "The maximum depth of each tree in the forest. Depth determines the number of rules each tree can create, resulting in a decision. Trees will not grow any deeper than this setting. The default is 30.",
          "datatype": "Long"
        },
        {
          "name": "max_samples(Optional)",
          "explanation": "The maximum number of samples that will be used for the regression analysis. A value that is less than or equal to 0 means that the system will use all the samples from the input target raster or point feature class to train the regression model.\r\nThe default value is 10,000.",
          "datatype": "Long"
        },
        {
          "name": "average_points_per_cell(Optional)",
          "explanation": "Specifies whether the average will be calculated when multiple training points fall into one cell. This parameter is applicable only when the input target is a point feature class.Unchecked—All points will be used when multiple training points fall into  a single cell. This is the default.Checked—The average value of the training points in a cell will be calculated.KEEP_ALL_POINTS—All points will be used when multiple training points fall into  a single cell. This is the default.AVERAGE_POINTS_PER_CELL—The average value of the training points in a cell will be calculated.",
          "datatype": "Boolean"
        },
        {
          "name": "percent_testing(Optional)",
          "explanation": "The percentage of test points that will be used for error checking. The tool checks for three types of errors: errors on training points, errors on test points, and errors on test location points. The default is 10.",
          "datatype": "Double"
        },
        {
          "name": "out_scatterplots(Optional)",
          "explanation": "The output scatter plots in PDF or HTML format. The output will include scatter plots of training data, test data, and location test data.",
          "datatype": "File"
        },
        {
          "name": "out_sample_features(Optional)",
          "explanation": "The output feature class that will contain target values and predicted values for training points, test points, and location test points.",
          "datatype": "Feature Class"
        }
      ],
      "summary": "Models the relationship between explanatory variables (independent variables) and a target dataset (dependent variable).",
      "extraction_date": "2025-10-01T15:12:21.474366"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Remove Raster Segment Tiling Artifacts",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/remove-raster-segment-tiling-artifacts.htm",
      "parameters": [
        {
          "name": "in_segmented_raster",
          "explanation": "Select the segmented raster with the tiling artifacts that you want to remove.",
          "datatype": "Raster Dataset; Mosaic Dataset; Raster Layer; Mosaic Layer; Image Service; String"
        },
        {
          "name": "tileSizeX(Optional)",
          "explanation": "Specify the tile width from Segment Mean Shift. If left blank, the default is 512 pixels.",
          "datatype": "Long"
        },
        {
          "name": "tileSizeY(Optional)",
          "explanation": "Specify the tile height from Segment Mean Shift. If left blank, the default is 512 pixels.",
          "datatype": "Long"
        }
      ],
      "summary": "Corrects segments or objects cut by tile boundaries during the segmentation process performed as a raster function. This tool is helpful for some regional processes, such as image segmentation, that have inconsistencies near image tile boundaries. This processing step is included in the Segment Mean Shift tool. It should only be used on a segmented image that was not created from that tool.",
      "extraction_date": "2025-10-01T15:12:24.011402"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Segment Mean Shift",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/segment-mean-shift.htm",
      "parameters": [
        {
          "name": "in_raster",
          "explanation": "The raster dataset to segment. This can be a multispectral or grayscale image.",
          "datatype": "Mosaic Layer; Raster Layer"
        },
        {
          "name": "spectral_detail(Optional)",
          "explanation": "The level of importance given to the spectral differences of features in the imagery.Valid values range from 1.0 to 20.0. A higher value is appropriate when there are features to classify separately that have similar spectral characteristics. Smaller values create spectrally smoother outputs. For example, with higher spectral detail in a forested scene, there will be greater discrimination between the tree species.",
          "datatype": "Double"
        },
        {
          "name": "spatial_detail(Optional)",
          "explanation": "The level of importance given to the proximity between features in the imagery.Valid values range from 1.0 to 20. A higher value is appropriate for a scene in which the features of interest are small and clustered together. Smaller values create spatially smoother outputs. For example, in an urban scene, impervious surfaces can be classified using a smaller spatial detail, or buildings and roads can be classified as separate classes using a higher spatial detail.",
          "datatype": "Long"
        },
        {
          "name": "min_segment_size(Optional)",
          "explanation": "The minimum size of a segment. Merge segments smaller than this size with their best fitting neighbor segment. This is related to the minimum mapping unit for your project.Units are in pixels.",
          "datatype": "Long"
        },
        {
          "name": "band_indexes(Optional)",
          "explanation": "The bands that will be used to segment the imagery, separated by a space. If no band indexes are specified, they are determined by the following criteria:\r\n If the raster has only 3 bands, those 3 bands are used If the raster has more than 3 bands, the tool assigns the red, green, and blue bands according to the raster's properties.If the red, green, and blue bands are not identified in the raster dataset's properties, bands 1, 2, and 3 are used.The band order will not change the result.Select bands that offer the most differentiation between the features of interest.",
          "datatype": "String"
        },
        {
          "name": "max_segment_size(Optional)",
          "explanation": "The maximum size of a segment. Segments that are larger than the specified size will be divided. Use this parameter to prevent artifacts in the output raster resulting from large segments.Units are in pixels.The default value is -1, meaning there is no limit on the segment size.",
          "datatype": "Long"
        }
      ],
      "summary": "Groups adjacent pixels that have similar spectral characteristics into segments.",
      "extraction_date": "2025-10-01T15:12:26.501357"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Train Iso Cluster Classifier",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/train-iso-cluster-classifier.htm",
      "parameters": [
        {
          "name": "in_raster",
          "explanation": "The raster dataset to classify.",
          "datatype": "Raster Layer; Mosaic Layer; Image Service; String"
        },
        {
          "name": "max_classes",
          "explanation": "Maximum number of desired classes to group pixels or segments. This should be set to be greater than the number of classes in your legend.It is possible that you will get fewer classes than what you specified for this parameter. If you need more, increase this value and aggregate classes after the training process is complete.",
          "datatype": "Long"
        },
        {
          "name": "out_classifier_definition",
          "explanation": "The output JSON format file that will contain attribute information, statistics, hyperplane vectors, and other information for the classifier. An .ecd file will be created.",
          "datatype": "File"
        },
        {
          "name": "in_additional_raster(Optional)",
          "explanation": "Ancillary raster datasets, such as a multispectral image or a DEM, will be incorporated to generate attributes and other required information for classification. This parameter is optional.",
          "datatype": "Raster Layer; Mosaic Layer; Image Service; String"
        },
        {
          "name": "max_iterations(Optional)",
          "explanation": "The maximum number of iterations the clustering process will run.The recommended range is between 10 and 20 iterations. Increasing this value will linearly increase the processing time.",
          "datatype": "Long"
        },
        {
          "name": "min_samples_per_cluster(Optional)",
          "explanation": "The minimum number of pixels or segments in a valid cluster or class.The default value of 20 is effective in creating statistically significant classes. You can increase this number for more larger clusters and less slivers; however, it may limit the overall number of classes that are created.",
          "datatype": "Long"
        },
        {
          "name": "skip_factor(Optional)",
          "explanation": "Number of pixels to skip for a pixel image input. If a segmented image is an input, specify the number of segments to skip.",
          "datatype": "Long"
        },
        {
          "name": "used_attributes[used_attributes;used_attributes,...](Optional)",
          "explanation": "Specifies the attributes that will be included in the attribute table associated with the output raster.COLOR—The RGB color values will be derived from the input raster on a per-segment basis. This is also known as average chromaticity color.MEAN—The average digital number (DN) will be derived from the optional pixel image on a per-segment basis.STD—The standard deviation will be derived from the optional pixel image on a per-segment basis.COUNT—The number of pixels composing the segment, on a per-segment basis.COMPACTNESS—The degree to which a segment is compact or circular, on a per-segment basis. The values range from 0 to 1, in which 1 is a circle.RECTANGULARITY—The degree to which the segment is rectangular, on a per-segment basis. The values range from 0 to 1, in which 1 is a rectangle.This parameter is only enabled if the Segmented key property is set to true on the input raster. If the only input to the tool is a segmented image, the default attributes are COLOR, COUNT, COMPACTNESS, and RECTANGULARITY. If an in_additional_raster value is included as an input with a segmented image, MEAN and STD are also available attributes.",
          "datatype": "String"
        },
        {
          "name": "max_merge_per_iter(Optional)",
          "explanation": "The maximum number of cluster merges per iteration. Increasing the number of merges will reduce the number of classes that are created. A lower value will result in more classes.",
          "datatype": "Long"
        },
        {
          "name": "max_merge_distance(Optional)",
          "explanation": "The maximum distance between cluster centers in feature space. Increasing the distance will allow more clusters to merge, resulting in fewer classes. A lower value will result in more classes. Values from 0 to 5 typically return the best results.",
          "datatype": "Double"
        }
      ],
      "summary": "Generates an Esri classifier definition file (.ecd) using the Iso Cluster classification definition. This tool performs an unsupervised classification.",
      "extraction_date": "2025-10-01T15:12:29.227092"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Train Maximum Likelihood Classifier",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/train-maximum-likelihood-classifier.htm",
      "parameters": [
        {
          "name": "in_raster",
          "explanation": "The raster dataset to classify.",
          "datatype": "Raster Layer; Mosaic Layer; Image Service; String"
        },
        {
          "name": "in_training_features",
          "explanation": "The training sample file or layer that delineates the training sites.These can be either shapefiles or feature classes that contain the training samples. The following field names are required in the training sample file:\r\nclassname—A text field indicating the name of the class categoryclassvalue—A long integer field containing the integer value for each class category",
          "datatype": "Feature Layer"
        },
        {
          "name": "out_classifier_definition",
          "explanation": "The output JSON format file that will contain attribute information, statistics, hyperplane vectors, and other information for the classifier. An .ecd file will be created.",
          "datatype": "File"
        },
        {
          "name": "in_additional_raster(Optional)",
          "explanation": "Incorporates ancillary raster datasets, such as a segmented image or DEM. This parameter is optional.",
          "datatype": "Raster Layer; Mosaic Layer; Image Service; String"
        },
        {
          "name": "used_attributes[used_attributes,...](Optional)",
          "explanation": "Specifies the attributes that will be included in the attribute table associated with the output raster.COLOR—The RGB color values will be derived from the input raster on a per-segment basis. This is also known as average chromaticity color.MEAN—The average digital number (DN) will be derived from the optional pixel image on a per-segment basis.STD—The standard deviation will be derived from the optional pixel image on a per-segment basis.COUNT—The number of pixels composing the segment, on a per-segment basis.COMPACTNESS—The degree to which a segment is compact or circular, on a per-segment basis. The values range from 0 to 1, in which 1 is a circle.RECTANGULARITY—The degree to which the segment is rectangular, on a per-segment basis. The values range from 0 to 1, in which 1 is a rectangle.This parameter is only enabled if the Segmented key property is set to true on the input raster. If the only input to the tool is a segmented image, the default attributes are COLOR, COUNT, COMPACTNESS, and RECTANGULARITY. If an in_additional_raster value is included as an input with a segmented image, MEAN and STD are also available attributes.",
          "datatype": "String"
        },
        {
          "name": "dimension_value_field(Optional)",
          "explanation": "Contains dimension values in the input training sample feature class.\r\nThis parameter is required to classify a time series of raster data using the change analysis raster output from the Analyze Changes Using CCDC tool.",
          "datatype": "Field"
        }
      ],
      "summary": "Generates an Esri classifier definition file (.ecd) using the Maximum Likelihood Classifier (MLC) classification definition.",
      "extraction_date": "2025-10-01T15:12:31.937362"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Train Random Trees Classifier",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/train-random-trees-classifier.htm",
      "parameters": [
        {
          "name": "in_raster",
          "explanation": "The raster dataset to classify.You can use any Esri-supported raster dataset. One option is a 3-band, 8-bit segmented raster dataset in which all the pixels in the same segment have the same color. The input can also be a single band, 8-bit, grayscale segmented raster.",
          "datatype": "Raster Layer; Mosaic Layer; Image Service; String"
        },
        {
          "name": "in_training_features",
          "explanation": "The training sample file or layer that delineates the training sites.These can be either shapefiles or feature classes that contain the training samples. The following field names are required in the training sample file:\r\nclassname—A text field indicating the name of the class categoryclassvalue—A long integer field containing the integer value for each class category",
          "datatype": "Feature Layer"
        },
        {
          "name": "out_classifier_definition",
          "explanation": "A JSON file that contains attribute information, statistics, or other information for the classifier. An .ecd file is created.",
          "datatype": "File"
        },
        {
          "name": "in_additional_raster(Optional)",
          "explanation": "Ancillary raster datasets, such as a multispectral image or a DEM, will be incorporated to generate attributes and other required information for classification. This parameter is optional.",
          "datatype": "Raster Layer; Mosaic Layer; Image Service; String"
        },
        {
          "name": "max_num_trees(Optional)",
          "explanation": "The maximum number of trees in the forest. Increasing the number of trees will lead to higher accuracy rates, although this improvement will level off eventually. The number of trees increases the processing time linearly.",
          "datatype": "Long"
        },
        {
          "name": "max_tree_depth(Optional)",
          "explanation": "The maximum depth of each tree in the forest. Depth is another way of saying the number of rules each tree is allowed to create to come to a decision. Trees will not grow any deeper than this setting.",
          "datatype": "Long"
        },
        {
          "name": "max_samples_per_class(Optional)",
          "explanation": "The maximum number of samples that will be used to define each class.The default value of 1000 is recommended when the inputs are nonsegmented rasters. A value that is less than or equal to 0 means that the system will use all the samples from the training sites to train the classifier.",
          "datatype": "Long"
        },
        {
          "name": "used_attributes[used_attributes;used_attributes,...](Optional)",
          "explanation": "Specifies the attributes that will be included in the attribute table associated with the output raster.\r\nCOLOR—The RGB color values will be derived from the input raster on a per-segment basis. This is also known as average chromaticity color.MEAN—The average digital number (DN) will be derived from the optional pixel image on a per-segment basis.STD—The standard deviation will be derived from the optional pixel image on a per-segment basis.COUNT—The number of pixels composing the segment, on a per-segment basis.COMPACTNESS—The degree to which a segment is compact or circular, on a per-segment basis. The values range from 0 to 1, in which 1 is a circle.RECTANGULARITY—The degree to which the segment is rectangular, on a per-segment basis. The values range from 0 to 1, in which 1 is a rectangle.This parameter is only enabled if the Segmented key property is set to true on the input raster. If the only input to the tool is a segmented image, the default attributes are COLOR, COUNT, COMPACTNESS, and RECTANGULARITY. If an in_additional_raster value is included as an input with a segmented image, MEAN and STD are also available attributes.",
          "datatype": "String"
        },
        {
          "name": "dimension_value_field(Optional)",
          "explanation": "Contains dimension values in the input training sample feature class.\r\nThis parameter is required to classify a time series of raster data using the change analysis raster output from the Analyze Changes Using CCDC tool.",
          "datatype": "Field"
        }
      ],
      "summary": "Generates an Esri classifier definition file (.ecd) using the Random Trees classification method. The random trees classifier is an image classification technique that is resistant to overfitting and can work with segmented images and other ancillary raster datasets. For standard image inputs, the tool accepts multiband imagery with any bit depth, and it will perform the Random Trees classification on a pixel basis or segment, based on the input training feature file.",
      "extraction_date": "2025-10-01T15:12:34.820070"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Train Support Vector Machine Classifier",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/train-support-vector-machine-classifier.htm",
      "parameters": [
        {
          "name": "in_raster",
          "explanation": "The raster dataset to classify.The preferred input is a 3-band, 8-bit segmented raster dataset in which all the pixels in the same segment have the same color. The input can also be a 1-band, 8-bit grayscale segmented raster. If no segmented raster is available, you can use any Esri-supported raster dataset.",
          "datatype": "Raster Layer; Mosaic Layer; Image Service; String"
        },
        {
          "name": "in_training_features",
          "explanation": "The training sample file or layer that delineates the training sites.These can be either shapefiles or feature classes that contain the training samples. The following field names are required in the training sample file:\r\nclassname—A text field indicating the name of the class categoryclassvalue—A long integer field containing the integer value for each class category",
          "datatype": "Feature Layer"
        },
        {
          "name": "out_classifier_definition",
          "explanation": "The output JSON format file that will contain attribute information, statistics, hyperplane vectors, and other information for the classifier. An .ecd file will be created.",
          "datatype": "File"
        },
        {
          "name": "in_additional_raster(Optional)",
          "explanation": "Ancillary raster datasets, such as a multispectral image or a DEM, will be incorporated to generate attributes and other required information for classification. This parameter is optional.",
          "datatype": "Raster Layer; Mosaic Layer; Image Service; String"
        },
        {
          "name": "max_samples_per_class(Optional)",
          "explanation": "The maximum number of samples that will be used to define each class.The default value of 500 is recommended when the inputs are nonsegmented rasters. A value that is less than or equal to 0 means that the system will use all the samples from the training sites to train the classifier.",
          "datatype": "Long"
        },
        {
          "name": "used_attributes[used_attributes;used_attributes,...](Optional)",
          "explanation": "Specifies the attributes that will be included in the attribute table associated with the output raster.\r\nCOLOR—The RGB color values will be derived from the input raster on a per-segment basis. This is also known as average chromaticity color.MEAN—The average digital number (DN) will be derived from the optional pixel image on a per-segment basis.STD—The standard deviation will be derived from the optional pixel image on a per-segment basis.COUNT—The number of pixels composing the segment, on a per-segment basis.COMPACTNESS—The degree to which a segment is compact or circular, on a per-segment basis. The values range from 0 to 1, in which 1 is a circle.RECTANGULARITY—The degree to which the segment is rectangular, on a per-segment basis. The values range from 0 to 1, in which 1 is a rectangle.This parameter is only enabled if the Segmented key property is set to true on the input raster. If the only input to the tool is a segmented image, the default attributes are COLOR, COUNT, COMPACTNESS, and RECTANGULARITY. If an in_additional_raster value is included as an input with a segmented image, MEAN and STD are also available attributes.",
          "datatype": "String"
        },
        {
          "name": "dimension_value_field(Optional)",
          "explanation": "Contains dimension values in the input training sample feature class.\r\nThis parameter is required to classify a time series of raster data using the change analysis raster output from the Analyze Changes Using CCDC tool.",
          "datatype": "Field"
        }
      ],
      "summary": "Generates an Esri classifier definition file (.ecd) using the Support Vector Machine (SVM) classification definition.",
      "extraction_date": "2025-10-01T15:12:37.572939"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Update Accuracy Assessment Points",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/update-accuracy-assessment-points.htm",
      "parameters": [
        {
          "name": "in_class_data",
          "explanation": "The input classification image or other thematic GIS reference data. The input can be a raster or feature class.Typical data is a classification image of a single band of integer data type.If using polygons as input, only use those that are not used as training samples. They can also be GIS land-cover data in shapefile or feature class format.",
          "datatype": "Raster Layer; Mosaic Layer; Feature Layer"
        },
        {
          "name": "in_points",
          "explanation": "The point feature class with the accuracy assessment points that will be updated.\r\nAll points from this input will be copied to the updated output feature class, and the target_field parameter value will be updated from the input raster or feature class data.",
          "datatype": "Feature Layer"
        },
        {
          "name": "out_points",
          "explanation": "The output point shapefile or feature class that contains the random points to be used for accuracy assessment.",
          "datatype": "Feature Class"
        },
        {
          "name": "target_field(Optional)",
          "explanation": "Specifies whether the input data is a classified image or ground truth data.A classified image is the image that was just classified. Ground truth data, or reference data, consists of identified features and will be compared to the classified results for quality assurance.\r\nCLASSIFIED—The input is a classified image. This is the default.GROUND_TRUTH—The input is reference data.",
          "datatype": "String"
        },
        {
          "name": "polygon_dimension_field(Optional)",
          "explanation": "The dimension field for the in_points parameter value. The assessment points will be updated based on the matching dimension values with this field.",
          "datatype": "Field"
        },
        {
          "name": "point_dimension_field(Optional)",
          "explanation": "The dimension field in the in_points parameter value. Input data with identical dimension values will be used to update corresponding points.\r\n\r\n\r\nWhen the in_class_data parameter value is a multidimensional raster, rasters with dimension values that match the dimension field in the test points will be used in updating. The multidimensional raster is expected to have one time dimension (StdTime field). Otherwise, the first dimension will be used to match the dimension field of the test points.",
          "datatype": "Field"
        }
      ],
      "summary": "Updates the Target field in the attribute table to compare reference points to the classified image. Accuracy assessment uses known points to assess the validity of the classification model.",
      "extraction_date": "2025-10-01T15:12:40.111897"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Understanding segmentation and classification",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/understanding-segmentation-and-classification.htm",
      "parameters": [],
      "summary": "",
      "extraction_date": "2025-10-01T15:12:42.616581"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Classify Objects Using Deep Learning",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/classify-objects-using-deep-learning.htm",
      "parameters": [
        {
          "name": "in_raster",
          "explanation": "The input image that will be used to classify objects.The input can be a single raster, multiple rasters in a mosaic dataset, an image service, a folder of images, or a feature class with image attachments.",
          "datatype": "Raster Dataset; Raster Layer; Mosaic Layer; Image Service; Map Server; Map Server Layer; Internet Tiled Layer; Folder; Feature Layer; Feature Class"
        },
        {
          "name": "out_feature_class",
          "explanation": "The output feature class that will contain geometries surrounding the objects or feature from the input feature class, as well as a field to store the categorization label.If the feature class already exists, the results will be appended to the existing feature class.",
          "datatype": "Feature Class"
        },
        {
          "name": "in_model_definition",
          "explanation": "The in_model_definition parameter value can be an Esri model definition JSON file (.emd), a JSON string, or a deep learning model package (.dlpk). A JSON string is useful when this tool is used on the server so you can paste the JSON string rather than upload the .emd file. The .dlpk file must be stored locally.It contains the path to the deep learning binary model file, the path to the Python raster function to be used, and other parameters such as preferred tile size or padding.",
          "datatype": "File; String"
        },
        {
          "name": "in_features(Optional)",
          "explanation": "The point, line, or polygon input feature class that identifies the location of each object or feature to be classified and labelled. Each row in the input feature class represents a single object or feature.If no input feature class is provided, it is assumed that each input image contains a single object to be classified. If the input image or images use a spatial reference, the output from the tool is a feature class in which the extent of each image is used as the bounding geometry for each labelled feature class. If the input image or images are not spatially referenced, the output from the tool is a table containing the image ID values and the class labels for each image.",
          "datatype": "Feature Class; Feature Layer"
        },
        {
          "name": "class_label_field(Optional)",
          "explanation": "The name of the field that will contain the class or category label in the output feature class.If no field name is provided, a ClassLabel field will be generated in the output feature class.",
          "datatype": "String"
        },
        {
          "name": "processing_mode(Optional)",
          "explanation": "Specifies how all raster items in a mosaic dataset or an image service will be processed. This parameter is applied when the input raster is a mosaic dataset or an image service.PROCESS_AS_MOSAICKED_IMAGE—All raster items in the mosaic dataset or image service will be mosaicked together and processed. This is the default.PROCESS_ITEMS_SEPARATELY—All raster items in the mosaic dataset or image service will be processed as separate images.",
          "datatype": "String"
        },
        {
          "name": "model_arguments[model_arguments,...](Optional)",
          "explanation": "The information from the in_model_definition parameter will be used to set the default values for this parameter. These arguments vary, depending on the model architecture. The following are supported model arguments for models trained in ArcGIS. ArcGIS pretrained models and custom deep learning models may have additional arguments that the tool supports.batch_size—The number of image tiles processed in each step of the model inference. This depends on the memory of your graphics card. The argument is available for all model architectures.test_time_augmentation—Performs test time augmentation while predicting. If true, predictions of flipped and rotated variants of the input image will be merged into the final output. The argument is available for all model architectures.score_threshold—The predictions above this confidence score are included in the result. The allowed values range from 0 to 1.0. The argument is available for all model architectures.",
          "datatype": "Value Table"
        },
        {
          "name": "caption_field(Optional)",
          "explanation": "The name of the field that will contain the text or caption in the output feature class. This parameter is only supported when an Image Captioner model is used.If no field name is specified, a Caption field will be generated in the output feature class.",
          "datatype": "String"
        }
      ],
      "summary": "Runs a trained deep learning model on an input raster and an optional feature class to produce a feature class or table in which each input object or feature has an assigned class or category label. This tool requires a model definition file containing trained model information. The model can be trained using the Train Deep Learning Model tool or by a third-party training software such as TensorFlow, PyTorch, or Keras. The model definition file can be an Esri model definition JSON file (.emd) or a deep learning model package, and it must contain the path to the Python raster function to be called to process each object and the path to the trained binary deep learning model file.",
      "extraction_date": "2025-10-01T15:12:47.224087"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Classify Pixels Using Deep Learning",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/classify-pixels-using-deep-learning.htm",
      "parameters": [
        {
          "name": "in_raster",
          "explanation": "The input raster dataset that will be classified. The input can be a single raster, multiple rasters in a mosaic dataset, an image service, a folder of images, or a feature class with image attachments.",
          "datatype": "Raster Dataset; Raster Layer; Mosaic Layer; Image Service; Map Server; Map Server Layer; Internet Tiled Layer; Folder; Feature Layer; Feature Class"
        },
        {
          "name": "in_model_definition",
          "explanation": "The in_model_definition parameter value can be an Esri model definition JSON file (.emd), a JSON string, or a deep learning model package (.dlpk). A JSON string is useful when this tool is used on the server so you can paste the JSON string rather than upload the .emd file. The .dlpk file must be stored locally.It contains the path to the deep learning binary model file, the path to the Python raster function to be used, and other parameters such as preferred tile size or padding.",
          "datatype": "File; String"
        },
        {
          "name": "arguments[arguments,...](Optional)",
          "explanation": "The information from the in_model_definition parameter will be used to set the default values for this parameter. These arguments vary, depending on the model architecture. The following are supported model arguments for models trained in ArcGIS. ArcGIS pretrained models and custom deep learning models may have additional arguments that the tool supports.batch_size—The number of image tiles processed in each step of the model inference. This depends on the memory of your graphics card. The argument is available for all model architectures.direction—The image is translated from one domain to another.  Options are AtoB and BtoA. The argument is only available for the CycleGAN architecture. For more information about this argument, see How CycleGAN works.merge_policy—The policy for merging augmented predictions. Available options are mean, max, and min. This is only applicable when test time augmentation is used. The argument is available for the MultiTaskRoadExtractor and ConnectNet architectures. If IsEdgeDetection is present in the model's .emd file, the BDCNEdgeDetector, HEDEdgeDetector, and MMSegmentation architectures are also available.n_timestep—The number of time steps that will be used. The default is 200. It can be increased and decreased based on the quality of generations. The argument is only supported for the Super Resolution with SR3 backbone model.padding—The number of pixels at the border of image tiles from which predictions are blended for adjacent tiles. To smooth the output while reducing artifacts, increase the value. The maximum value of the padding can be half the tile size value. The argument is available for all model architectures.predict_background—Specifies whether the background class will be classified. If true, the background class is also classified. The argument is available for UNET, PSPNET, DeepLab, MMSegmentation, and SAMLoRA.return_probability_raster—Specifies whether the output will be a probability raster. If true, the output will be a probability raster. If false, the output will be a binary classified raster. The default is false.  If ArcGISLearnVersion is 1.8.4 or later in the model's .emd file, the MultiTaskRoadExtractor and ConnectNet architectures are available. If ArcGISLearnVersion is 1.8.4 or later and IsEdgeDetection is present in the model's .emd file, the BDCNEdgeDetector, HEDEdgeDetector, and MMSegmentation architectures are also available.sampling_type—The type of sampling that will be used. Two types of sampling are available: ddim and ddpm. The  default is ddim, which  generates results in fewer time steps compared to ddpm. The argument is only supported for the Super Resolution with SR3 backbone model.schedule—An optional string that sets the type of schedule. The default schedule is the same as the model it was trained on. The argument is only supported for the Super Resolution with SR3 backbone model.test_time_augmentation—Specifies whether test time augmentation will be performed while predicting. If true, predictions of flipped and rotated variants of the input image will be merged into the final output. The argument is available for UNET, PSPNET, DeepLab, HEDEdgeDetector, BDCNEdgeDetector, ConnectNet, MMSegmentation, Multi-Task Road Extractor, and SAMLoRA.tile_size—The width and height of image tiles into which the imagery will be split for prediction.  The argument is only available for the CycleGAN architecture.thinning—Specifies whether predicted edges will be thinned or skeletonized.\r\nOptions are True and False. If IsEdgeDetection is present in the model's .emd file, the BDCNEdgeDetector, HEDEdgeDetector, and MMSegmentation architectures are available.threshold—The predictions that have a confidence score higher than this threshold are included in the result. The allowed values range from 0 to 1.0. If ArcGISLearnVersion is 1.8.4 or later in the model's .emd file, the MultiTaskRoadExtractor and ConnectNet architectures are available. If ArcGISLearnVersion is 1.8.4 or later and IsEdgeDetection is present in the model's .emd file, the BDCNEdgeDetector, HEDEdgeDetector, and MMSegmentation architectures are also available.",
          "datatype": "Value Table"
        },
        {
          "name": "processing_mode",
          "explanation": "Specifies how all raster items in a mosaic dataset or an image service will be processed. This parameter is applied when the input raster is a mosaic dataset or an image service.PROCESS_AS_MOSAICKED_IMAGE—All raster items in the mosaic dataset or image service will be mosaicked together and processed. This is the default.PROCESS_ITEMS_SEPARATELY—All raster items in the mosaic dataset or image service will be processed as separate images.",
          "datatype": "String"
        },
        {
          "name": "out_classified_folder(Optional)",
          "explanation": "The folder where the output classified rasters will be stored. A mosaic dataset will be generated using the classified rasters in this folder.This parameter is required when the input raster is a folder of images or a mosaic dataset in which all items are to be processed separately. The default is a folder in the project folder.",
          "datatype": "Folder"
        },
        {
          "name": "out_featureclass(Optional)",
          "explanation": "The feature class where the output classified rasters will be stored.This parameter is required when the input raster is a feature class of images.If the feature class already exists, the results will be appended to the existing feature class.",
          "datatype": "Feature Class"
        },
        {
          "name": "overwrite_attachments(Optional)",
          "explanation": "Specifies whether existing image attachments will be overwritten.NO_OVERWRITE—Existing image attachments will not be overwritten and new image attachments will be stored in a new feature class. When this option is specified, the out_featureclass parameter must be populated. This is the default.OVERWRITE—The existing feature class will be overwritten with the new updated  attachments.This parameter is only valid when the in_raster parameter value is a feature class with image attachments.",
          "datatype": "Boolean"
        },
        {
          "name": "use_pixelspace(Optional)",
          "explanation": "Specifies whether inferencing will be performed on images in pixel space.NO_PIXELSPACE—Inferencing will be performed in map space. This is the default.PIXELSPACE—Inferencing will be performed in image space, and the output will be transformed back to map space. This option is useful when using oblique imagery or street-view imagery, where the features may become distorted using map space.",
          "datatype": "Boolean"
        }
      ],
      "summary": "Runs a trained deep learning model on an input raster to produce a classified raster, with each valid pixel having an assigned class label. This tool requires a model definition file containing trained model information. The model can be trained using the Train Deep Learning Model tool or by a third-party training software such as TensorFlow, PyTorch, or Keras. The model definition file can be an Esri model definition JSON file (.emd) or a deep learning model package, and it must contain the path to the Python raster function to be called to process each object and the path to the trained binary deep learning model file.",
      "extraction_date": "2025-10-01T15:12:49.856215"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Compute Accuracy For Object Detection",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/compute-accuracy-for-object-detection.htm",
      "parameters": [
        {
          "name": "detected_features",
          "explanation": "The polygon feature class containing the objects detected from the Detect Objects Using Deep Learning tool.",
          "datatype": "Feature Class; Feature Layer"
        },
        {
          "name": "ground_truth_features",
          "explanation": "The polygon feature class containing ground truth data.",
          "datatype": "Feature Class; Feature Layer"
        },
        {
          "name": "out_accuracy_table",
          "explanation": "The output accuracy table.",
          "datatype": "Table"
        },
        {
          "name": "out_accuracy_report(Optional)",
          "explanation": "The name of the output accuracy report. The report is a PDF document containing accuracy metrics and charts.",
          "datatype": "File"
        },
        {
          "name": "detected_class_value_field(Optional)",
          "explanation": "The field in the detected objects feature class that contains the class values or class names.If no field name is provided, a Classvalue or Value field will be used. If these fields do not exist, all records will be identified as belonging to one class.The class values or class names must match those in the ground reference feature class exactly.",
          "datatype": "Field"
        },
        {
          "name": "ground_truth_class_value_field(Optional)",
          "explanation": "The field in the ground truth feature class that contains the class values.If no field name is provided, a Classvalue or Value field will be used. If these fields do not exist, all records will be identified as belonging to one class.The class values or class names must match those in the detected objects feature class exactly.",
          "datatype": "Field"
        },
        {
          "name": "min_iou(Optional)",
          "explanation": "The IoU ratio that will be used as a threshold to evaluate the accuracy of the object detection model. The numerator is the area of overlap between the predicted bounding box and the ground reference bounding box. The denominator is the area of union or the area encompassed by both bounding boxes. The IoU ranges from 0 to 1.",
          "datatype": "Double"
        },
        {
          "name": "mask_features(Optional)",
          "explanation": "A polygon feature class that delineates the area or areas where accuracy will be computed. Only the features that intersect the mask will be assessed for accuracy.",
          "datatype": "Feature Class; Feature Layer"
        }
      ],
      "summary": "Calculates the accuracy of a deep learning model by comparing the detected objects from the Detect Objects Using Deep Learning tool to ground truth data. Learn more about how Compute Accuracy For Object Detection works.",
      "extraction_date": "2025-10-01T15:12:52.279762"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Compute Accuracy For Pixel Classification",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/compute-accuracy-for-pixel-classification.htm",
      "parameters": [
        {
          "name": "in_raster",
          "explanation": "The input classified raster.",
          "datatype": "Mosaic Layer; Raster Layer; Image Service; String; Raster Dataset; Mosaic Dataset"
        },
        {
          "name": "in_ground_truth_data",
          "explanation": "The input classification image or other thematic GIS reference data. The input can be a raster or feature class.Typical data is a classification image of a single band of integer data type.If using polygons as input, only use those that are not used as training samples. They can also be GIS land-cover data in shapefile or feature class format.",
          "datatype": "Mosaic Layer; Raster Layer; Feature Layer"
        },
        {
          "name": "out_confusion_matrix",
          "explanation": "The output file name of the confusion matrix in table format.The format of the table is determined by the output location and path. By default, the output will be a geodatabase table. If the path is not in a geodatabase, specify the .dbf extension to save it in dBASE format.",
          "datatype": "Table"
        },
        {
          "name": "num_random_points(Optional)",
          "explanation": "The total number of random points that will be generated.The actual number may exceed but never fall below this number, depending on sampling strategy and number of classes. The default number of randomly generated points is 500.",
          "datatype": "Long"
        },
        {
          "name": "sampling(Optional)",
          "explanation": "Specifies the sampling scheme that will be used.STRATIFIED_RANDOM—Randomly distributed points will be created in each class in which each class has a number of points proportional to its relative area. This is the defaultEQUALIZED_STRATIFIED_RANDOM—Randomly distributed points will be created in each class in which each class has the same number of points.RANDOM—Randomly distributed points will be created throughout the image.",
          "datatype": "String"
        },
        {
          "name": "min_point_distance(Optional)",
          "explanation": "The minimum distance between the reference points. The default is 0.",
          "datatype": "Double"
        }
      ],
      "summary": "Computes a confusion matrix, based on errors of omission and commission, and the Intersection over Union (IoU) score. The accuracy is computed  between the output from the Classify Pixels Using Deep Learning tool  and the ground truth data. The tool is only valid for pixel classification models, not other models used with the Classify Pixels Using Deep Learning tool.",
      "extraction_date": "2025-10-01T15:12:54.636612"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Detect Change Using Deep Learning",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/detect-change-using-deep-learning.htm",
      "parameters": [
        {
          "name": "from_raster",
          "explanation": "The input raster before the change.",
          "datatype": "Raster Dataset; Raster Layer; Mosaic Layer; Image Service; Map Server; Map Server Layer; Internet Tiled Layer"
        },
        {
          "name": "to_raster",
          "explanation": "The input raster after the change.",
          "datatype": "Raster Dataset; Raster Layer; Mosaic Layer; Image Service; Map Server; Map Server Layer; Internet Tiled Layer"
        },
        {
          "name": "out_classified_raster",
          "explanation": "The output classified raster that shows the change.",
          "datatype": "Raster Dataset"
        },
        {
          "name": "in_model_definition",
          "explanation": "The in_model_definition parameter value can be an Esri model definition JSON file (.emd), a JSON string, or a deep learning model package (.dlpk). A JSON string is useful when this tool is used on the server so you can paste the JSON string rather than upload the .emd file. The .dlpk file must be stored locally.It contains the path to the deep learning binary model file, the path to the Python raster function to be used, and other parameters such as preferred tile size or padding.",
          "datatype": "File; String"
        },
        {
          "name": "arguments[arguments,...](Optional)",
          "explanation": "The information from the in_model_definition parameter will be used to set the default values for this parameter. These arguments vary, depending on the model architecture. The following are supported model arguments for models trained in ArcGIS. ArcGIS pretrained models and custom deep learning models may have additional arguments that the tool supports.padding—The number of pixels at the border of image tiles from which predictions are blended for adjacent tiles. To smooth the output while reducing artifacts, increase the value. The maximum value of the padding can be half of the tile size value. The argument is available for all model architectures.batch_size—The number of image tiles processed in each step of the model inference. This depends on the memory of your graphic card. The argument is available for all model architectures.",
          "datatype": "Value Table"
        }
      ],
      "summary": "Runs a trained deep learning model to detect change between two rasters. This tool requires a model definition file containing trained model information. The model definition file can be an Esri model definition JSON file (.emd) or a deep learning model package, and it must contain the path to the Python raster function to be called to process each object and the path to the trained binary deep learning model file.",
      "extraction_date": "2025-10-01T15:12:57.210778"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Detect Control Points",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/detect-control-points.htm",
      "parameters": [
        {
          "name": "in_mosaic_dataset",
          "explanation": "The mosaic dataset that contains the source imagery from which the ground control points will be created.",
          "datatype": "Mosaic Dataset; Mosaic Layer"
        },
        {
          "name": "in_control_points",
          "explanation": "The input control point set that contains a list of ground control point features.",
          "datatype": "File; Feature Class; Feature Layer; String"
        },
        {
          "name": "out_control_points",
          "explanation": "The output ground control point features.",
          "datatype": "Feature Class"
        },
        {
          "name": "out_folder_image_chips(Optional)",
          "explanation": "The output folder of image chips.",
          "datatype": "Folder"
        },
        {
          "name": "tile_size(Optional)",
          "explanation": "The tile size of the output image chips.The default tile size is 1024.",
          "datatype": "Long"
        },
        {
          "name": "number_tie_points_per_gcp(Optional)",
          "explanation": "The number of tie points for each ground control point.The default is 5.",
          "datatype": "Long"
        }
      ],
      "summary": "Detects ground control points in a mosaic dataset.",
      "extraction_date": "2025-10-01T15:12:59.803826"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Detect Objects Using Deep Learning",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/detect-objects-using-deep-learning.htm",
      "parameters": [
        {
          "name": "in_raster",
          "explanation": "The input image that will be used to detect objects. The input can be a single raster, multiple rasters in a mosaic dataset, an image service, a folder of images, a feature class with image attachments, or an oriented imagery dataset or layer.",
          "datatype": "Raster Dataset; Raster Layer; Mosaic Layer; Image Service; Map Server; Map Server Layer; Internet Tiled Layer; Folder; Feature Layer; Feature Class; Oriented Imagery Layer"
        },
        {
          "name": "out_detected_objects",
          "explanation": "The output feature class that will contain geometries circling the object or objects detected in the input image.If the feature class already exists, the results will be appended to the existing feature class.",
          "datatype": "Feature Class"
        },
        {
          "name": "in_model_definition",
          "explanation": "The in_model_definition parameter value can be an Esri model definition JSON file (.emd), a JSON string, or a deep learning model package (.dlpk). A JSON string is useful when this tool is used on the server so you can paste the JSON string rather than upload the .emd file. The .dlpk file must be stored locally.It contains the path to the deep learning binary model file, the path to the Python raster function to be used, and other parameters such as preferred tile size or padding.",
          "datatype": "File; String"
        },
        {
          "name": "arguments[arguments,...](Optional)",
          "explanation": "The information from the in_model_definition parameter will be used to set the default values for this parameter. These arguments vary, depending on the model architecture. The following are supported model arguments for models trained in ArcGIS. ArcGIS pretrained models and custom deep learning models may have additional arguments that the tool supports.padding—The number of pixels at the border of image tiles from which predictions will be blended for adjacent tiles. To smooth the output while reducing artifacts, increase the value. The maximum value of the padding can be half the tile size value. The argument is available for all model architectures.threshold—The detections that have a confidence score higher than this threshold will be included in the result. The allowed values range from 0 to 1.0. The argument is available for all model architectures.batch_size—The number of image tiles that will be processed in each step of the model inference. This depends on the memory of your graphics card. The argument is available for all model architectures.nms_overlap—The maximum overlap ratio for two overlapping features, which is defined as the ratio of intersection area over union area. The default is 0.1. The argument is available for all model architectures.exclude_pad_detections—If true, potentially truncated detections near the edges that are in the padded region of image chips will be filtered. The argument is available for SSD, RetinaNet, YOLOv3, DETReg, MMDetection, and Faster RCNN only.tta_scales—Performs test time augmentation using different scales. Each scale means the pixel block will be processed with the scales provided. The default scale is 1, which means no scaling occurs. The different scales are separated by commas, for example, 0.9,1,1.1.  In this case, the pixel block will be processed three times, first at the scale of 0.9, then with no scale change, and then with a scale of 1.1. test_time_augmentation—Performs test time augmentation while predicting. If true, predictions of flipped and rotated orientations of the input image will be merged into the final output and their confidence values will be averaged. This may cause the confidence values to fall below the threshold for objects that are only detected in a few orientations (of the image). The argument is available for all model architectures.tile_size—The width and height of image tiles into which the imagery will be split for prediction. The argument is only available for MaskRCNN.merge_policy—The policy for merging augmented predictions. Available options are mean, max, and min. This is only applicable when test time augmentation is used. The argument is only available for MaskRCNN.output_classified_raster—The path to the output raster. The argument is only available for MaXDeepLab.",
          "datatype": "Value Table"
        },
        {
          "name": "run_nms(Optional)",
          "explanation": "Specifies whether nonmaximum suppression will be performed in which duplicate objects are identified and the duplicate features with lower confidence value are removed.NO_NMS—Nonmaximum suppression will not be performed. All objects that are detected will be in the output feature class. This is the default.NMS—Nonmaximum suppression will be performed and duplicate objects that are detected will be removed. When the inputs are oriented imagery layers, the duplicates are retained with null ground geometries.",
          "datatype": "Boolean"
        },
        {
          "name": "confidence_score_field(Optional)",
          "explanation": "The name of the field in the feature class that will contain the confidence scores as output by the object detection method.This parameter is required when the run_nms parameter is set to NMS.",
          "datatype": "String"
        },
        {
          "name": "class_value_field(Optional)",
          "explanation": "The name of the class value field in the input feature class. If no field name is provided, a Classvalue or Value field will be used. If these fields do not exist, all records will be identified as belonging to one class.",
          "datatype": "String"
        },
        {
          "name": "max_overlap_ratio(Optional)",
          "explanation": "The maximum overlap ratio for two overlapping features, which is defined as the ratio of intersection area over union area. The default is 0.",
          "datatype": "Double"
        },
        {
          "name": "processing_mode(Optional)",
          "explanation": "Specifies how all raster items in a mosaic dataset or an image service will be processed. This parameter is applied when the input raster is a mosaic dataset or an image service.PROCESS_AS_MOSAICKED_IMAGE—All raster items in the mosaic dataset or image service will be mosaicked together and processed. This is the default.PROCESS_ITEMS_SEPARATELY—All raster items in the mosaic dataset or image service will be processed as separate images.PROCESS_CANDIDATE_ITEMS_ONLY—Only raster items with a value of 1 or 2 in the Candidate field of the input mosaic dataset's attribute table will be processed.",
          "datatype": "String"
        },
        {
          "name": "use_pixelspace(Optional)",
          "explanation": "Specifies whether inferencing will be performed on images in pixel space.NO_PIXELSPACE—Inferencing will be performed in map space. This is the default.PIXELSPACE—Inferencing will be performed in image space, and the output will be transformed back to map space. This option is useful when using oblique imagery or street-view imagery, where the features may become distorted using map space.",
          "datatype": "Boolean"
        },
        {
          "name": "in_objects_of_interest[in_objects_of_interest,...](Optional)",
          "explanation": "Specifies the objects that will be detected by the tool. The available options will be based on the in_model_definition parameter value.This parameter is only active when the model detects more than one type of object.",
          "datatype": "String"
        }
      ],
      "summary": "Runs a trained deep learning model on an input raster to produce a feature class containing the objects it finds. The features can be bounding boxes or polygons around the objects found or points at the centers of the objects. This tool requires a model definition file containing trained model information. The model can be trained using the Train Deep Learning Model tool or by a third-party training software such as TensorFlow, PyTorch, or Keras. The model definition file can be an Esri model definition JSON file (.emd) or a deep learning model package, and it must contain the path to the Python raster function to be called to process each object and the path to the trained binary deep learning model file.",
      "extraction_date": "2025-10-01T15:13:02.426164"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Export Training Data For Deep Learning",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/export-training-data-for-deep-learning.htm",
      "parameters": [
        {
          "name": "in_raster",
          "explanation": "The input source imagery, typically multispectral imagery.Examples of the types of input source imagery include multispectral satellite, drone, aerial, and National Agriculture Imagery Program (NAIP). The input can be a folder of images.",
          "datatype": "Raster Dataset; Raster Layer; Mosaic Layer; Image Service; Map Server; Map Server Layer; Internet Tiled Layer; Folder"
        },
        {
          "name": "out_folder",
          "explanation": "The folder where the output image chips and metadata will be stored.The folder can also be a folder URL that uses a cloud storage connection file (*.acs).",
          "datatype": "Folder"
        },
        {
          "name": "in_class_data",
          "explanation": "The training sample data in either vector or raster form. Vector inputs should follow the training sample format generated using the Training Samples Manager pane. Raster inputs should follow a classified raster format generated by the Classify Raster tool. The raster input can also be from a folder of classified rasters. Classified raster inputs require a corresponding raster attribute\r\ntable.\r\nInput tables should follow a training sample format generated by the Label Objects for Deep Learning button in the Training Samples Manager pane. Following the proper training sample format will produce optimal results with the statistical information. However, the input can also be a point feature class without a class value field or an integer raster without class information.",
          "datatype": "Feature Class; Feature Layer; Raster Dataset; Raster Layer; Mosaic Layer; Image Service; Table; Folder"
        },
        {
          "name": "image_chip_format",
          "explanation": "Specifies the raster format that will be used for the image chip outputs.The PNG and JPEG formats support up to three bands.TIFF—TIFF format will be used.PNG—PNG format will be used.JPEG—JPEG format will be used.MRF—Meta Raster Format (MRF) will be used.",
          "datatype": "String"
        },
        {
          "name": "tile_size_x(Optional)",
          "explanation": "The size of the image chips for the x dimension.",
          "datatype": "Long"
        },
        {
          "name": "tile_size_y(Optional)",
          "explanation": "The size of the image chips for the y dimension.",
          "datatype": "Long"
        },
        {
          "name": "stride_x(Optional)",
          "explanation": "The distance to move in the x direction when creating the next image chips.When stride is equal to tile size, there will be no overlap. When stride is equal to half the tile size, there will be 50 percent overlap.",
          "datatype": "Long"
        },
        {
          "name": "stride_y(Optional)",
          "explanation": "The distance to move in the y direction when creating the next image chips.When stride is equal to tile size, there will be no overlap. When stride is equal to half the tile size, there will be 50 percent overlap.",
          "datatype": "Long"
        },
        {
          "name": "output_nofeature_tiles(Optional)",
          "explanation": "Specifies whether image chips that do not capture training samples will be exported.ALL_TILES—All image chips, including those that do not capture training samples, will be exported.ONLY_TILES_WITH_FEATURES—Only image chips that capture training samples will be exported. This is the default.When set to ALL_TILES, image chips that do not capture labeled data will also be exported. When set to ONLY_TILES_WITH_FEATURES, those image chips will not be exported.",
          "datatype": "Boolean"
        },
        {
          "name": "metadata_format(Optional)",
          "explanation": "Specifies the format that will be used for the output metadata labels. If the input training sample data is a feature class layer, such as a building layer or a standard classification training sample file, use the KITTI Labels or PASCAL Visual Object Classes option (KITTI_rectangles or PASCAL_VOC_rectangles in Python). The output metadata is a .txt file or an .xml file containing the training sample data contained in the minimum bounding rectangle. The name of the metadata file matches the input source image name. If the input training sample data is a class map, use the Classified Tiles option (Classified_Tiles in Python) as the output metadata format.KITTI_rectangles—The metadata will follow the same format as the Karlsruhe Institute of Technology and Toyota Technological Institute (KITTI) Object Detection Evaluation dataset. The KITTI dataset is a vision benchmark suite. The label files are plain text files. All values, both numerical and strings, are separated by spaces, and each row corresponds to one object.This format is used for object detection.PASCAL_VOC_rectangles—The metadata will follow the same format as the Pattern Analysis, Statistical Modeling and Computational Learning, Visual Object Classes (PASCAL_VOC) dataset. The PASCAL VOC dataset is a standardized image dataset for object class recognition. The label files are in XML format and contain information about image name, class value, and bounding boxes. This format is used for object detection. This is the default.Classified_Tiles—The output will be one classified image chip per input image chip. No other metadata for each image chip is used. Only the statistics output has more information about the classes, such as class names, class values, and output statistics. This format is primarily used\r\nfor pixel classification. This format is also used for change\r\ndetection when the output is one classified image chip from two image\r\nchips.RCNN_Masks—The output will be image chips that have a mask on the areas where the sample exists. The model generates bounding boxes and segmentation masks for each instance of an object in the image. This format is based on Feature Pyramid Network (FPN) and a ResNet101 backbone in the deep learning framework model. This format is used for object detection, but it can also be used for  object tracking when the Siam Mask model type is used during training, as well as time series pixel classification when the PSETAE architecture is used.Labeled_Tiles—Each output tile will be labeled with a specific class. This format is used for object classification.MultiLabeled_Tiles—Each output tile will be labeled with one or more classes. For example, a tile may be labeled agriculture and also cloudy. This format is used for object classification.Export_Tiles—The output will be image chips with no label. This format is used\r\nfor image translation techniques, such as Pix2Pix and Super\r\nResolution.CycleGAN—The output will be image chips with no label. This format is used\r\nfor image translation technique CycleGAN, which is used to train\r\nimages that do not overlap.Imagenet—Each output tile will be labeled with a specific class. This format is used for object classification, but it can also be used for  object tracking when the Deep Sort model type is used during training.Panoptic_Segmentation—The output will be one classified image chip\r\nand one instance per input image chip. The output will also have image chips\r\nthat mask the areas where the sample exists; these image chips will be\r\nstored in a different folder.This format is used for both pixel\r\nclassification and instance segmentation, so two output labels\r\nfolders will be produced.\r\nFor the KITTI metadata format, 15 columns are created, but only 5 of them are used in the tool. The first column is the class value. The next 3 columns are skipped. Columns 5 through 8 define the minimum bounding rectangle, which is composed of four image coordinate locations: left, top, right, and bottom pixels. The minimum bounding rectangle encompasses the training chip used in the deep learning classifier. The remaining columns are not used.\r\nThe following is an example of the PASCAL_VOC_rectangles option:  &lt;?xml version=”1.0”?&gt;\n- &lt;layout&gt;\n      &lt;image&gt;000000000&lt;/image&gt;\n      &lt;object&gt;1&lt;/object&gt;\n    - &lt;part&gt;\n         &lt;class&gt;1&lt;/class&gt;\n       - &lt;bndbox&gt;\n            &lt;xmin&gt;31.85&lt;/xmin&gt;\n            &lt;ymin&gt;101.52&lt;/ymin&gt;\n            &lt;xmax&gt;256.00&lt;/xmax&gt;\n            &lt;ymax&gt;256.00&lt;/ymax&gt;\n         &lt;/bndbox&gt;\n      &lt;/part&gt;\n  &lt;/layout&gt;For more information, see the Microsoft PASCAL Visual Object Classes (VOC) Challenge paper.",
          "datatype": "String"
        },
        {
          "name": "start_index(Optional)",
          "explanation": "Legacy:This parameter has been deprecated. Use a value of 0 or # in Python.",
          "datatype": "Long"
        },
        {
          "name": "class_value_field(Optional)",
          "explanation": "The field that contains the class values. If no field is specified, the system searches for a value or classvalue field. The field should be numeric, usually an integer. If the feature does not contain a class field, the system determines that all records belong to one class.",
          "datatype": "Field"
        },
        {
          "name": "buffer_radius(Optional)",
          "explanation": "The radius of a buffer around each training sample that will be used to delineate a training sample area. This allows you to create circular polygon training samples from points.\r\nThe linear unit of the in_class_data parameter value's spatial reference is used.",
          "datatype": "Double"
        },
        {
          "name": "in_mask_polygons(Optional)",
          "explanation": "A polygon feature class that delineates the area where image chips will be created.Only image chips that fall completely within the polygons will be created.",
          "datatype": "Feature Layer"
        },
        {
          "name": "rotation_angle(Optional)",
          "explanation": "The rotation angle that will be used to generate image chips.\r\nAn image chip will first be generated with no rotation. It will then be rotated at the specified angle to create additional image chips. The image will be rotated and have a chip created until it has been fully rotated. For example, if you specify a rotation angle of 45 degrees, the tool will create eight image chips. The eight image chips will be created at the following angles: 0, 45, 90, 135, 180, 255, 270, and 315.The default rotation angle is 0, which creates one default image chip.",
          "datatype": "Double"
        },
        {
          "name": "reference_system(Optional)",
          "explanation": "Specifies the type of reference system that will be used to interpret the input image. The reference system specified must match the reference system used to train the deep learning model.MAP_SPACE—A map-based coordinate system will be used. This is the default.PIXEL_SPACE—Image space will be used, with no rotation and no distortion.",
          "datatype": "String"
        },
        {
          "name": "processing_mode(Optional)",
          "explanation": "Specifies how all raster items in a mosaic dataset or an image service will be processed. This parameter is applied when the input raster is a mosaic dataset or an image service.PROCESS_AS_MOSAICKED_IMAGE—All raster items in the mosaic dataset or image service will be mosaicked together and processed. This is the default.PROCESS_ITEMS_SEPARATELY—All raster items in the mosaic dataset or image service will be processed as separate images.",
          "datatype": "String"
        },
        {
          "name": "blacken_around_feature(Optional)",
          "explanation": "Specifies whether the pixels around each object or feature in each image tile will be masked out.This parameter only applies when the metadata_format parameter is set to Labeled_Tiles and an input feature class or classified raster has been specified.NO_BLACKEN—Pixels surrounding objects or features will not be masked out. This is the default.BLACKEN_AROUND_FEATURE—Pixels surrounding objects or features will be masked out.",
          "datatype": "Boolean"
        },
        {
          "name": "crop_mode(Optional)",
          "explanation": "Specifies whether the exported tiles will be cropped so that they are all the same size.This parameter only applies when the  metadata_format parameter is set to either Labeled_Tiles or Imagenet, and an input feature class or classified raster has been specified.FIXED_SIZE—Exported tiles will be cropped to the same size and will center on the feature. This is the default.BOUNDING_BOX—Exported tiles will be cropped so that the bounding geometry surrounds only the feature in the tile.",
          "datatype": "String"
        },
        {
          "name": "in_raster2(Optional)",
          "explanation": "An additional input  imagery source that will be used for image translation methods.This parameter is valid when the metadata_format parameter is set to Classified_Tiles, Export_Tiles, or CycleGAN.",
          "datatype": "Raster Dataset; Raster Layer; Mosaic Layer; Image Service; Map Server; Map Server Layer; Internet Tiled Layer; Folder"
        },
        {
          "name": "in_instance_data(Optional)",
          "explanation": "The training sample data collected that contains classes for instance segmentation. The input can also be a point feature class without a class value field or an integer raster without class information.This parameter is only valid when the metadata_format parameter is set to Panoptic_Segmentation.",
          "datatype": "Feature Class; Feature Layer; Raster Dataset; Raster Layer; Mosaic Layer; Image Service; Table; Folder"
        },
        {
          "name": "instance_class_value_field(Optional)",
          "explanation": "The field that contains the class values for instance segmentation. If no field is specified, the tool will use a value or class value field if one is present. If the feature does not contain a class field, the tool will determine that all records belong to one class.This parameter is only valid when the metadata_format parameter is set to Panoptic_Segmentation.",
          "datatype": "Field"
        },
        {
          "name": "min_polygon_overlap_ratio(Optional)",
          "explanation": "The minimum overlap percentage for a feature to be included in the training data. If the percentage overlap is less than the value specified, the feature will be excluded from the training chip and will not be added to the label file. The percent value is expressed as a decimal. For example, to specify an overlap of 20 percent, use a value of 0.2. The default value is 0, which means that all features will be included. This parameter improves the performance of the tool and also improves inferencing. The speed is improved since less training chips are created. The inferencing is improved since the model is trained to only detect large patches of objects and ignores small corners of features. This means less false positives will be detected, and less false positives will be removed by  the Non Maximum Suppression tool.This parameter is enabled when the in_class_data parameter value is a feature class.",
          "datatype": "Double"
        }
      ],
      "summary": "Converts labeled vector or raster data to deep learning training datasets using a remote sensing image. The output is a folder of image chips and a folder of metadata files in the specified format.",
      "extraction_date": "2025-10-01T15:13:05.279623"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Extract Features Using AI Models",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/extract-features-using-ai-models.htm",
      "parameters": [
        {
          "name": "in_raster",
          "explanation": "The input raster on which processing will be performed.If the mode parameter is specified as Only Postprocess, a raster with binary classification is required for this parameter.",
          "datatype": "Raster Layer; Raster Dataset; Mosaic Layer"
        },
        {
          "name": "mode",
          "explanation": "Specifies the mode that will be used for the processing of the input raster. Infer and Postprocess—Features will be extracted from the imagery  and postprocessed. This is the default.Only Postprocess—The input raster will be directly postprocessed. \r\nA single band raster with binary classification is required for this option.",
          "datatype": "String"
        },
        {
          "name": "out_location",
          "explanation": "The  geodatabase where the intermediate output from the models and the final postprocessed output will be stored.",
          "datatype": "Workspace"
        },
        {
          "name": "out_prefix",
          "explanation": "A prefix that will be added to the name of the outputs that will be saved to the output location. The prefix will also be used as the name of a group layer that will be used to display all outputs.",
          "datatype": "String"
        },
        {
          "name": "area_of_interest(Optional)",
          "explanation": "The geographical extent that will be used to extract features. Only features within the area of interest will be extracted.",
          "datatype": "Feature Set"
        },
        {
          "name": "pretrained_models[pretrained_models,...](Optional)",
          "explanation": "The ArcGIS pretrained models from ArcGIS Living Atlas of the World that can be used on the provided input raster.  This parameter requires an internet connection to download the pretrained models.",
          "datatype": "String"
        },
        {
          "name": "additional_models[additional_models,...](Optional)",
          "explanation": "The deep learning models that can be used on the provided input raster and the postprocessing workflow that will be used for additional model files (.dlpk and .emd). Available postprocessing workflows are as follows: Line Regularization—The postprocessing workflow  will extract line features from a single band raster with binary classification and generate a polyline feature class after refining it. This workflow also supports deep learning models that generate polyline feature classes.Parcel Regularization—The postprocessing workflow  will extract parcels from a single band raster with binary classification and generate a polygon feature class after refining it.Polygon Regularization—The postprocessing workflow will generate a polygon feature class after refining it. This workflow is only compatible with object detection models.Polygon Segmentation—The postprocessing workflow will generate a polygon feature class containing the detected objects using their centroid or bounding box. The segmentation method is specified using the prompt parameter.None—No postprocessing workflow will be applied. This is the default.",
          "datatype": "Value Table"
        },
        {
          "name": "confidence_threshold(Optional)",
          "explanation": "The minimum confidence of deep learning model that will be used when detecting objects. The value must be between 0 and 1.",
          "datatype": "Double"
        },
        {
          "name": "save_intermediate_output(Optional)",
          "explanation": "Specifies whether the intermediate outputs will be saved to the output location. The term intermediate outputs refers to the results generated after the model has been inferenced.\r\n\r\n\r\n\r\n\r\n\r\n\r\nTRUE—The intermediate outputs will be saved to the output location.FALSE—The intermediate outputs will not be saved. This is the default.",
          "datatype": "Boolean"
        },
        {
          "name": "test_time_augmentation(Optional)",
          "explanation": "Specifies whether predictions of flipped and rotated variants of the input image will be merged into the final output.\r\n\r\n\r\n\r\n\r\n\r\n\r\nTRUE—Predictions of flipped and rotated variants of the input image will be merged into the final output.FALSE—Predictions of flipped and rotated variants of the input image will not be merged into the final output. This is the default.",
          "datatype": "Boolean"
        },
        {
          "name": "buffer_distance(Optional)",
          "explanation": "The  distance that will be used to buffer  polyline  features before they are used in postprocessing. The default is 15 meters.",
          "datatype": "Linear Unit"
        },
        {
          "name": "extend_length(Optional)",
          "explanation": "The maximum distance a  line segment will be extended to an intersecting feature. The default is 25 meters.",
          "datatype": "Linear Unit"
        },
        {
          "name": "smoothing_tolerance(Optional)",
          "explanation": "The tolerance used by the Polynomial Approximation with Exponential Kernel (PAEK) algorithm. The default is 30 meters.",
          "datatype": "Linear Unit"
        },
        {
          "name": "dangle_length(Optional)",
          "explanation": "The length at which line segments that do not touch another line at both endpoints (dangles) will be trimmed. The default is 5 meters.",
          "datatype": "Linear Unit"
        },
        {
          "name": "in_road_features(Optional)",
          "explanation": "A road feature class that will be used for refining the  parcels. The input can be a polygon or polyline feature class.",
          "datatype": "Feature Layer; Feature Class"
        },
        {
          "name": "road_buffer_width(Optional)",
          "explanation": "The buffer distance that will be used for the input road features.  The default value is 5 meters for polyline features and 0 meters for polygon features.",
          "datatype": "Linear Unit"
        },
        {
          "name": "regularize_parcels(Optional)",
          "explanation": "Specifies whether extracted parcels will be normalized by eliminating undesirable artifacts in their geometry.\r\n\r\n\r\n\r\n\r\n\r\n\r\nTRUE—Extracted parcels will be normalized. This is the default.FALSE—Extracted parcels will not be normalized.",
          "datatype": "Boolean"
        },
        {
          "name": "post_processing_workflow(Optional)",
          "explanation": "Specifies the postprocessing workflow that will be used. Line Regularization—Line features will be extracted from a single band raster with binary classification and a polyline feature class will be generated after refining it. Parcel Regularization—Parcels will be extracted from a single band raster with binary classification and a polygon feature class will be generated after refining it.Polygon Regularization— A polygon feature class will be generated after refining it. This workflow is only compatible with object detection models.",
          "datatype": "String"
        },
        {
          "name": "out_features(Optional)",
          "explanation": "The feature class containing the postprocessed output.",
          "datatype": "Feature Class"
        },
        {
          "name": "parcel_tolerance(Optional)",
          "explanation": "The minimum distance between\r\ncoordinates before they are considered equal. This parameter is used to reduce slivers between extracted parcels. The default is 3 meters.",
          "datatype": "Linear Unit"
        },
        {
          "name": "regularization_method(Optional)",
          "explanation": "Specifies the regularization method that will be used in postprocessing.Right Angles—Shapes composed of 90° angles between adjoining edges will be constructed. This is the default.Right Angles and Diagonals—Shapes composed of 45° and 90° angles between adjoining edges will be constructed.Any Angles—Shapes that form any angles between adjoining edges will be constructed.Circle—The maximum distance from the boundary of the feature being processed will be used.",
          "datatype": "String"
        },
        {
          "name": "poly_tolerance(Optional)",
          "explanation": "The maximum distance that the regularized footprint can deviate from the boundary of its originating feature. The default is 1 meter.",
          "datatype": "Linear Unit"
        },
        {
          "name": "prompt(Optional)",
          "explanation": "Specifies the segmentation method that will be used when the additional_models parameter is set to Polygon Segmentation.Centroid—  The centroid of the detections will be used to indicate to the polygon segmentation model what to segment in the input raster.Bounding Box—  The bounding box of the detections will be used to indicate to the polygon segmentation model what to segment in the input raster.None—No segmentation method will be used. This is the default",
          "datatype": "String"
        },
        {
          "name": "in_features(Optional)",
          "explanation": "The feature class on which postprocessing will be performed. This parameter is only supported when the post_processing_workflow parameter is set to Line Regularization or Polygon Regularization.",
          "datatype": "Feature Layer; Feature Class"
        },
        {
          "name": "out_summary(Optional)",
          "explanation": "The table that will contain a list of outputs that were generated along with their respective paths.",
          "datatype": "Table"
        }
      ],
      "summary": "Runs one or more pretrained deep learning models on an input raster to extract features and automate the postprocessing of the inferenced outputs. Learn more about how Extract Features Using AI Models works",
      "extraction_date": "2025-10-01T15:13:08.124513"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Non Maximum Suppression",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/non-maximum-suppression.htm",
      "parameters": [
        {
          "name": "in_featureclass",
          "explanation": "The input feature class or feature layer containing overlapping or duplicate features.",
          "datatype": "Feature Class; Feature Layer"
        },
        {
          "name": "confidence_score_field",
          "explanation": "The field in the feature class that contains the confidence scores as output by the object detection method.",
          "datatype": "Field"
        },
        {
          "name": "out_featureclass",
          "explanation": "The output feature class with the duplicate features removed.",
          "datatype": "Feature Class"
        },
        {
          "name": "class_value_field(Optional)",
          "explanation": "The class value field in the input feature class. If not specified, the tool will use the standard class value fields  Classvalue and Value. If these fields do not exist, all features will be treated as the same object class.",
          "datatype": "Field"
        },
        {
          "name": "max_overlap_ratio(Optional)",
          "explanation": "The maximum overlap ratio for two overlapping features. This is defined as the ratio of intersection area over union area. The default is 0.",
          "datatype": "Double"
        }
      ],
      "summary": "Identifies duplicate features from the output of the Detect Objects Using Deep Learning tool as a postprocessing step and creates a new output with no duplicate features. The Detect Objects Using Deep Learning tool can return more than one bounding box or polygon for the same object, especially as a tiling side effect. If two features overlap more than a given maximum ratio, the feature with the lower confidence value will be removed.",
      "extraction_date": "2025-10-01T15:13:10.533525"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Train Deep Learning Model",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/train-deep-learning-model.htm",
      "parameters": [
        {
          "name": "in_folder[in_folder,...]",
          "explanation": "The folders containing the image chips, labels, and statistics required to train the model. This is the output from the Export Training Data For Deep Learning tool.Multiple input folders are supported when the following conditions are met: The metadata format type must be classified tiles, labeled tiles, multilabeled tiles, Pascal Visual Object Classes, or RCNN masks.All training data must have the same metadata format.All training data must have the same number of bands.",
          "datatype": "Folder"
        },
        {
          "name": "out_folder",
          "explanation": "The output folder location where the trained model will be stored.",
          "datatype": "Folder"
        },
        {
          "name": "max_epochs(Optional)",
          "explanation": "The maximum number of epochs for which the model will be trained. A maximum epoch of 1 means the dataset will be passed forward and backward through the neural network one time. The default value is 20.",
          "datatype": "Long"
        },
        {
          "name": "model_type(Optional)",
          "explanation": "Specifies the model type that will be used to train the deep learning model.BDCN_EDGEDETECTOR— The Bi-Directional Cascade Network (BDCN) architecture will be used to train the model. BDCN Edge Detector is used for pixel classification. This approach is useful to improve edge detection for objects at different scales.CHANGEDETECTOR—The Change detector architecture will be used to train the model. Change detector is used for pixel classification. This approach creates a model object that uses two spatial-temporal images to create a classified raster of the change. The input training data for this model type uses the Classified Tiles metadata format.CLIMAX—ClimaX is architecture will be used to train the model. This model is primarily used for weather and climate analysis. ClimaX is used for pixel classification. The preliminary data used for this method is multidimensional data.CONNECTNET—The ConnectNet architecture will be used to train the model. ConnectNet is used for pixel classification. This approach is useful for road network extraction from satellite imagery.CYCLEGAN—The CycleGAN architecture will be used to train the model. CycleGAN is used for image-to-image translation. This approach creates a model object that generates images of one type to another. This approach is unique in that the images to be trained do not need to overlap. The input training data for this model type uses the CycleGAN metadata format.DEEPLAB—The DeepLabV3 architecture will be used to train the model. DeepLab is used for pixel classification.DEEPSORT—The Deep Sort architecture will be used to train the model. Deep Sort is used for object detection in videos. The model is trained using frames of the video and detects the classes and bounding boxes of the objects in each frame. The input training data for this model type uses the Imagenet metadata format. While Siam Mask is useful for tracking an object, Deep Sort is useful in training a model to track multiple objects.DETREG—The DETReg architecture will be used to train the model. DETReg is\r\nused for object detection. The input training data for this model\r\ntype uses the Pascal Visual Object Classes. This model type is GPU intensive; it requires a dedicated GPU with at least 16 GB of memory to run properly.FASTERRCNN—The FasterRCNN architecture will be used to train the model. FasterRCNN is used for object detection.FEATURE_CLASSIFIER—The Feature classifier architecture will be used to train the model. Feature Classifier is used for object or image classification.HED_EDGEDETECTOR— The Holistically-Nested Edge Detection (HED) architecture will be used to train the model. HED Edge Detector is used for pixel classification. This approach is useful for edge and object boundary detection.IMAGECAPTIONER—The Image captioner architecture will be used to train the model. Image captioner is used for image-to-text translation. This approach creates a model that generates text captions for an image.MASKRCNN—The MaskRCNN architecture will be used to train the model. MaskRCNN is used for object detection. This approach is used for instance segmentation, which is precise delineation of objects in an image. This model type can be used to detect building footprints. It uses the MaskRCNN metadata format for training data as input. Class values for input training data must start at 1. This model type can only be trained using a CUDA-enabled GPU.MAXDEEPLAB—The MaX-DeepLab architecture will be used to train the model.\r\nMaX-DeepLab is used for panoptic segmentation. This approach\r\ncreates a model object that generates images and features. The\r\ninput training data for this model type uses the Panoptic segmentation metadata format.MMDETECTION—The MMDetection architecture will be used to train the model. MMDetection is used for object detection. The supported metadata formats are Pascal Visual Object Class rectangles and KITTI rectangles.MMSEGMENTATION—The MMSegmentation architecture will be used to train the model. MMSegmentation is used for pixel classification. The supported metadata format is Classified Tiles.MULTITASK_ROADEXTRACTOR— The Multi Task Road Extractor architecture will be used to train the model. Multi Task Road Extractor is used for pixel classification. This approach is useful for road network extraction from satellite imagery.PIX2PIX—The Pix2Pix architecture will be used to train the model. Pix2Pix is used for image-to-image translation. This approach creates a model object that generates images of one type to another. The input training data for this model type uses the Export Tiles metadata format.PIX2PIXHD—The Pix2PixHD architecture will be used to train the model. Pix2PixHD is used for image-to-image translation. This approach creates a model object that generates images of one type to another. The input training data for this model type uses the Export Tiles metadata format.PSETAE—The Pixel-Set Encoders and Temporal Self-Attention (PSETAE) architecture will be used to train the model for time series classification. PSETAE is used for pixel classification. The preliminary data used for this method is multidimensional data.PSPNET—The Pyramid Scene Parsing Network (PSPNET) architecture will be used to train the model. PSPNET is used for pixel classification.RETINANET—The RetinaNet architecture will be used to train the model. RetinaNet is used for object detection. The input training data for this model type uses the Pascal Visual Object Classes metadata format.RTDETRV2—The improved Real-Time DEtection TRansformer (RTDetrV2) architecture will be used to train the model. RTDetrV2 builds on the previous real-time detector, RT-DETR. RTDetrV2 is used for object detection. The input training data for this model type uses the Pascal Visual Object Classes and KITTI rectangles metadata formats.SAMLORA—The Segment anything model (SAM) with Low Rank Adaption (LoRA) will be used to train the model. This model type uses the SAM as a foundational model and will fine-tune to a specific task with relatively low computing requirements and a smaller dataset.SIAMMASK— The Siam Mask architecture will be used to train the model. Siam Mask is used for object detection in videos. The model is trained using frames of the video and detects the classes and bounding boxes of the objects in each frame. The input training data for this model type uses the MaskRCNN metadata format.SSD—The Single Shot Detector (SSD) architecture will be used to train the model. SSD is used for object detection. The input training data for this model type uses the Pascal Visual Object Classes metadata format.SUPERRESOLUTION—The Super-resolution architecture will be used to train the model. Super-resolution is used for image-to-image translation. This approach creates a model object that increases the resolution and improves the quality of images. The input training data for this model type uses the Export Tiles metadata format.UNET—The U-Net architecture will be used to train the model. U-Net is used for pixel classification.YOLOV3—The YOLOv3 architecture will be used to train the model. YOLOv3 is used for object detection.",
          "datatype": "String"
        },
        {
          "name": "batch_size(Optional)",
          "explanation": "The number of training samples that will be processed for training at one time.Increasing the batch size can improve tool performance; however, as the batch size increases, more memory is used. When not enough GPU memory is available for the batch size set, the tool tries to estimate and use an optimum batch size. If an out of memory error occurs, use a smaller batch size.",
          "datatype": "Long"
        },
        {
          "name": "arguments[arguments,...](Optional)",
          "explanation": "The information from the model_type parameter will be used to set the default values for this parameter. These arguments vary, depending on the model architecture. The supported model arguments for models trained in ArcGIS are described below. ArcGIS pretrained models and custom deep learning models may have additional arguments that the tool supports.For more information about which arguments are available for each model type, see Deep learning arguments.",
          "datatype": "Value Table"
        },
        {
          "name": "learning_rate(Optional)",
          "explanation": "The rate at which existing information will be overwritten with newly acquired information throughout the training process. If no value is specified, the optimal learning rate will be extracted from the learning curve during the training process.",
          "datatype": "Double"
        },
        {
          "name": "backbone_model(Optional)",
          "explanation": "Specifies the preconfigured neural network that will be used as the architecture for training the new model. This method is known as Transfer Learning.1.40625deg—This backbone was trained on imagery in which the  resolution of each grid cell covers an area of 1.40625 degrees by 1.40625 degrees. This is used for weather and climate predictions.  This is a higher resolution setting, allowing for more precise outputs but requires more computational power.5.625deg—This backbone was trained on imagery in which the  resolution of each grid cell covers an area of 5.625 degrees by 5.625 degrees. This is used for weather and climate predictions. This is considered a low-resolution setting but requires less computational power. DENSENET121—The preconfigured model will be a dense network trained on the Imagenet Dataset that contains more than 1 million\r\nimages and is 121 layers deep. Unlike ResNET, which combines the layer using summation, DenseNet combines the layers using concatenation.DENSENET161—The preconfigured model will be a dense network trained on the Imagenet Dataset that contains more than 1 million\r\nimages and is 161 layers deep. Unlike ResNET, which combines the layer using summation, DenseNet combines the layers using concatenation.DENSENET169—The preconfigured model will be a dense network trained on the Imagenet Dataset that contains more than 1 million\r\nimages and is 169 layers deep. Unlike ResNET, which combines the layer using summation, DenseNet combines the layers using concatenation.DENSENET201—The preconfigured model will be a dense network trained on the Imagenet Dataset that contains more than 1 million\r\nimages and is 201 layers deep. Unlike ResNET, which combines the layer using summation, DenseNet combines the layers using concatenation.MOBILENET_V2—The preconfigured model will be trained on the Imagenet Database and is 54 layers deep and intended for Edge device computing, since it uses less memory.RESNET18—The preconfigured model will be a residual network\r\ntrained on the Imagenet Dataset that contains more than 1 million\r\nimages and is 18 layers deep.RESNET34—The preconfigured model will be a residual network\r\ntrained on the Imagenet Dataset that contains more than 1 million\r\nimages and is 34 layers deep. This is the default.RESNET50—The preconfigured model will be a residual network\r\ntrained on the Imagenet Dataset that contains more than 1 million\r\nimages and is 50 layers deep.RESNET101—The preconfigured model will be a residual network\r\ntrained on the Imagenet Dataset that contains more than 1 million\r\nimages and is 101 layers deep.RESNET152—The preconfigured model will be a residual network\r\ntrained on the Imagenet Dataset that contains more than 1 million\r\nimages and is 152 layers deep.VGG11—The preconfigured model will be a convolution neural network trained on the Imagenet Dataset that contains more than 1 million\r\nimages to classify images into 1,000 object categories and is 11 layers deep.VGG11_BN—The preconfigured model will be based on the VGG network but with batch normalization, which means each layer in the network is normalized. It trained on the Imagenet dataset and has 11 layers.VGG13—The preconfigured model will be a convolution neural network trained on the Imagenet Dataset that contains more than 1 million\r\nimages to classify images into 1,000 object categories and is 13 layers deep.VGG13_BN—The preconfigured model will be based on the VGG network but with batch normalization, which means each layer in the network is normalized. It trained on the Imagenet dataset and has 13 layers.VGG16—The preconfigured model will be a convolution neural network trained on the Imagenet Dataset that contains more than 1 million\r\nimages to classify images into 1,000 object categories and is 16 layers deep.VGG16_BN—The preconfigured model will be based on the VGG network but with batch normalization, which means each layer in the network is normalized. It trained on the Imagenet dataset and has 16 layers.VGG19—The preconfigured model will be a convolution neural network trained on the Imagenet Dataset that contains more than 1 million\r\nimages to classify images into 1,000 object categories and is 19 layers deep.VGG19_BN—The preconfigured model will be based on the VGG network but with batch normalization, which means each layer in the network is normalized. It trained on the Imagenet dataset and has 19 layers.DARKNET53—The preconfigured model will be a convolution neural network trained on the Imagenet Dataset that contains more than 1 million images and is 53 layers deep. REID_V1—The preconfigured model will be a convolution neural network trained on the Imagenet Dataset that is used for object tracking.REID_V2—The preconfigured model will be a convolution neural network trained on the Imagenet Dataset that is used for object tracking.RESNEXT50—The preconfigured model will be a convolution neural network trained on the Imagenet Dataset and is 50 layers deep. It is a homogeneous neural network, which reduces the number of hyperparameters required by conventional ResNet. WIDE_RESNET50—The preconfigured model will be a convolution neural network trained on the Imagenet Dataset and is 50 layers deep. It has the same architecture as ResNET but with more channels.SR3—The preconfigured model will use the Super Resolution via Repeated Refinement (SR3) model. SR3 adapts denoising diffusion probabilistic models to conditional image generation and performs super-resolution through a stochastic denoising process. For more information, see Image Super-Resolution via Iterative Refinement on the arXiv site.  SR3_UVIT—This backbone model refers to a specific implementation of Vision Transformer (ViT)-based architecture designed for  diffusion models within  image generation and SR3 tasks.VIT_B—The preconfigured Segment Anything Model (SAM) will be used with a base neural network size. This is the smallest size. For more information, see Segment Anything on the arXiv site.VIT_L—The preconfigured Segment Anything Model (SAM) will be used with a large neural network size. For more information, see Segment Anything on the arXiv site.VIT_H—The preconfigured Segment Anything Model (SAM) will be used with a huge neural network size. This is the largest size. For more information, see Segment Anything on the arXiv site.Additionally, supported convolution neural networks from the PyTorch Image Models (timm) can be specified using timm as a prefix, for example, timm:resnet31 , timm:inception_v4 , timm:efficientnet_b3, and so on.",
          "datatype": "String"
        },
        {
          "name": "pretrained_model(Optional)",
          "explanation": "A pretrained model that will be used to fine-tune the new model. The input is an Esri model definition file (.emd) or a deep learning package file (.dlpk).A pretrained model with similar classes can be fine-tuned to fit the new model. The pretrained model must have been trained with the same model type and backbone model that will be used to train the new model. Fine-tuning is only supported for models that have been trained using ArcGIS.",
          "datatype": "File"
        },
        {
          "name": "validation_percentage(Optional)",
          "explanation": "The percentage of training samples that will be used for validating the model. The default value is 10.",
          "datatype": "Double"
        },
        {
          "name": "stop_training(Optional)",
          "explanation": "Specifies whether early stopping will be implemented.STOP_TRAINING—Early stopping will be implemented, and the model training will stop when the model is no longer improving, regardless of the max_epochs parameter value specified. This is the default.CONTINUE_TRAINING—Early stopping will not be implemented, and the model training will continue until the max_epochs parameter value is reached.",
          "datatype": "Boolean"
        },
        {
          "name": "freeze(Optional)",
          "explanation": "Specifies whether the backbone layers in the pretrained model will be frozen, so that the weights and biases remain as originally designed. FREEZE_MODEL—The backbone layers will be frozen, and the predefined weights and biases will not be altered in the backbone_model parameter. This is the default.UNFREEZE_MODEL—The backbone layers will not be frozen, and the weights and biases of the backbone_model parameter can be altered to fit the training samples. This takes more time to process but typically produces better results.",
          "datatype": "Boolean"
        },
        {
          "name": "augmentation(Optional)",
          "explanation": "Specifies the type of data augmentation that will be used.Data augmentation is a technique of artificially increasing the training set by creating modified copies of a dataset using existing data.DEFAULT—The default data augmentation methods and values will be used.The default data augmentation methods are crop, dihedral_affine, brightness, contrast, and zoom. These default values typically work well for satellite imagery.NONE—No data augmentation will be used.CUSTOM—Data augmentation values will be specified using the augmentation_parameters parameter. This enables direct control over the crop, rotate, brightness, contrast, and zoom transformations.FILE—Fastai transforms for data augmentation of training and validation datasets will be specified using the transforms.json file, which is in the same folder as the training data For more information about the various\r\ntransformations, see vision transforms on the fastai website.",
          "datatype": "String"
        },
        {
          "name": "augmentation_parameters[augmentation_parameters,...](Optional)",
          "explanation": "Specifies the value for each transform in the augmentation parameter.\r\nrotate—The image will be randomly rotated (in degrees) by a probability (p). If degrees is a range (a,b), a value will be uniformly assigned from a to b. The default value is 30.0; 0.5.brightness—The brightness of the image will be randomly adjusted depending on the value of change, with a probability (p). A change of 0 will transform the image to darkest, and a change of 1 will transform the image to lightest. A change of 0.5 will not adjust the brightness. If change is a range (a,b), the augmentation will uniformly assign a value from a to b. The default value is (0.4,0.6); 1.0. contrast—The contrast of the image will be randomly adjusted depending on the value of scale, with a probability (p). A scale of 0 will transform the image to gray scale, and a scale greater than 1 will transform the image to super contrast. A scale of 1 doesn't adjust the contrast. If scale is a range (a,b), the augmentation will uniformly assign a value from a to b. The default value is (0.75, 1.5); 1.0.zoom—The image will be randomly zoomed in depending on the value of scale. The zoom value is in the form scale(a,b); p. The default value is (1.0, 1.2); 1.0 in which p is the probability. Only a scale of greater than 1.0 will zoom in on the image. If scale is a range (a,b), it will uniformly assign a value from a to b.crop—The image will be randomly cropped.  The crop value is in the form size;p;row_pct;col_pct in which p is probability. The position is given by (col_pct, row_pct), with col_pct and row_pct being normalized between 0 and 1. If col_pct or row_pct is a range (a,b), it will uniformly assign a value from a to b. The default value is chip_size;1.0; (0, 1); (0, 1) in which 224 is the default chip size.",
          "datatype": "Value Table"
        },
        {
          "name": "chip_size(Optional)",
          "explanation": "The size of the image that will be used to train the model. Images will be cropped to the specified chip size.The default chip size will be the same as the tile size of the training data. If the x- and y- tile size are different, the smaller value will be used as the default chip size. The chip size should be less than the smallest x- or y- tile size of all images in the input folders.",
          "datatype": "Long"
        },
        {
          "name": "resize_to(Optional)",
          "explanation": "Resizes the image chips. Once a chip is resized, pixel blocks of chip size will be cropped and used for training. This parameter applies to object detection (PASCAL VOC), object classification (labeled tiles), and super-resolution data only.The resize value is often half the chip size value. If the resize value is less than the chip size value, the resize value is used to create the pixel blocks for training.",
          "datatype": "String"
        },
        {
          "name": "weight_init_scheme(Optional)",
          "explanation": "Specifies the scheme in which the weights will be initialized for the layer. To train a model with multispectral data, the model must accommodate the various types of bands available. This is done by reinitializing the first layer in the model. RANDOM—Random weights will be initialized\r\nfor non-RGB bands, while pretrained weights will be preserved for RGB\r\nbands. This is the default.RED_BAND—Weights corresponding to the red band from the\r\npretrained model's layer will be cloned for non-RGB bands, while\r\npretrained weights will be preserved for RGB bands.ALL_RANDOM— Random weights will be initialized for RGB bands as well\r\nas non-RGB bands. This option applies only to multispectral imagery.This parameter is only applicable when multispectral imagery is used in the model.",
          "datatype": "String"
        },
        {
          "name": "monitor(Optional)",
          "explanation": "Specifies the metric that will be monitored while checkpointing and early stopping.VALID_LOSS—The validation loss will be monitored. When the validation loss no longer changes significantly, the model will stop. This is the default.AVERAGE_PRECISION—The weighted mean of precision at each threshold will be monitored. When this value no longer changes significantly, the model will stop.ACCURACY—The ratio between the number of correct predictions to the total number of predictions will be monitored. When this value no longer changes significantly, the model will stop.F1_SCORE—The combination of the precision and recall scores of the model will be monitored. When this value no longer changes significantly, the model will stop.MIOU—The average between the intersection over union (IoU) of the segmented objects over all the images of the test dataset will be monitored. When this value no longer changes significantly, the model will stop.DICE—Model performance will be monitored using the Dice metric. When this value no longer changes significantly, the model will stop.This value can range from 0 to 1. The value 1 corresponds to a pixel perfect match between the validation data and training data.PRECISION—The precision, which measures the model's accuracy in classifying a sample as positive, will be monitored. When this value no longer changes significantly, the model will stop.The precision is the ratio between the number of positive samples correctly classified and the total number of samples classified (either correctly or incorrectly).RECALL—The recall, which measures the model's ability to detect positive samples, will be monitored. When this value no longer changes significantly, the model will stop. The higher the recall, the more positive samples are detected. The recall value is the ratio between the number of positive samples correctly classified as positive and the total number of positive samples. CORPUS_BLEU—The Corpus blue score will be monitored. When this value no longer changes significantly, the model will stop.This score is used to calculate accuracy for multiple sentences, such as a paragraph or a document. MULTI_LABEL_FBETA—The weighted harmonic mean of precision and recall will be monitored. When this value no longer changes significantly, the model will stop.This is often referred to as the F-beta score.",
          "datatype": "String"
        },
        {
          "name": "tensorboard(Optional)",
          "explanation": "Specifies whether Tensorboard metrics will be enabled while the tool is training. \r\nTensorboard can be accessed using the URL in the tool messages.This parameter is only supported for the following models: CycleGAN, DeepLab, Faster RCNN, Feature Classifier, Image Captioner, Mask RCNN, Multi-Task Road Extractor, Pix2Pix, PSPNet Classifier, RetinaNet, Single-Shot Detector, SuperResolution, and U-Net Classifier.DISABLE_TENSORBOARD—Tensorboard metrics will not be enabled. This is the default.ENABLE_TENSORBOARD—Tensorboard metrics will be enabled.",
          "datatype": "Boolean"
        }
      ],
      "summary": "Trains a deep learning model using the output from the Export Training Data For Deep Learning tool.",
      "extraction_date": "2025-10-01T15:13:13.779946"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Train Using AutoDL",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/train-using-autodl.htm",
      "parameters": [
        {
          "name": "in_data",
          "explanation": "The folders containing the image chips, labels, and statistics required to train the model. This is the output from the Export Training Data For Deep Learning tool.   The metadata format of the exported data  must be  Classified_Tiles, PASCAL_VOC_rectangles, or  KITTI_rectangles.",
          "datatype": "Folder"
        },
        {
          "name": "out_model",
          "explanation": "The output trained model that will be saved as a deep learning package (.dlpk file).",
          "datatype": "File"
        },
        {
          "name": "pretrained_model(Optional)",
          "explanation": "A pretrained model that will be used to fine-tune the new model. The input is an Esri model definition file (.emd) or a deep learning package file (.dlpk).\r\nA pretrained model with similar classes can be fine-tuned to fit the new model. The pretrained model must have been trained with the same model type and backbone model that will be used to train the new model.",
          "datatype": "File"
        },
        {
          "name": "total_time_limit(Optional)",
          "explanation": "The total time limit in hours it will take for AutoDL model training. The default is 2 hours.",
          "datatype": "Double"
        },
        {
          "name": "autodl_mode(Optional)",
          "explanation": "Specifies the AutoDL mode that will be used and how intensive the AutoDL\r\nsearch will be.\r\n\r\n\r\n\r\n\r\n\r\n\r\nBASIC—The basic mode will be used. This mode is used to train all selected networks without hyperparameter tuning.  ADVANCED— The advanced mode will be used. This mode is used to perform hyperparameter tuning on  the top two  performing models.",
          "datatype": "String"
        },
        {
          "name": "networks[networks,...](Optional)",
          "explanation": "Specifies the architectures that will be used to train the model. \r\nSingleShotDetector—The SingleShotDetector architecture will be used to train the model. SingleShotDetector is used for object detection.RetinaNet—The RetinaNet architecture will be used to train the model. RetinaNet is used for object detection.FasterRCNN—The FasterRCNN architecture will be used to train the model. FasterRCNN is used for object detection.YOLOv3—The YOLOv3 architecture will be used to train the model. YOLOv3 is used for object detection.HRNet—The HRNet architecture will be used to train the model. HRNet is used for pixel classification.ATSS—The ATSS architecture will be used to train the model. ATSS is used for object detection.CARAFE—The CARAFE architecture will be used to train the model. CARAFE is used for object detection.CascadeRCNN—The CascadeRCNN architecture will be used to train the model. CascadeRCNN is used for object detection.CascadeRPN—The CascadeRPN architecture will be used to train the model. CascadeRPN is used for object detection.DCN—The DCN architecture will be used to train the model. DCN is used for object detection.DeepLab—The DeepLab architecture will be used to train the model. DeepLab is used for pixel classification.UnetClassifier—The UnetClassifier architecture will be used to train the model. UnetClassifier is used for pixel classification.DeepLabV3Plus—The DeepLabV3Plus architecture will be used to train the model. DeepLabV3Plus is used for pixel classification.PSPNetClassifier—The PSPNetClassifier architecture will be used to train the model. PSPNetClassifier is used for pixel classification.ANN—The ANN architecture will be used to train the model. ANN is used for pixel classification.APCNet—The APCNet architecture will be used to train the model. APCNet is used for pixel classification.CCNet—The CCNet architecture will be used to train the model. CCNet is used for pixel classification.CGNet—The CGNet architecture will be used to train the model. CGNet is used for pixel classification.DETReg—The DETReg architecture will be used to train the model. DETReg is used for object detection.DynamicRCNN—The DynamicRCNN architecture will be used to train the model. DynamicRCNN is used for object detection.EmpiricalAttention—The EmpiricalAttention architecture will be used to train the model. EmpiricalAttention is used for object detection.FCOS—The FCOS architecture will be used to train the model. FCOS is used for object detection.FoveaBox—The FoveaBox architecture will be used to train the model. FoveaBox is used for object detection.FSAF—The FSAF architecture will be used to train the model. FSAF is used for object detection.GHM—The GHM architecture will be used to train the model. GHM is used for object detection.LibraRCNN—The LibraRCNN architecture will be used to train the model. LibraRCNN is used for object detection.PaFPN—The PaFPN architecture will be used to train the model. PaFPN is used for object detection.Res2Net—The Res2Net architecture will be used to train the model. Res2Net is used for object detection.SABL—The SABL architecture will be used to train the model. SABL is used for object detection.VFNet—The VFNet architecture will be used to train the model. VFNet is used for object detection.DMNet—The DMNet architecture will be used to train the model. DMNet is used for pixel classification.DNLNet—The DNLNet architecture will be used to train the model. DNLNet is used for pixel classification.FastSCNN—The FastSCNN architecture will be used to train the model. FastSCNN is used for pixel classification.FCN—The FCN architecture will be used to train the model. FCN is used for pixel classification.GCNet—The GCNet architecture will be used to train the model. GCNet is used for pixel classification.MobileNetV2—The MobileNetV2 architecture will be used to train the model. MobileNetV2 is used for pixel classification.NonLocalNet—The NonLocalNet architecture will be used to train the model. NonLocalNet is used for pixel classification.Mask2Former—The  Mask2Former architecture will be used to train the model. Mask2Former is used for pixel classification.PSANet—The PSANet architecture will be used to train the model. PSANet is used for pixel classification.SemFPN—The SemFPN architecture will be used to train the model. SemFPN is used for pixel classification.UperNet—The UperNet architecture will be used to train the model. UperNet is used for pixel classification.MaskRCNN—The MaskRCNN architecture will be used to train the model. MaskRCNN is used for object detection.SamLoRA—The SamLoRA architecture will be used to train the model. SamLoRA is used for pixel classification.RTDetrV2—The RTDetrV2 architecture will be used to train the model. RTDetrV2 is used for object detection.By default, all the networks will be used.",
          "datatype": "String"
        },
        {
          "name": "save_evaluated_models(Optional)",
          "explanation": "Specifies whether all evaluated models will be saved.SAVE_ALL_MODELS— All evaluated models will be saved.SAVE_BEST_MODEL—Only the best performing model will be saved. This is the default.",
          "datatype": "Boolean"
        }
      ],
      "summary": "Trains a deep learning model by building training pipelines and automating much of the training process.  This includes data augmentation, model selection, hyperparameter tuning, and batch size deduction. Its outputs include performance metrics of the best model on the training data, as well as the trained  deep learning model package (.dlpk file) that can be used as input for the Extract Features Using AI Models tool to predict on new imagery. Learn more about how AutoDL works",
      "extraction_date": "2025-10-01T15:13:16.468353"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Deep learning model architectures",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/overview-of-the-deep-learning-models.htm",
      "parameters": [],
      "summary": "",
      "extraction_date": "2025-10-01T15:13:19.002223"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Install deep learning frameworks for ArcGIS",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/help/analysis/deep-learning/install-deep-learning-frameworks.htm",
      "parameters": [],
      "summary": "",
      "extraction_date": "2025-10-01T15:13:21.204682"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Deep learning using the ArcGIS Image Analyst extension",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/help/analysis/image-analyst/deep-learning-in-arcgis-pro.htm",
      "parameters": [],
      "summary": "",
      "extraction_date": "2025-10-01T15:13:23.629621"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Sample",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/sample.htm",
      "parameters": [
        {
          "name": "in_rasters[in_raster,...]",
          "explanation": "The rasters with values that will be sampled based on the input location data.\r\nThe process_as_multidimensional parameter is only supported when the input is a single, multidimensional raster.",
          "datatype": "Raster Layer"
        },
        {
          "name": "in_location_data",
          "explanation": "The data identifying positions where a sample will be taken.The input can be a raster or a feature class.",
          "datatype": "Raster Layer; Feature Layer"
        },
        {
          "name": "out_table",
          "explanation": "The output table or feature class containing the sampled cell values.The output format is determined by the output location and path. By default, the output will be a geodatabase table or a geodatabase feature class in a geodatabase workspace or a dBASE table or a shapefile feature class in a folder workspace.The output data type to generate a table or a feature class is controlled by the generate_feature_class parameter.",
          "datatype": "Table; Point feature class"
        },
        {
          "name": "resampling_type(Optional)",
          "explanation": "The resampling algorithm that will be used to sample a raster to determine how the values will be obtained from the raster.\r\nNEAREST—Nearest neighbor assignment will be used. This is the default.BILINEAR—Bilinear interpolation will be used.CUBIC—Cubic convolution will be used.",
          "datatype": "String"
        },
        {
          "name": "unique_id_field(Optional)",
          "explanation": "A field containing a different value for every location or feature in the input location raster or features.",
          "datatype": "Field"
        },
        {
          "name": "process_as_multidimensional(Optional)",
          "explanation": "Specifies how the input rasters will be processed.This parameter is only available when the input is a single, multidimensional raster.ALL_SLICES—Samples will be processed for all dimensions (such as time or depth) of a multidimensional dataset.CURRENT_SLICE—Samples will be processed from the current slice of a multidimensional dataset. This is the default.",
          "datatype": "Boolean"
        },
        {
          "name": "acquisition_definition[acquisition_definition,...](Optional)",
          "explanation": "Specifies the time, depth, or other acquisition data associated with the location features.Only the following combinations are supported:\r\nDimension + Start field or valueDimension + Start field or value + End field or valueDimension + Start field or value + Relative value or days before + Relative value or days after\r\nRelative value or days before and Relative value or days after only support nonnegative values.\r\nStatistics will be calculated using the statistics_type parameter for variables within this dimension range.",
          "datatype": "Value Table"
        },
        {
          "name": "statistics_type(Optional)",
          "explanation": "Specifies the statistic type to be calculated.MINIMUM—The minimum value within the specified range will be calculated.MAXIMUM—The maximum value within the specified range will be calculated.MEDIAN—The median value within the specified range will be calculated.MEAN—The average for the specified range will be calculated. SUM—The total value of the variables within the specified range will be calculated.MAJORITY—The value that occurs most frequently will be calculated.MINORITY—The value that occurs least frequently will be calculated.STD—The standard deviation will be calculated.PERCENTILE—A defined percentile within the specified range will be calculated.",
          "datatype": "String"
        },
        {
          "name": "percentile_value(Optional)",
          "explanation": "The percentile to calculate when the Statistics Type parameter is set to Percentile.\r\nThe percentile to calculate when the statistics_type parameter is set to PERCENTILE.This value can range from 0 to 100. The default is 90.",
          "datatype": "Double"
        },
        {
          "name": "buffer_distance(Optional)",
          "explanation": "The distance around the location data features. The buffer distance is specified in the linear unit of the location feature's spatial reference. If the feature uses a geographic reference, the unit will be degrees.Statistics will be calculated within this buffer area.",
          "datatype": "Double; Field"
        },
        {
          "name": "layout(Optional)",
          "explanation": "Specifies whether sampled values will appear in rows or columns in the output table.ROW_WISE—Sampled values will appear in separate rows in the output table. This is the default.COLUMN_WISE—Sampled values will appear in separate columns in the output table. This option is only valid when the input multidimensional raster contains one variable and one dimension, and each slice is a single-band raster.",
          "datatype": "Boolean"
        },
        {
          "name": "generate_feature_class(Optional)",
          "explanation": "Specifies whether a point feature class with sampled values in its attribute table or a table with sampled values will be generated.TABLE—A table with sampled values will be generated. This is the default.FEATURE_CLASS—A point feature class with sampled values in its attribute table will be generated.",
          "datatype": "Boolean"
        }
      ],
      "summary": "Creates a table or a point feature class that shows the values of cells from a raster, or a set of rasters, for defined locations. The locations are defined by raster cells, points, polylines, or polygons. Learn more about how Sample works",
      "extraction_date": "2025-10-01T15:13:28.347445"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Interpolate From Spatiotemporal Points",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/interpolate-from-spatiotemporal-points.htm",
      "parameters": [
        {
          "name": "in_dataset",
          "explanation": "The input point layer, trajectory layer, or trajectory dataset.",
          "datatype": "Trajectory Layer; Feature Layer; Mosaic Dataset; Mosaic Layer"
        },
        {
          "name": "variable_field",
          "explanation": "A field containing variable values.",
          "datatype": "String"
        },
        {
          "name": "time_field",
          "explanation": "A field containing time values.",
          "datatype": "String"
        },
        {
          "name": "temporal_aggregation(Optional)",
          "explanation": "Specifies the temporal aggregation of the output multidimensional raster. The interpolation algorithm uses all available data within these time periods to calculate the output slice.DAILY—The data values will be aggregated into daily time steps. This is the default.WEEKLY—The data values will be aggregated into weekly time steps.MONTHLY—The data values will be aggregated into monthly time steps.QUARTERLY—The data values will be aggregated into quarterly time steps.YEARLY—The data values will be aggregated into yearly time steps.",
          "datatype": "String"
        },
        {
          "name": "cell_size(Optional)",
          "explanation": "The output cell size. By default, the cell size will be the shorter of the width or the height of the input point feature extent, divided by 250.",
          "datatype": "Double"
        },
        {
          "name": "interpolation_method",
          "explanation": "Specifies the interpolation method that will be used.\r\nIDW—Inverse distance weighted interpolation will be used.TRIANGULATION—Triangulation interpolation will be used.MEAN—Mean interpolation will be used.MEDIAN— Median interpolation will be used.NATURAL_NEIGHBOR—Natural neighbor interpolation will be used.NEAREST_NEIGHBOR—Nearest neighbor interpolation will be used.QUADRATIC—Quadratic interpolation will be used.",
          "datatype": "String"
        }
      ],
      "summary": "Interpolates temporal point data into a multidimensional raster.",
      "extraction_date": "2025-10-01T15:13:32.514815"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Optimal Interpolation",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/optimal-interpolation.htm",
      "parameters": [
        {
          "name": "in_background_raster",
          "explanation": "The input background raster also known as the background field.",
          "datatype": "Raster Dataset; Raster Layer; Image Service"
        },
        {
          "name": "in_obs_data",
          "explanation": "The input point features that will be used for interpolation.",
          "datatype": "Feature Layer; Trajectory Layer"
        },
        {
          "name": "obs_field",
          "explanation": "The field containing observation values that will be used for interpolation.",
          "datatype": "String"
        },
        {
          "name": "background_error_var",
          "explanation": "The error variance of the background measurements. The input can be a single value or an error variance raster. If a single value is provided, the value will be used as the error variance for all background measurements. If an error variance raster is provided, each cell in the background data will obtain its error variance from the corresponding background error variance raster. The error variance raster must have the same cell size and extent as the background data.",
          "datatype": "Double; Raster Dataset; Raster Layer; Image Service"
        },
        {
          "name": "obs_error_var",
          "explanation": "The error variance of the observations. The input can be a single value or a field from the observation data. If a single value is provided, the value will be used as the error variance for all observations. If a field in the observation data is provided, values in the field will be used as the error variance for each corresponding observation point.",
          "datatype": "Double; String"
        },
        {
          "name": "background_error_corr_length(Optional)",
          "explanation": "The correlation length between background measurements. The default is three times the cell size of the in_background_raster parameter value.",
          "datatype": "Double"
        }
      ],
      "summary": "Statistically assimilates data combined from multiple sources to produce an output raster. The tool can be used to merge background data, such as model outputs, with observation data, such as point measurements, to perform interpolation. Learn more about how Optimal Interpolation works.",
      "extraction_date": "2025-10-01T15:13:34.982568"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Raster Calculator",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/raster-calculator.htm",
      "parameters": [
        {
          "name": "expression",
          "explanation": "Note:In Python, create and run map algebra expressions using the Image Analyst module, which is an extension of the ArcPy Python site package.See Map algebra to learn how to perform an analysis in Python.",
          "datatype": "Raster Calculator Expression"
        },
        {
          "name": "output_raster",
          "explanation": "Note:See Create output for information about producing output from map algebra expressions in Python.",
          "datatype": "Raster Dataset"
        }
      ],
      "summary": "Build and run a single map algebra expression using Python syntax. Learn more about how Raster Calculator works",
      "extraction_date": "2025-10-01T15:13:39.405509"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "A quick tour of using map algebra in Image Analyst",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/a-quick-tour-of-using-map-algebra-in-image-analyst.htm",
      "parameters": [],
      "summary": "",
      "extraction_date": "2025-10-01T15:13:41.795382"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Conditional",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/an-overview-of-the-conditional-math-tools-in-image-analyst.htm",
      "parameters": [],
      "summary": "",
      "extraction_date": "2025-10-01T15:13:45.741339"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Logical",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/an-overview-of-the-logical-math-tools-in-image-analyst.htm",
      "parameters": [],
      "summary": "",
      "extraction_date": "2025-10-01T15:13:48.295646"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Trigonometric",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/an-overview-of-the-trigonometric-math-tools-in-image-analyst.htm",
      "parameters": [],
      "summary": "",
      "extraction_date": "2025-10-01T15:13:50.661357"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Abs",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/abs.htm",
      "parameters": [
        {
          "name": "in_raster_or_constant",
          "explanation": "The input raster for which to calculate the absolute values.To use a number as an input for this parameter, the cell size and extent must first be set in the environment.",
          "datatype": "Raster Layer; Constant"
        }
      ],
      "summary": "Calculates the absolute value of the cells in a raster.",
      "extraction_date": "2025-10-01T15:13:53.255929"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Divide",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/divide.htm",
      "parameters": [
        {
          "name": "in_raster_or_constant1",
          "explanation": "The input whose values will be divided by the second input.A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To specify a number for both inputs, the cell size and extent must first be set in the environment.",
          "datatype": "Raster Layer; Constant"
        },
        {
          "name": "in_raster_or_constant2",
          "explanation": "The input whose values the first input are to be divided by.A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To specify a number for both inputs, the cell size and extent must first be set in the environment.",
          "datatype": "Raster Layer; Constant"
        }
      ],
      "summary": "Divides the values of two rasters on a cell-by-cell basis.",
      "extraction_date": "2025-10-01T15:13:55.706493"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Exp",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/exp.htm",
      "parameters": [
        {
          "name": "in_raster_or_constant",
          "explanation": "The input values for which to find the base e exponential.To use a number as an input for this parameter, the cell size and extent must first be set in the environment.",
          "datatype": "Raster Layer; Constant"
        }
      ],
      "summary": "Calculates the base e exponential of the cells in a raster.",
      "extraction_date": "2025-10-01T15:13:58.104426"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Exp10",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/exp10.htm",
      "parameters": [
        {
          "name": "in_raster_or_constant",
          "explanation": "The input values for which to find the base 10 exponential.To use a number as an input for this parameter, the cell size and extent must first be set in the environment.",
          "datatype": "Raster Layer; Constant"
        }
      ],
      "summary": "Calculates the base 10 exponential of the cells in a raster.",
      "extraction_date": "2025-10-01T15:14:00.535533"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Exp2",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/exp2.htm",
      "parameters": [
        {
          "name": "in_raster_or_constant",
          "explanation": "The input values for which to find the base 2 exponential.To use a number as an input for this parameter, the cell size and extent must first be set in the environment.",
          "datatype": "Raster Layer; Constant"
        }
      ],
      "summary": "Calculates the base 2 exponential of the cells in a raster.",
      "extraction_date": "2025-10-01T15:14:03.272228"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Float",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/float.htm",
      "parameters": [
        {
          "name": "in_raster_or_constant",
          "explanation": "The input raster to be converted to floating point.To use a number as an input for this parameter, the cell size and extent must first be set in the environment.",
          "datatype": "Raster Layer; Constant"
        }
      ],
      "summary": "Converts each cell value of a raster into a floating-point representation.",
      "extraction_date": "2025-10-01T15:14:05.808963"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Int",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/int.htm",
      "parameters": [
        {
          "name": "in_raster_or_constant",
          "explanation": "The input raster to be converted to integer.To use a number as an input for this parameter, the cell size and extent must first be set in the environment.",
          "datatype": "Raster Layer; Constant"
        }
      ],
      "summary": "Converts each cell value of a raster to an integer by truncation.",
      "extraction_date": "2025-10-01T15:14:08.335587"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Ln",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/ln.htm",
      "parameters": [
        {
          "name": "in_raster_or_constant",
          "explanation": "Input values for which to find the natural logarithm (Ln).To use a number as an input for this parameter, the cell size and extent must first be set in the environment.",
          "datatype": "Raster Layer; Constant"
        }
      ],
      "summary": "Calculates the natural logarithm (base e) of cells in a raster.",
      "extraction_date": "2025-10-01T15:14:10.716573"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Log10",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/log10.htm",
      "parameters": [
        {
          "name": "in_raster_or_constant",
          "explanation": "Input values for which to find the base 10 logarithm.To use a number as an input for this parameter, the cell size and extent must first be set in the environment.",
          "datatype": "Raster Layer; Constant"
        }
      ],
      "summary": "Calculates the base 10 logarithm of cells in a raster.",
      "extraction_date": "2025-10-01T15:14:13.167705"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Log2",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/log2.htm",
      "parameters": [
        {
          "name": "in_raster_or_constant",
          "explanation": "Input values for which to find the base 2 logarithm.To use a number as an input for this parameter, the cell size and extent must first be set in the environment.",
          "datatype": "Raster Layer; Constant"
        }
      ],
      "summary": "Calculates the base 2 logarithm of cells in a raster.",
      "extraction_date": "2025-10-01T15:14:15.595136"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Minus",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/minus.htm",
      "parameters": [
        {
          "name": "in_raster_or_constant1",
          "explanation": "The input from which to subtract the values in the second input.A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To specify a number for both inputs, the cell size and extent must first be set in the environment.",
          "datatype": "Raster Layer; Constant"
        },
        {
          "name": "in_raster_or_constant2",
          "explanation": "The input values to subtract from the values in the first input.A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To specify a number for both inputs, the cell size and extent must first be set in the environment.",
          "datatype": "Raster Layer; Constant"
        }
      ],
      "summary": "Subtracts the value of the second input raster from the value of the first input raster on a cell-by-cell basis.",
      "extraction_date": "2025-10-01T15:14:18.052672"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Mod",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/mod.htm",
      "parameters": [
        {
          "name": "in_raster_or_constant1",
          "explanation": "The numerator input.A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To specify a number for both inputs, the cell size and extent must first be set in the environment.",
          "datatype": "Raster Layer; Constant"
        },
        {
          "name": "in_raster_or_constant2",
          "explanation": "The denominator input.A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To specify a number for both inputs, the cell size and extent must first be set in the environment.",
          "datatype": "Raster Layer; Constant"
        }
      ],
      "summary": "Finds the remainder (modulo) of the first raster when divided by the second raster on a cell-by-cell basis.",
      "extraction_date": "2025-10-01T15:14:20.609616"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Negate",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/negate.htm",
      "parameters": [
        {
          "name": "in_raster_or_constant",
          "explanation": "The input raster to be negated (multiplied by -1).To use a number as an input for this parameter, the cell size and extent must first be set in the environment.",
          "datatype": "Raster Layer; Constant"
        }
      ],
      "summary": "Changes the sign (multiplies by -1) of the cell values of the input raster on a cell-by-cell basis.",
      "extraction_date": "2025-10-01T15:14:23.014920"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Plus",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/plus.htm",
      "parameters": [
        {
          "name": "in_raster_or_constant1",
          "explanation": "The input whose values will be added to.A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To specify a number for both inputs, the cell size and extent must first be set in the environment.",
          "datatype": "Raster Layer; Constant"
        },
        {
          "name": "in_raster_or_constant2",
          "explanation": "The input whose values will be added to the first input.A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To specify a number for both inputs, the cell size and extent must first be set in the environment.",
          "datatype": "Raster Layer; Constant"
        }
      ],
      "summary": "Adds (sums) the values of two rasters on a cell-by-cell basis.",
      "extraction_date": "2025-10-01T15:14:25.425221"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Power",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/power.htm",
      "parameters": [
        {
          "name": "in_raster_or_constant1",
          "explanation": "The input values to be raised to the power defined by the second input.A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To specify a number for both inputs, the cell size and extent must first be set in the environment.",
          "datatype": "Raster Layer; Constant"
        },
        {
          "name": "in_raster_or_constant2",
          "explanation": "The input that determines the power the values in the first input will be raised to.A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To specify a number for both inputs, the cell size and extent must first be set in the environment.",
          "datatype": "Raster Layer; Constant"
        }
      ],
      "summary": "Raises the cell values in a raster to the power of the values found in another raster.",
      "extraction_date": "2025-10-01T15:14:27.793437"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Round Down",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/round-down.htm",
      "parameters": [
        {
          "name": "in_raster_or_constant",
          "explanation": "The input values to be rounded down.To use a number as an input for this parameter, the cell size and extent must first be set in the environment.",
          "datatype": "Raster Layer; Constant"
        }
      ],
      "summary": "Returns the next lower integer value, just represented as a floating point, for each cell in a raster.",
      "extraction_date": "2025-10-01T15:14:30.233512"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Round Up",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/round-up.htm",
      "parameters": [
        {
          "name": "in_raster_or_constant",
          "explanation": "The input values to be rounded up.To use a number as an input for this parameter, the cell size and extent must first be set in the environment.",
          "datatype": "Raster Layer; Constant"
        }
      ],
      "summary": "Returns the next higher integer value, just represented as a floating point, for each cell in a raster.",
      "extraction_date": "2025-10-01T15:14:32.599824"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Square",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/square.htm",
      "parameters": [
        {
          "name": "in_raster_or_constant",
          "explanation": "The input values to find the square of.To use a number as an input for this parameter, the cell size and extent must first be set in the environment.",
          "datatype": "Raster Layer; Constant"
        }
      ],
      "summary": "Calculates the square of the cell values in a raster.",
      "extraction_date": "2025-10-01T15:14:34.966457"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Square Root",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/square-root.htm",
      "parameters": [
        {
          "name": "in_raster_or_constant",
          "explanation": "The input values to find the square root of.To use a number as an input for this parameter, the cell size and extent must first be set in the environment.",
          "datatype": "Raster Layer; Constant"
        }
      ],
      "summary": "Calculates the square root of the cell values in a raster.",
      "extraction_date": "2025-10-01T15:14:37.348246"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Times",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/times.htm",
      "parameters": [
        {
          "name": "in_raster_or_constant1",
          "explanation": "The input containing the values to be multiplied.A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To specify a number for both inputs, the cell size and extent must first be set in the environment.",
          "datatype": "Raster Layer; Constant"
        },
        {
          "name": "in_raster_or_constant2",
          "explanation": "The input containing the values by which the first input will be multiplied.A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To specify a number for both inputs, the cell size and extent must first be set in the environment.",
          "datatype": "Raster Layer; Constant"
        }
      ],
      "summary": "Multiplies the values of two rasters on a cell-by-cell basis.",
      "extraction_date": "2025-10-01T15:14:39.890453"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Extract Video Frames To Images",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/extract-video-frames-to-images.htm",
      "parameters": [
        {
          "name": "in_video",
          "explanation": "The input video file in any of the supported video file formats, including  .av1, .avi, .csv, .gpx, .h264, .h265, .json, .mp2, .mp4, .m2ts, .mpeg, .mpeg2, .mpeg4, .mpg, .mpg2, .mpg4, .ps, .ts, .vob, and .wmv.",
          "datatype": "File"
        },
        {
          "name": "out_folder",
          "explanation": "The \r\nfile directory where the output images and metadata will be saved.",
          "datatype": "Folder"
        },
        {
          "name": "image_type(Optional)",
          "explanation": "Specifies the output image format.JPEG—The output will be in JPEG image format.TIFF—The output will be in TIFF image format. This is the default.NITF—The output will be in NITF image format.PNG—The output will be in PNG image format.",
          "datatype": "String"
        },
        {
          "name": "image_overlap(Optional)",
          "explanation": "The maximum overlap percentage between two images. If the overlap between a candidate image and the last image written to disk is greater than this value, the candidate image will be ignored. The default percentage is 100 percent, which writes all images to disk.",
          "datatype": "Double"
        },
        {
          "name": "require_fresh_metadata(Optional)",
          "explanation": "Specifies whether  only video frames with associated metadata will be extracted and saved.   REQUIRE_FRESH_METADATA—Only video frames with associated metadata will be saved.NO_REQUIRE_FRESH_METADATA—All video frames will be saved.  This is the default.",
          "datatype": "Boolean"
        },
        {
          "name": "min_time(Optional)",
          "explanation": "The minimum time interval between video frames that will be saved. If no value is provided, all video frames will be saved as images.",
          "datatype": "Time Unit"
        }
      ],
      "summary": "Extracts video frame images and associated metadata from a full-motion video (FMV)-compliant video stream.  The extracted images can be added to a mosaic dataset or other tools and functions for further analysis.",
      "extraction_date": "2025-10-01T15:14:43.927035"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Video Metadata To Feature Class",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/video-metadata-to-feature-class.htm",
      "parameters": [
        {
          "name": "in_video",
          "explanation": "The FMV-compliant input video file containing essential metadata for each frame of the video data. The supported video file types are .av1, .avi, .csv, .gpx, .h264, .h265, .json, .mp2, .mp4, .m2ts, .mpeg, .mpeg2, .mpeg4, .mpg, .mpg2, .mpg4, .ps, .ts, .vob, and .wmv.",
          "datatype": "File"
        },
        {
          "name": "csv_file(Optional)",
          "explanation": "An output .csv or .json file containing metadata about the video frames for specific times.The metadata file is in the same format used by the Video Multiplexer tool.",
          "datatype": "File"
        },
        {
          "name": "flightpath(Optional)",
          "explanation": "The feature class containing the sensor's flight path information.",
          "datatype": "Feature Class"
        },
        {
          "name": "flightpath_type(Optional)",
          "explanation": "Specifies the feature class type that will be used for the flight path.POINT—A point feature class will be used.POLYLINE—A polyline feature class will be used. This is the default.",
          "datatype": "String"
        },
        {
          "name": "imagepath(Optional)",
          "explanation": "The output feature class containing the image path information.",
          "datatype": "Feature Class"
        },
        {
          "name": "imagepath_type(Optional)",
          "explanation": "Specifies the feature class type that will be used for the image path.  If you're using a point output, the center of each video frame image will appear on the map.POINT—A point feature class will be used.POLYLINE—A polyline feature class will be used.  This is the default.",
          "datatype": "String"
        },
        {
          "name": "footprint(Optional)",
          "explanation": "The output feature class containing the video image footprint information.",
          "datatype": "Feature Class"
        },
        {
          "name": "start_time(Optional)",
          "explanation": "The metadata recording start time from the beginning of the video. The input format is d.hh:mm:ss, and the default start time is 0.00:00:00. Metadata time stamps are not used in this field; the time of the video file is used.",
          "datatype": "Time Unit; Date"
        },
        {
          "name": "stop_time(Optional)",
          "explanation": "The metadata recording end time. The input format is d.hh:mm:ss. If  no value is provided, the value will default to the end of the video. Metadata time stamps are not used in this field.",
          "datatype": "Time Unit; Date"
        },
        {
          "name": "min_distance(Optional)",
          "explanation": "The distance between the features in sequential video frames. If no value is provided, every metadata feature will be extracted and added to the feature class.",
          "datatype": "Linear Unit"
        },
        {
          "name": "min_time(Optional)",
          "explanation": "The time interval between the features in sequential video frames. If no value is provided, every metadata feature will be extracted and added to the feature class.",
          "datatype": "Time Unit"
        },
        {
          "name": "vmti(Optional)",
          "explanation": "The output feature dataset containing the video VMTI information.",
          "datatype": "Feature Dataset"
        }
      ],
      "summary": "Extracts the platform, frame center, frame outline, and attributes metadata from a full-motion video (FMV)-compliant video. The output geometry and attributes are saved as feature classes.",
      "extraction_date": "2025-10-01T15:14:46.532696"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Video Multiplexer",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/video-multiplexer.htm",
      "parameters": [
        {
          "name": "in_video_file",
          "explanation": "The input video file that will be converted to an FMV-compliant video file.The following file types are supported: .av1,.avi, .csv, .gpx, .h264, .h265, .json, .mp2, .mp4, .m2ts, .mpeg, .mpeg2, .mpeg4, .mpg, .mpg2, .mpg4, .ps, .ts, .vob, and .wmv.",
          "datatype": "File"
        },
        {
          "name": "metadata_file",
          "explanation": "A .csv, .json, or .gpx file containing metadata about the video frames for specific times. Each column in the metadata file represents one metadata field, and one of the columns must be a time reference. The time reference is  UNIX time stamp (seconds past 1970) multiplied by one million, which is stored as an integer. The time is stored this way so that  any instant in time (down to one millionth of a second) can be referenced with an integer. Consequently, a time difference between two entries of 500,000 represents one half of a second in elapsed time.The first row contains the field names for the metadata columns. These field names can be matched to their corresponding field names using the FMV_Multiplexer_Field_Mapping_Template.csv file in C:\\Program Files\\ArcGIS\\Pro\\Resources\\MotionImagery if needed. Each subsequent row contains the metadata values for the time indicated in the time field.",
          "datatype": "File"
        },
        {
          "name": "out_video_file",
          "explanation": "The name of the output video file, including the file extension.The supported output video file is a .ts file.",
          "datatype": "File"
        },
        {
          "name": "metadata_mapping_file(Optional)",
          "explanation": "A .csv file that contains 5 columns and 87 rows and is based on the FMV_Multiplexer_Field_Mapping_Template.csv template file obtained from C:\\Program Files\\ArcGIS\\Pro\\Resources\\MotionImagery.Each row represents one of the standard MISB metadata tags, such as sensor latitude. The first two columns contain the MISB index and MISB tag name. The third column contains the field name as it appears in the in_metadata_file parameter if present. When the third column is populated, the tool can match the metadata field names to the proper FMV metadata tags. The fourth and fifth columns represent the units and notes associated with the tag, respectively.",
          "datatype": "File"
        },
        {
          "name": "timeshift_file(Optional)",
          "explanation": "A file containing defined time shift intervals.Ideally, the video images and the metadata are synchronized in time. In this case, the image footprint in FMV surrounds features that can be seen in the video image. Sometimes there is a mismatch between the timing of the video and the timing in the metadata. This leads to an apparent time delay between when a ground feature is surrounded by the image footprint and when that ground feature is visible in the video image. If this time shift is observable and consistent, the multiplexer can adjust the timing of the metadata to match the video.If there is a mismatch between the timing of the video and metadata, specify the time shift in the FMV_Multiplexer_TimeShift_Template.csv template in C:\\Program Files\\ArcGIS\\Pro\\Resources\\MotionImagery. The time shift observations file is a .csv file containing two columns (elapsed time and time shift) and one or more data rows. A row for column names is optional. \r\nFor example, if the video image has a five-second lag for the entire time, the time shift observation file will have one line: 0:00, -5. The entire video is shifted five seconds.If there is a five-second lag at the 0:18 mark of the video, and a nine-second lag at the 2:21 mark of the video, the time shift observation file will have the following two lines:0:18, -5\n2:21, -9In this case, the video is shifted differently at the beginning of the video and at the end of the video.You can define any number of time shift intervals in the time shift observation file.",
          "datatype": "File"
        },
        {
          "name": "elevation_layer(Optional)",
          "explanation": "The source of the elevation needed for calculating the video frame corner coordinates. The source can be a layer, image service, or an average ground elevation or ocean depth. The average elevation value must include the units of measurement such as meters or feet or other measure of length.The accuracy of the video footprint and frame center depend on the accuracy of the DEM data source provided. It is recommended that you provide a DEM layer or image service. If you do not have access to DEM data, you can provide an average elevation and unit relative to sea level, such as 15 feet or 10 meters. In the case of a submersible, you can enter -15 feet or -10 meters, for example. Using an average elevation or ocean depth is not as accurate as providing a DEM or bathymetric data.To calculate frame corner coordinates, the average elevation value must always be less than the sensor's altitude or depth as recorded in the metadata. For example, if the video was filmed at a sensor altitude of 10 meters and higher, a valid average elevation could be 9 meters or less. If a video was filmed underwater at a depth of -10 meters and deeper, the valid average elevation (relative to sea level) could be -11 or deeper. If the sensor altitude value is less than the average elevation value, the four corner coordinates will not be calculated for that record. If you do not know the average elevation of the project area, use a DEM.",
          "datatype": "Raster Layer; Image Service; Linear Unit"
        },
        {
          "name": "input_coordinate_system(Optional)",
          "explanation": "The coordinate system that will be used for the metadata_file parameter value.",
          "datatype": "Coordinate System"
        }
      ],
      "summary": "Creates a single full-motion video (FMV)-compliant video file that combines an archived video stream file and a separate associated metadata file synchronized by a time stamp. The process of combining the two files containing the video and metadata files is called multiplexing.",
      "extraction_date": "2025-10-01T15:14:49.199264"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Aggregate Multidimensional Raster",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/aggregate-multidimensional-raster.htm",
      "parameters": [
        {
          "name": "in_multidimensional_raster",
          "explanation": "The input multidimensional raster dataset.",
          "datatype": "Raster Dataset; Raster Layer; Mosaic Dataset; Mosaic Layer; Image Service; File"
        },
        {
          "name": "dimension",
          "explanation": "The aggregation dimension. This is the dimension along which the variables will be aggregated.",
          "datatype": "String"
        },
        {
          "name": "aggregation_method(Optional)",
          "explanation": "Specifies the mathematical method that will be used to combine the aggregated slices in an interval.MEAN—The mean of a pixel's values will be calculated across all slices in the interval. This is the default.MAXIMUM—The maximum value of a pixel will be calculated across all slices in the interval.MAJORITY—The pixel value that occurred most frequently will be calculated across all slices in the interval.MINIMUM—The minimum value of a pixel will be calculated across all slices in the interval.MINORITY—The pixel value that occurred least frequently will be calculated across all slices in the interval.MEDIAN—The median value of a pixel will be calculated across all slices in the interval.PERCENTILE—The percentile of values for a pixel will be calculated across all slices in the interval. The 90th percentile is calculated by default. You can specify other values (from 0 to 100) using the Percentile value parameter.RANGE—The range of values for a pixel will be calculated across all slices in the interval.STD—The standard deviation of a pixel's values will be calculated across all slices in the interval.SUM—The sum of a pixel's values will be calculated across all slices in the interval.VARIETY—The number of unique pixel values will be calculated across all slices in the interval.CUSTOM—The pixel value will be calculated based on a custom raster function.\r\n\r\nWhen aggregation_method is set to CUSTOM, the aggregation_function parameter becomes enabled.",
          "datatype": "String"
        },
        {
          "name": "variables[variables,...](Optional)",
          "explanation": "The variable or variables that will be aggregated along the given dimension. If no variable is specified, all variables with the selected dimension will be aggregated.For example, to aggregate daily temperature data into monthly average values, specify temperature as the variable to be aggregated. If you do not specify any variables and you have both daily temperature and daily precipitation variables, both variables will be aggregated into monthly averages and the output multidimensional raster will include both variables.",
          "datatype": "String"
        },
        {
          "name": "aggregation_def(Optional)",
          "explanation": "Specifies the dimension interval for which the data will be aggregated.ALL—The data values will be aggregated across all slices. This is the default.INTERVAL_KEYWORD—The variable data will be aggregated using a commonly known interval.INTERVAL_VALUE—The variable data will be aggregated using a user-specified interval and unit.INTERVAL_RANGES—The variable data will be aggregated between specified pairs of values or dates.",
          "datatype": "String"
        },
        {
          "name": "interval_keyword(Optional)",
          "explanation": "Specifies the keyword interval that will be used when aggregating along the dimension. This parameter is required when the aggregation_def parameter is set to INTERVAL_KEYWORD and the aggregation must be across time.\r\nHOURLY—The data values will be aggregated into hourly time steps, and the result will include every hour in the time series.DAILY—The data values will be aggregated into daily time steps, and the result will include every day in the time series.WEEKLY—The data values will be aggregated into weekly time steps, and the result will include every week in the time series.DEKADLY—The data values will be aggregated into 3 periods of 10 days each. The last period can contain more or fewer than 10 days. The output will include 3 slices for each month.PENTADLY—The data values will be aggregated into 6 periods of 5 days each. The last period can contain more or fewer than 5 days. The output will include 6 slices for each month.MONTHLY—The data values will be aggregated into monthly time steps, and the result will include every month in the time series.QUARTERLY—The data values will be aggregated into quarterly time steps, and the result will include every quarter in the time series.YEARLY—The data values will be aggregated into yearly time steps, and the result will include every year in the time series.RECURRING_DAILY—The data values will be aggregated into daily time steps, and the result will include one aggregated value per Julian day. The output will include, at most, 366 daily time slices.RECURRING_WEEKLY—The data values will be aggregated into weekly time steps, and the result will include one aggregated value per week. The output will include, at most, 53 weekly time slices.RECURRING_MONTHLY—The data values will be aggregated into monthly time steps, and the result will include one aggregated value per month. The output will include, at most, 12 monthly time slices.RECURRING_QUARTERLY—The data values will be aggregated into quarterly time steps, and the result will include one aggregated value per quarter. The output will include, at most, 4 quarterly time slices.",
          "datatype": "String"
        },
        {
          "name": "interval_value(Optional)",
          "explanation": "The size of the interval that will be used for the aggregation. This parameter is required when the aggregation_def parameter is set to INTERVAL_VALUE.For example, to aggregate 30 years of monthly temperature data into 5-year increments, enter 5 as the interval_value, and specify interval_unit as YEARS.",
          "datatype": "Double"
        },
        {
          "name": "interval_unit(Optional)",
          "explanation": "The unit that will be used for the interval_value parameter. This parameter is required when the dimension parameter is set to a time field and the aggregation_def parameter is set to INTERVAL_VALUE.If you are aggregating anything other than time, this option will not be available and the unit for the interval value will match the variable unit of the input multidimensional raster data.HOURS—The data values will be aggregated into hourly time slices at the interval provided.DAYS—The data values will be aggregated into daily time slices at the interval provided.WEEKS—The data values will be aggregated into weekly time slices at the interval provided.MONTHS—The data values will be aggregated into monthly time slices at the interval provided.YEARS—The data values will be aggregated into yearly time slices at the interval provided.",
          "datatype": "String"
        },
        {
          "name": "interval_ranges[interval_ranges,...](Optional)",
          "explanation": "Interval ranges specified in a value table will be used to aggregate groups of values. The value table consists of pairs of minimum and maximum range values, with data type Double or Date.\r\nThis parameter is required when the aggregation_def parameter is set to INTERVAL_RANGE.",
          "datatype": "Value Table"
        },
        {
          "name": "aggregation_function(Optional)",
          "explanation": "A custom raster function that will be used to compute the pixel values of the aggregated rasters. The input is a raster function JSON object or an .rft.xml file created from a function chain or a custom Python raster function.This parameter is required when the aggregation_method parameter is set to CUSTOM.",
          "datatype": "File; String"
        },
        {
          "name": "ignore_nodata(Optional)",
          "explanation": "Specifies whether NoData values will be ignored in the analysis.DATA—The analysis will include all valid pixels along a given dimension and ignore NoData pixels. This is the default.NODATA—The analysis will result in NoData if there are NoData values for the pixels along the given dimension.",
          "datatype": "Boolean"
        },
        {
          "name": "dimensionless(Optional)",
          "explanation": "Specifies whether the layer will have dimension values. This parameter is only enabled if a single slice is selected to create a layer.NO_DIMENSIONS— The layer will not have dimension values.DIMENSIONS—The layer will have dimension values. This is the default.",
          "datatype": "Boolean"
        },
        {
          "name": "percentile_value(Optional)",
          "explanation": "The percentile that will be calculated. The default is 90, indicating the 90th percentile.The values can range from 0 to 100. The 0th percentile is essentially equivalent to the minimum statistic, and the 100th percentile is equivalent to maximum. A value of 50 will produce essentially the same result as the median statistic.\r\nThis parameter is only supported if the statistics_type parameter is set to PERCENTILE.",
          "datatype": "Double"
        },
        {
          "name": "percentile_interpolation_type(Optional)",
          "explanation": "Specifies the method of percentile interpolation that will be used when there is an even number of values from the input raster to be calculated.NEAREST—The nearest available value to the desired percentile will be used. In this case, the output pixel type will be the same as that of the input value raster.LINEAR—The weighted average of the two surrounding values from the desired percentile will be used. In this case, the output pixel type will be floating point.",
          "datatype": "String"
        }
      ],
      "summary": "Generates a multidimensional raster dataset by combining existing multidimensional raster variables along a dimension.",
      "extraction_date": "2025-10-01T15:14:53.759192"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Find Argument Statistics",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/find-argument-statistics.htm",
      "parameters": [
        {
          "name": "in_raster",
          "explanation": "The input multidimensional or multiband raster to be analyzed.",
          "datatype": "Raster Dataset; Raster Layer; Mosaic Dataset; Mosaic Layer; Image Service; File"
        },
        {
          "name": "dimension(Optional)",
          "explanation": "The dimension from which the statistic will be extracted. If the input raster is not a multidimensional raster, this parameter is not required.",
          "datatype": "String"
        },
        {
          "name": "dimension_def(Optional)",
          "explanation": "Specifies how the statistic will be extracted from the dimension. ALL—The statistic will be extracted across all dimensions. This is the default.INTERVAL_KEYWORD—The statistic will be extracted from the time dimension according to the interval keyword.",
          "datatype": "String"
        },
        {
          "name": "interval_keyword(Optional)",
          "explanation": "The unit of time for which the statistic will be extracted.For example, you have five years of daily sea surface temperature data and you want to know the year in which the maximum temperature was observed. Set Statistics Type to Argument of the maximum, set Dimension Definition to Interval Keyword, and set Keyword Interval to Yearly. Alternatively, if you want to know the month in which the maximum temperature was consistently observed, set Statistics Type to  Argument of the maximum, set Dimension Definition to Interval Keyword, and set Keyword Interval to Recurring Monthly. This will generate a raster in which each pixel contains the month in which the statistic was reached across the five-year record (for example, 08/18/2018, 08/25/2016, 08/07/2013).\r\nThis parameter is required when the dimension parameter is set to StdTime and the dimension_def parameter is set to INTERVAL_KEYWORD.RECURRING_DAILY—The statistic will be extracted across days.RECURRING_WEEKLY—The statistic will be extracted across weeks.RECURRING_MONTHLY—The statistic will be extracted across months.RECURRING_QUARTERLY—The statistic will be extracted across quarters.HOURLY—The statistic will be extracted for the  hour in which the statistic was reached.DAILY—The statistic will be extracted for the  day in which the statistic was reached.WEEKLY—The statistic will be extracted for the  week in which the statistic was reached.MONTHLY—The statistic will be extracted for the  month in which the statistic was reached.QUARTERLY—The statistic will be extracted for the  quarter in which the statistic was reached.YEARLY—The statistic will be extracted for the  year in which the statistic was reached.",
          "datatype": "String"
        },
        {
          "name": "variables[variables,...](Optional)",
          "explanation": "The variable or variables to be analyzed. If the input raster is not multidimensional, the pixel values of the multiband raster are considered the variable. If the input raster is multidimensional and no variable is specified, all variables with the selected dimension will be analyzed.For example, to find the years in which temperature values were highest, specify temperature as the variable to be analyzed. If you do not specify any variables and you have both temperature and precipitation variables, both variables will be analyzed, and the output multidimensional raster will include both variables.",
          "datatype": "String"
        },
        {
          "name": "statistics_type(Optional)",
          "explanation": "Specifies the statistic to extract from the variable or variables along the given dimension.ARGUMENT_MIN—The dimension value at which the minimum variable value is reached will be extracted. This is the default.ARGUMENT_MAX—The dimension value at which the maximum variable value is reached will be extracted.ARGUMENT_MEDIAN—The dimension value at which the median variable value is reached will be extracted.DURATION—The longest dimension duration value between the minimum and maximum variable values will be extracted.ARGUMENT_VALUE—The dimension value at which the specified variable value is reached will be extracted.",
          "datatype": "String"
        },
        {
          "name": "min(Optional)",
          "explanation": "The minimum variable value to be used to extract the duration.This parameter is required when the statistics_type parameter is set to DURATION.",
          "datatype": "Double"
        },
        {
          "name": "max(Optional)",
          "explanation": "The maximum variable value to be used to extract the duration.This parameter is required when the statistics_type parameter is set to DURATION.",
          "datatype": "Double"
        },
        {
          "name": "multiple_occurrence(Optional)",
          "explanation": "The pixel value to use to indicate that a given argument statistic was reached more than once in the input raster dataset. If not specified, the pixel value will be the value of the dimension as specified by the\r\noccurrence parameter, either the first or last occurrence.",
          "datatype": "Long"
        },
        {
          "name": "ignore_nodata(Optional)",
          "explanation": "Specifies whether NoData values will be ignored in the analysis.DATA—The analysis will include all valid pixels along a given dimension and ignore NoData pixels. This is the default.NODATA—The analysis will result in NoData if there are NoData values for the pixels along the given dimension.",
          "datatype": "Boolean"
        },
        {
          "name": "value(Optional)",
          "explanation": "The value at which a comparison will be made to extract the dimension value.This parameter is required when the Statistics Type parameter is set to Argument of the value.",
          "datatype": "Long"
        },
        {
          "name": "comparison(Optional)",
          "explanation": "Specifies the comparison type that will be used to extract the dimension value.EQUAL_TO—The extracted dimension is equal to the specified value. This is the default.GREATER_THAN—The extracted dimension is greater than the specified value.SMALLER_THAN—The extracted dimension is smaller than the specified value.",
          "datatype": "String"
        },
        {
          "name": "occurrence(Optional)",
          "explanation": "Specifies whether the value of the dimension will be returned the first time or last time the argument statistic is reached.FIRST_OCCURRENCE—The value of the dimension will be returned the first time the argument statistic is reached. This is the default.LAST_OCCURRENCE—The value of the dimension will be returned the last time the argument statistic is reached.",
          "datatype": "String"
        }
      ],
      "summary": "Extracts the dimension value or band index at which a given statistic is attained for each pixel in a multidimensional or multiband raster.",
      "extraction_date": "2025-10-01T15:14:56.394173"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Generate Multidimensional Anomaly",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/generate-multidimensional-anomaly.htm",
      "parameters": [
        {
          "name": "in_multidimensional_raster",
          "explanation": "The input multidimensional raster dataset.",
          "datatype": "Raster Dataset; Raster Layer; Mosaic Dataset; Mosaic Layer; Image Service; File"
        },
        {
          "name": "variables[variables,...](Optional)",
          "explanation": "The variable or variables for which anomalies will be calculated. If no variable is specified, all variables with a time dimension will be analyzed.",
          "datatype": "String"
        },
        {
          "name": "method(Optional)",
          "explanation": "Specifies the method that will be used to calculate the anomaly.\r\nDIFFERENCE_FROM_MEAN—The difference between a pixel's value and the mean of that pixel's values across slices defined by the interval will be calculated. This is the default.PERCENT_DIFFERENCE_FROM_MEAN—The percent difference between a pixel's value and the mean of that pixel's values across slices defined by the interval will be calculated.PERCENT_OF_MEAN—The percent of the mean will be calculated.Z_SCORE—The z-score for each pixel will be calculated. A z-score of 0 indicates the pixel's value is identical to the mean. A z-score of 1 indicates the pixel's value is 1 standard deviation from the mean. If a z-score is 2, the pixel's value is 2 standard deviations from the mean, and so on.DIFFERENCE_FROM_MEDIAN—The difference between a pixel's value and the mathematical median of that pixel's values across slices defined by the interval will be calculated.PERCENT_DIFFERENCE_FROM_MEDIAN—The percent difference between a pixel's value and the mathematical median of that pixel's values across slices defined by the interval will be calculated.PERCENT_OF_MEDIAN—The percent of the mathematical median will be calculated.",
          "datatype": "String"
        },
        {
          "name": "calculation_interval(Optional)",
          "explanation": "Specifies the temporal interval that will be used to calculate the mean.ALL—The mean is calculated across all slices for each pixel.YEARLY—The yearly mean is calculated for each pixel.RECURRING_MONTHLY—The monthly mean is calculated for each pixel.RECURRING_WEEKLY—The weekly mean is calculated for each pixel.RECURRING_DAILY—The daily mean is calculated for each pixel.HOURLY—The hourly mean is calculated for each pixel.EXTERNAL_RASTER—An existing raster dataset that contains the mean or median value for each pixel is referenced.",
          "datatype": "String"
        },
        {
          "name": "ignore_nodata(Optional)",
          "explanation": "Specifies whether NoData values will be ignored in the analysis.DATA—The analysis will include all valid pixels along a given dimension and ignore NoData pixels. This is the default.NODATA—The analysis will result in NoData if there are NoData values for the pixels along the given dimension.",
          "datatype": "Boolean"
        },
        {
          "name": "reference_mean_raster(Optional)",
          "explanation": "The reference raster dataset that contains a previously calculated mean for each pixel. The anomalies will be calculated in comparison to this mean.",
          "datatype": "Raster Layer; Raster Dataset; Mosaic Layer; Mosaic Dataset"
        }
      ],
      "summary": "Computes the anomaly for each slice in an existing multidimensional raster to generate a new multidimensional raster. An anomaly is the deviation of an observation from its standard or mean value.",
      "extraction_date": "2025-10-01T15:14:58.938687"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Generate Trend Raster",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/generate-trend-raster.htm",
      "parameters": [
        {
          "name": "in_multidimensional_raster",
          "explanation": "The input multidimensional raster dataset.",
          "datatype": "Raster Dataset; Raster Layer; Mosaic Dataset; Mosaic Layer; Image Service; File"
        },
        {
          "name": "dimension",
          "explanation": "The dimension along which a trend will be extracted for the variable or variables selected in the analysis.",
          "datatype": "String"
        },
        {
          "name": "variables[variables,...](Optional)",
          "explanation": "The variable or variables for which trends will be calculated. If no variable is specified, the first variable in the multidimensional raster will be analyzed.",
          "datatype": "String"
        },
        {
          "name": "line_type(Optional)",
          "explanation": "Specifies the type of trend analysis to perform to pixel values along a dimension.LINEAR—Variable pixel values will be fitted along a linear trend line. This is the default.POLYNOMIAL—Variable pixel values will be fitted along a second order polynomial trend line.HARMONIC—Variable pixel values will be fitted along a harmonic trend line.MANN-KENDALL—Variable pixel values will be evaluated using the Mann-Kendall trend test.SEASONAL-KENDALL—Variable pixel values will be evaluated using the Seasonal-Kendall trend test.",
          "datatype": "String"
        },
        {
          "name": "frequency(Optional)",
          "explanation": "The frequency or the polynomial order number to use in the trend fitting. If the trend type is polynomial, this parameter specifies the polynomial order. If the trend type is harmonic, this parameter specifies the number of models to use to fit the trend.This parameter is only included in the trend analysis when the dimension being analyzed is time.If the line_type parameter is HARMONIC,  the default value is 1, meaning a first order harmonic curve is used to fit the model.If the line_type parameter is POLYNOMIAL, the default value is 2, or second order polynomial.",
          "datatype": "Long"
        },
        {
          "name": "ignore_nodata(Optional)",
          "explanation": "Specifies whether NoData values will be ignored in the analysis.DATA—The analysis will include all valid pixels along a given dimension and ignore NoData pixels. This is the default.NODATA—The analysis will result in NoData if there are NoData values for the pixels along the given dimension.",
          "datatype": "Boolean"
        },
        {
          "name": "cycle_length(Optional)",
          "explanation": "The length of periodic variation to model.  This parameter is required when line_type is set to HARMONIC. For example, leaf greenness often has one strong cycle of variation in a single year, so the cycle length is 1 year. Hourly temperature data \r\nhas one strong cycle of variation throughout a single day, so the cycle length is 1 day.The default length is 1 year for data that varies on an annual cycle.",
          "datatype": "Double"
        },
        {
          "name": "cycle_unit(Optional)",
          "explanation": "Specifies the time unit to be used for the length of a harmonic cycle.DAYS—The unit for the length of the harmonic cycle is days.YEARS—The unit for the length of the harmonic cycle is years. This is the default.",
          "datatype": "String"
        },
        {
          "name": "rmse(Optional)",
          "explanation": "Specifies whether the root mean square error (RMSE) of the trend fit line will be calculated.RMSE—The RMSE will be calculated. This is the default.NO_RMSE—The RMSE will not be calculated.",
          "datatype": "Boolean"
        },
        {
          "name": "r2(Optional)",
          "explanation": "Specifies whether the R-squared goodness-of-fit statistic  for the trend fit line will be calculated.R2—The R-squared value will be calculated.NO_R2—The R-squared value will not be calculated. This is the default.",
          "datatype": "Boolean"
        },
        {
          "name": "slope_p_value(Optional)",
          "explanation": "Specifies whether the p-value statistic  for the slope coefficient of the trend line will be calculated.SLOPEPVALUE—The p-value will be calculated. NO_SLOPEPVALUE—The p-value will not be calculated. This is the default.",
          "datatype": "Boolean"
        },
        {
          "name": "seasonal_period(Optional)",
          "explanation": "Specifies the time unit to be used for the length of a seasonal period when performing the Seasonal-Kendall test.DAYS—The unit for the length of the seasonal period is days. This is the default.MONTHS—The unit for the length of the seasonal period is months.",
          "datatype": "String"
        }
      ],
      "summary": "Estimates the trend for each pixel along a dimension for one or more variables in a multidimensional raster.",
      "extraction_date": "2025-10-01T15:15:01.791468"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Interpolate From Spatiotemporal Points",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/interpolate-from-spatiotemporal-points.htm",
      "parameters": [
        {
          "name": "in_dataset",
          "explanation": "The input point layer, trajectory layer, or trajectory dataset.",
          "datatype": "Trajectory Layer; Feature Layer; Mosaic Dataset; Mosaic Layer"
        },
        {
          "name": "variable_field",
          "explanation": "A field containing variable values.",
          "datatype": "String"
        },
        {
          "name": "time_field",
          "explanation": "A field containing time values.",
          "datatype": "String"
        },
        {
          "name": "temporal_aggregation(Optional)",
          "explanation": "Specifies the temporal aggregation of the output multidimensional raster. The interpolation algorithm uses all available data within these time periods to calculate the output slice.DAILY—The data values will be aggregated into daily time steps. This is the default.WEEKLY—The data values will be aggregated into weekly time steps.MONTHLY—The data values will be aggregated into monthly time steps.QUARTERLY—The data values will be aggregated into quarterly time steps.YEARLY—The data values will be aggregated into yearly time steps.",
          "datatype": "String"
        },
        {
          "name": "cell_size(Optional)",
          "explanation": "The output cell size. By default, the cell size will be the shorter of the width or the height of the input point feature extent, divided by 250.",
          "datatype": "Double"
        },
        {
          "name": "interpolation_method",
          "explanation": "Specifies the interpolation method that will be used.\r\nIDW—Inverse distance weighted interpolation will be used.TRIANGULATION—Triangulation interpolation will be used.MEAN—Mean interpolation will be used.MEDIAN— Median interpolation will be used.NATURAL_NEIGHBOR—Natural neighbor interpolation will be used.NEAREST_NEIGHBOR—Nearest neighbor interpolation will be used.QUADRATIC—Quadratic interpolation will be used.",
          "datatype": "String"
        }
      ],
      "summary": "Interpolates temporal point data into a multidimensional raster.",
      "extraction_date": "2025-10-01T15:15:04.291462"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Multidimensional Principal Components",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/multidimensional-principal-components.htm",
      "parameters": [
        {
          "name": "in_multidimensional_raster",
          "explanation": "The input multidimensional raster.The tool processes data along one dimension, such as a time series raster or a data cube defined by a nontime dimension [X, Y, Z]. If an input variable includes multiple dimensions, such as depth and time, the first dimension value will be used by default. You can use the Make Multidimensional Raster Layer tool or Subset Multidimensional Raster tool to redefine the multidimensional data as needed, such as configuring multidimensional data into a dataset with one dimension.",
          "datatype": "Raster Dataset; Mosaic Dataset; Raster Layer; Mosaic Layer; Image Service; File"
        },
        {
          "name": "mode",
          "explanation": "Specifies the method that will be used to perform principal component analysis.DIMENSION_REDUCTION—The input time series data will be treated as a set of images. Principal components that  extract prevalent pattens over time will be computed. This is the default.SPATIAL_REDUCTION—The input time series data will be treated as a set of pixels. Principal components that  extract prevalent pattens and locations over time will be computed as a set of one-dimensional arrays stored in a table.",
          "datatype": "String"
        },
        {
          "name": "dimension",
          "explanation": "The dimension name used to process the principal components.",
          "datatype": "String"
        },
        {
          "name": "out_pc",
          "explanation": "The name of the output raster dataset. When the mode parameter is specified as DIMENSION_REDUCTION, the output will be a multiband raster with the components as bands. The first band is the first principal component with the largest eigenvalue, the second band has the principal component with the second largest eigenvalue, and so on. The output is in CRF file format (.crf), which maintains the multidimensional information.When the mode parameter is specified as SPATIAL_REDUCTION, the output is a table containing a set of time series data representing the principal components.",
          "datatype": "Raster Dataset; Table"
        },
        {
          "name": "out_loadings",
          "explanation": "The output loadings data contributing to the principal components.When the mode parameter is specified as DIMENSION_REDUCTION, the output will be a table containing the weights that each input raster contributed to the principal components. These weights define the correlations of the input data and the output principal components.   Use the .csv file extension to output the loadings as a comma-separated values file.When the mode parameter is specified as SPATIAL_REDUCTION, the output is a raster where pixel values are the weights contributing the principal components.  Pixels with larger values are more corelated to the principal components.  This output may have a larger cell size than the input raster because a random reprojection is applied to reduce the computation complexity.",
          "datatype": "Table; Raster Dataset"
        },
        {
          "name": "out_eigenvalues(Optional)",
          "explanation": "The output Eigenvalues table. Eigenvalues are values indicating the variance percentage of each component. Eigenvalues help you define the number of principal components that are needed to represent the dataset.",
          "datatype": "Table"
        },
        {
          "name": "variable(Optional)",
          "explanation": "The variable of the input multidimensional raster used in computation. If the input raster is multidimensional and no variable is specified, only the first variable will be analyzed, by default.For example, to find the years in which temperature values were highest, specify temperature as the variable to be analyzed. If you do not specify any variables and you have both temperature and precipitation variables, both variables will be analyzed, and the output multidimensional raster will include both variables.",
          "datatype": "String"
        },
        {
          "name": "number_of_pc(Optional)",
          "explanation": "The number of principal components to compute, usually fewer than the number of input rasters.This parameter also takes the form of a percentage (%).  For example, a value of 90% means the number of components that can explain 90 percent of variance in the data will be computed.",
          "datatype": "String"
        }
      ],
      "summary": "Transforms multidimensional rasters into  their principal components, loadings, and eigenvalues. The tool transforms the data into a reduced number of components that account for the variance of the data, so that spatial and temporal patterns can be readily identified.",
      "extraction_date": "2025-10-01T15:15:06.857413"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Multidimensional Raster Correlation",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/multidimensional-raster-correlation.htm",
      "parameters": [
        {
          "name": "in_mdim_raster1",
          "explanation": "The input multidimensional raster dataset.The first multidimensional raster in any supported format.",
          "datatype": "Raster Dataset; Raster Layer; Mosaic Dataset; Mosaic Layer; Image Service"
        },
        {
          "name": "in_mdim_raster2",
          "explanation": "The second multidimensional raster that will be correlated with the first input. The length of the dimension used in the calculation must be greater than 2. Autocorrelation will be calculated if the in_mdim_raster1 parameter value is the same as the in_mdim_raster2 parameter value.  Autocorrelation refers to the degree of correlation of the same variables between two successive time intervals.",
          "datatype": "Raster Dataset; Raster Layer; Mosaic Dataset; Mosaic Layer; Image Service"
        },
        {
          "name": "dimension1(Optional)",
          "explanation": "A  dimension name in the first dataset, along which the pixel array is defined. When the input has two nonspatial dimensions, a dimension must be specified. The length of the dimension used in the calculation must be greater than 2.",
          "datatype": "String"
        },
        {
          "name": "variable1(Optional)",
          "explanation": "A variable name from the first input raster.",
          "datatype": "String"
        },
        {
          "name": "dimension2(Optional)",
          "explanation": "A dimension name in the second dataset. The length of the dimension used in the calculation must be greater than 2.",
          "datatype": "String"
        },
        {
          "name": "variable2(Optional)",
          "explanation": "A variable name from the second input raster.",
          "datatype": "String"
        },
        {
          "name": "corr_method(Optional)",
          "explanation": "Specifies the correlation calculation method that will be used.PEARSON—The correlation method will be Pearson. This is the default.SPEARMAN—The correlation method will be Spearman.KENDALL—The correlation method will be Kendall.",
          "datatype": "String"
        },
        {
          "name": "lag(Optional)",
          "explanation": "Calculate a correlation value by shifting the pixel array by the step specified, from 0 to dimension/2, depending on the time lag.  The default is 0.",
          "datatype": "Long"
        },
        {
          "name": "calculate_xcorr(Optional)",
          "explanation": "Specifies whether the cross correlation will be computed at lags.When ALL_CROSS_CORRELATION is specified, the correlations will be calculated at each lag within a range defined by the lag value. For example, if the lag value is 2, correlations of -2, -1, 0, 1, and 2 will be calculated and stored as bands in the output raster.\r\nALL_CROSS_CORRELATION— The cross correlation will be computed at lags.NO_CROSS_CORRELATION—The cross correlation will not be computed at lags.  This is the default.",
          "datatype": "Boolean"
        },
        {
          "name": "calculate_pvalue(Optional)",
          "explanation": "Specifies whether the p-value will be computed at lags. P-value is a confidence value that describes how well the two variables are correlated.CALCULATE_P_VALUE— The p-value will be computed at lags.  The output will include additional bands storing the p-values.NO_P_VALUE—The p-value will not be computed at lags.  This is the default.",
          "datatype": "Boolean"
        },
        {
          "name": "out_max_corr_raster(Optional)",
          "explanation": "A 2-band raster with maximum correlation values and the lags at which the maximum correlations occur. The raster will be created when the calculate_xcorr parameter is specified as ALL_CROSS_CORRELATION.",
          "datatype": "Raster Dataset"
        }
      ],
      "summary": "Analyzes correlations between two variables in one or two multidimensional rasters. The tool takes two multidimensional rasters as input, compares two variables using the Pearson, Kendall, or Spearman correlation method, and outputs a correlation raster, with each pixel representing the correlation values of the corresponding pixel arrays. The output raster can map where the two variables are correlated and where they are not correlated. The tool can also calculate cross correlation when the lag is a nonzero value and calculate auto correlation when the two inputs are the same. You can analyze correlations between two variables in one or two multidimensional rasters.   The output is a correlation raster in which each pixel is the correlation of the two time series from the two variables.   The tool can be used to calculate correlation with a lag, a cross correlation, or an autocorrelation. For example, the correlation raster in the images below was calculated from a soil moisture variable over time and a precipitation variable over time.",
      "extraction_date": "2025-10-01T15:15:09.433019"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Predict Using Trend Raster",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/predict-using-trend-raster.htm",
      "parameters": [
        {
          "name": "in_multidimensional_raster",
          "explanation": "The input multidimensional trend raster from the Generate Trend Raster tool.",
          "datatype": "Raster Dataset; Raster Layer; Mosaic Dataset; Mosaic Layer; Image Service; File"
        },
        {
          "name": "variables[variables,...](Optional)",
          "explanation": "The variable or variables that will be predicted in the analysis. If no variables are specified, all variables will be used.",
          "datatype": "String"
        },
        {
          "name": "dimension_def(Optional)",
          "explanation": "Specifies the method used to provide prediction dimension values.BY_VALUE—The prediction will be calculated for a single dimension value or a list of dimension values defined by the Values parameter (dimension_values in Python). This is the default.For example, you want to predict yearly precipitation for the years 2050, 2100, and 2150.BY_INTERVAL—The prediction will be calculated for an interval of the dimension defined by a start and an end value.For example, you want to predict yearly precipitation for every year between 2050 and 2150.",
          "datatype": "String"
        },
        {
          "name": "dimension_values[dimension_values,...](Optional)",
          "explanation": "The dimension value or values to be used in the prediction. The format of the time, depth, and height values must match the format of the dimension values used to generate the trend raster. If the trend raster was generated for the StdTime dimension, the format would be YYYY-MM-DDTHH:MM:SS, for example 2050-01-01T00:00:00. Multiple values are separated with a semicolon. This parameter is required when the dimension_def parameter is set to BY_VALUE.",
          "datatype": "String"
        },
        {
          "name": "start(Optional)",
          "explanation": "The start date, height, or depth of the dimension interval to be used in the prediction.",
          "datatype": "String"
        },
        {
          "name": "end(Optional)",
          "explanation": "The end date, height, or depth of the dimension interval to be used in the prediction.",
          "datatype": "String"
        },
        {
          "name": "interval_value(Optional)",
          "explanation": "The number of steps between two dimension values to be included in the prediction. \r\nThe default value is 1.For example, to predict temperature values every five years, use a value of 5.",
          "datatype": "Double"
        },
        {
          "name": "interval_unit(Optional)",
          "explanation": "Specifies the unit that will be used for the interval value. This parameter only applies when the dimension of analysis is a time dimension.\r\n\r\nHOURS—The prediction will be calculated for each hour in the range of time described by the start, end, and interval_value parameters.DAYS—The prediction will be calculated for each day in the range of time described by the start, end, and interval_value parameters.WEEKS—The prediction will be calculated for each week in the range of time described by the start, end, and interval_value parameters.MONTHS—The prediction will be calculated for each month in the range of time described by the start, end, and interval_value parameters.YEARS—The prediction will be calculated for each year in the range of time described by the start, end, and interval_value parameters.",
          "datatype": "String"
        }
      ],
      "summary": "Computes a forecasted multidimensional raster using the output trend raster from the Generate Trend Raster tool.",
      "extraction_date": "2025-10-01T15:15:12.021204"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Summarize Categorical Raster",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/summarize-categorical-raster.htm",
      "parameters": [
        {
          "name": "in_raster",
          "explanation": "The input multidimensional raster of integer type.",
          "datatype": "Raster Dataset; Raster Layer; Mosaic Dataset; Mosaic Layer; Image Service; String"
        },
        {
          "name": "out_table",
          "explanation": "The output summary table. Geodatabase, database, text, Microsoft Excel, and comma-separated value (CSV) tables are supported.",
          "datatype": "Table"
        },
        {
          "name": "dimension(Optional)",
          "explanation": "The input dimension to use for the summary. \r\nIf there is more than one dimension and no value is specified, all slices will be summarized using all combinations of dimension values.",
          "datatype": "String"
        },
        {
          "name": "aoi(Optional)",
          "explanation": "The polygon feature layer containing the area or areas of interest to use when calculating the pixel count per category. If no area of interest is specified, the entire raster dataset will be included in the analysis.",
          "datatype": "Feature Layer"
        },
        {
          "name": "aoi_id_field(Optional)",
          "explanation": "The field in the polygon feature layer that defines each area of interest. Text and integer fields are supported.",
          "datatype": "Field"
        }
      ],
      "summary": "Generates a table containing the pixel count for each class, in each slice of an input categorical raster.",
      "extraction_date": "2025-10-01T15:15:14.386154"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Weighted Sum",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/weighted-sum.htm",
      "parameters": [
        {
          "name": "in_rasters",
          "explanation": "TheWeighted Sum tool overlays several rasters, multiplying each by their given weight and summing them together.An Overlay class is used to define the table. The WSTable object is used to specify a Python list of input rasters and weight them accordingly.The form of the object is:\r\nWSTable(weightedSumTable)",
          "datatype": "WSTable"
        }
      ],
      "summary": "Overlays several rasters, multiplying each by their given weight and summing them together. Learn more about how Weighted Sum works",
      "extraction_date": "2025-10-01T15:15:18.716416"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Classify Raster Using Spectra",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/classify-raster-using-spectra.htm",
      "parameters": [
        {
          "name": "in_raster",
          "explanation": "The input multiband raster.",
          "datatype": "Mosaic Layer; Raster Layer; Image Service; String; Raster Dataset; Mosaic Dataset"
        },
        {
          "name": "in_spectra_file",
          "explanation": "The  spectral information for different pixel classes.\r\nThe spectral information can be provided as point features, a training sample point feature class generated from the Training Samples Manager pane, or  a  .json file that contains the class spectral profiles.",
          "datatype": "Feature Layer; File; String"
        },
        {
          "name": "method",
          "explanation": "Specifies the spectral matching method that will be used.SAM—The  vector angle between the input multiband raster and the reference spectra will be calculated in which the spectra of each pixel is treated as a vector. Angle values are in radians.SID—The spectral information divergence between the input multiband raster and the reference spectra will be calculated. A score will be calculated for each pixel based on the divergence between the probability distributions of the pixel and reference spectra. Values are in radians.",
          "datatype": "String"
        },
        {
          "name": "thresholds(Optional)",
          "explanation": "The threshold for spectral matching. Pixel values that exceed this value will be classified as undefined. This can be a single value applied to all spectral classes or a space-delimited list of values for each class.",
          "datatype": "String"
        },
        {
          "name": "out_score_raster(Optional)",
          "explanation": "A multiband raster that stores the matching results for each end member. The band order follows the order of the classes in the in_spectra_file parameter value. If the input is a multidimensional raster, the output format must be CRF.",
          "datatype": "Raster Dataset"
        },
        {
          "name": "out_classifier_definition(Optional)",
          "explanation": "The output .ecd file.",
          "datatype": "File"
        }
      ],
      "summary": "Classifies a multiband raster dataset using spectral matching techniques. The input spectral data can be provided as a point feature class or a .json file.",
      "extraction_date": "2025-10-01T15:15:22.977529"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Detect Image Anomalies",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/detect-image-anomalies.htm",
      "parameters": [
        {
          "name": "in_raster",
          "explanation": "A multiband or hyperspectral image.",
          "datatype": "Raster Dataset; Raster Layer; Mosaic Dataset; Mosaic Layer; Image Service"
        },
        {
          "name": "out_raster",
          "explanation": "A single band raster that stores the anomaly scores between 0-1 as a floating point number.  Zero (0) is the background value, and large values approaching 1 are potential anomaly pixels. Use the file extension to specify the output format, including  .tif (TIFF), .crf (CRF), .mrf (MRF), and .dat (ENVI DAT).",
          "datatype": "Raster Dataset"
        },
        {
          "name": "method(Optional)",
          "explanation": "Specifies the anomaly calculation method that will be used. \r\nRXD—The RXD method will be used to extract pixels that are significantly different from the background pixel values. This is the default.UTD—The UTD method will be used to extract background pixels from the input image.KMEANS—The KMEANS method will be used to extract pixels that significantly deviate from the established clusters in the data using K-means clustering algorithms.",
          "datatype": "String"
        },
        {
          "name": "num_cluster(Optional)",
          "explanation": "The number of clusters that will be used when the method parameter is set to KMEANS.",
          "datatype": "Long"
        },
        {
          "name": "background_region(Optional)",
          "explanation": "A polygon feature class that will define the region to be used to calculate background statistics when the method parameter is set to RXD or UTD.",
          "datatype": "Feature Set"
        },
        {
          "name": "recompute_stats(Optional)",
          "explanation": "Specifies whether statistics will be recomputed for the output score raster when the method parameter is set to RXD or UTD.  The RXD and UTD options require accurate statistics in which the skip factor must be 1.RECOMPUTE_STATS—Statistics will be recomputed for the output score raster. This is the default.NOT_RECOMPUTE_STATS—Statistics will not be recomputed for the output score raster.",
          "datatype": "Boolean"
        }
      ],
      "summary": "Processes a multiband or hyperspectral image and creates an anomaly score raster. An anomaly score raster is a single band raster, with values between 0 and 1.",
      "extraction_date": "2025-10-01T15:15:25.487021"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Linear Spectral Unmixing",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/linear-spectral-unmixing.htm",
      "parameters": [
        {
          "name": "in_raster",
          "explanation": "The \r\ninput raster dataset.",
          "datatype": "Raster Dataset; Mosaic Dataset; Mosaic Layer; Raster Layer; File; Image Service"
        },
        {
          "name": "in_spectral_profile_file",
          "explanation": "The  spectral information for the different land-cover classes.This can be provided as polygon features, a classifier definition file (.ecd) generated from the Train Maximum Likelihood Classifier tool, or  a  JSON format file (.json) that contains the class spectral profiles.",
          "datatype": "File; Feature Layer; String"
        },
        {
          "name": "value_option[value_option,...](Optional)",
          "explanation": "Specifies how the output pixel values will be defined.SUM_TO_ONE—Class values for each pixel will be provided in decimal format with the sum of all classes equal to 1. For example, Class1 = 0.16; Class2 = 0.24; Class3 = 0.60.NON_NEGATIVE—There will be no negative output values.",
          "datatype": "String"
        }
      ],
      "summary": "Performs subpixel classification and calculates the fractional abundance of different land-cover types for individual pixels.",
      "extraction_date": "2025-10-01T15:15:27.878922"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Cell Statistics",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/cell-statistics.htm",
      "parameters": [
        {
          "name": "in_rasters_or_constants[in_raster_or_constant,...]",
          "explanation": "A list of input rasters for which a statistical operation will be calculated for each cell in the analysis window.A number can be used as an input; however, the cell size and extent must first be set in the environment.If the processing_as_multiband parameter is set to MULTI_BAND, all multiband inputs should have an equal number of bands.",
          "datatype": "Raster Layer; Constant"
        },
        {
          "name": "statistics_type(Optional)",
          "explanation": "Specifies the statistic type to be calculated.MEAN—The mean (average) of the inputs will be calculated. This is the default.MAJORITY—The majority (value that occurs most often) of the inputs will be determined.MAXIMUM—The maximum (largest value) of the inputs will be determined.MEDIAN—The median of the inputs will be calculated.MINIMUM—The minimum (smallest value) of the inputs will be determined.MINORITY—The minority (value that occurs least often) of the inputs will be determined.PERCENTILE—The percentile of the inputs will be calculated. The 90th percentile is calculated by default. You can specify other values (from 0 to 100) using the percentile_value parameter.RANGE—The range (difference between largest and smallest value) of the inputs will be calculated.STD—The standard deviation of the inputs will be calculated.SUM—The sum (total of all values) of the inputs will be calculated.VARIETY—The variety (number of unique values) of the inputs will be calculated.The default statistic type is MEAN.",
          "datatype": "String"
        },
        {
          "name": "ignore_nodata(Optional)",
          "explanation": "Specifies whether NoData values will be ignored by the statistic calculation.DATA—At the processing cell location, if any of the input rasters has NoData, that NoData value will be ignored. The statistics will be computed by only considering the cells with valid data. This is the default.NODATA—If the processing cell location for any of the input rasters is NoData, the output for that cell will be NoData.",
          "datatype": "Boolean"
        },
        {
          "name": "process_as_multiband(Optional)",
          "explanation": "Specifies how the input multiband raster bands will be processed.\r\nSINGLE_BAND—Each band from a multiband raster input will be processed separately as a single band raster. This is the default.MULTI_BAND—Each multiband raster input will be processed as a multiband raster. The operation will be performed for each band from one input using the corresponding band number from the other inputs.",
          "datatype": "Boolean"
        },
        {
          "name": "percentile_value(Optional)",
          "explanation": "The percentile value that will be calculated. The default is 90, indicating the 90th percentile.The value can range from 0 to 100. The 0th percentile is essentially equivalent to the minimum statistic, and the 100th percentile is equivalent to the maximum statistic. A value of 50 will produce essentially the same result as the median statistic.This parameter is only supported if the statistics_type parameter is set to PERCENTILE.",
          "datatype": "Double"
        },
        {
          "name": "percentile_interpolation_type(Optional)",
          "explanation": "Specifies the method of interpolation that will be used when the specified percentile value is between two input cell values.AUTO_DETECT—If the input rasters are of integer pixel type, the NEAREST method will be used. If the input rasters are of floating point pixel type, the LINEAR method will be used. This is the default.NEAREST—The nearest available value to the desired percentile will be used. In this case, the output pixel type will be the same as that of the input rasters.LINEAR—The weighted average of the two surrounding values from the percentile will be used. In this case, the output pixel type will be floating point.",
          "datatype": "String"
        }
      ],
      "summary": "Calculates a per-cell statistic from multiple rasters. The available statistics are Majority, Maximum, Mean, Median, Minimum, Minority, Percentile, Range, Standard deviation, Sum, and Variety. Learn more about how Cell Statistics works",
      "extraction_date": "2025-10-01T15:15:32.215379"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Focal Statistics",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/focal-statistics.htm",
      "parameters": [
        {
          "name": "in_raster",
          "explanation": "The raster for which the focal statistics for each input cell will be calculated.",
          "datatype": "Raster Layer"
        },
        {
          "name": "neighborhood(Optional)",
          "explanation": "The cells surrounding a processing cell that will be used in the statistic calculation. There are several predefined neighborhood types to choose from, or a custom kernel can be defined.Once the neighborhood type is selected, other parameters can be set to fully define the shape, size, and units of measure. The default neighborhood is a square rectangle with a width and height of three cells.The shape of the neighborhoods are defined by the Neighborhood class. The available neighborhood types are NbrAnnulus, NbrCircle, NbrRectangle, NbrWedge, NbrIrregular, and NbrWeight.The following are the forms of the available neighborhood types:NbrAnnulus({innerRadius}, {outerRadius}, {units})A ring or donut-shaped neighborhood defined by an inner radius and an outer radius. The minimum value for radius is 1 cell, and the outer radius must be larger than the inner. The maximum inner radius is 2046 cells, and the maximum outer radius is 2047 cells. The default annulus is an inner radius of 1 cell and an outer radius of 3 cells.NbrCircle({radius}, {units}A circular neighborhood with the given radius. The minimum value for radius is 1 cell, and the maximum value is 2047 cells. The default radius is 3 cells.NbrRectangle({width}, {height}, {units})A rectangular neighborhood defined by width and height. The minimum value for width or height is 1 cell, and the maximum value is 4096 cells. The default is a square with a width and height of 3 cells.NbrWedge({radius}, {startAngle}, {endAngle}, {units})A wedge-shaped neighborhood defined by a radius, a start angle, and an end angle. The minimum value for radius is 1 cell, and the maximum value is 2047 cells. The wedge extends counterclockwise from the starting angle to the ending angle. Angles are specified in degrees, with 0 or 360 representing east. Negative angles can be used. The default wedge is from 0 to 90 degrees, with a radius of 3 cells.NbrIrregular(inKernelFile)A custom neighborhood with specifications set by the identified kernel text file. The minimum value for width or height of the kernel is 1 cell, and the maximum value is 4096 cells.NbrWeight(inKernelFile)A custom neighborhood with specifications set by the identified kernel text file, which can apply weights to the members of the neighborhood. The minimum value for width or height of the kernel is 1 cell, and the maximum value is 4096 cells.For the NbrAnnulus, Nbrcircle, NbrRectangle and NbrWedge neighborhoods, the distance units for the parameters can be specified in CELL units or MAP units. Cell units is the default.For kernel neighborhoods, the first line in the kernel file defines the width and height of the neighborhood in numbers of cells. The subsequent lines indicate how the input value that corresponds to that location in the kernel will be processed. A value of 0 in the kernel file for either the irregular or the weight neighborhood type indicates the corresponding location will not be included in the calculation. For the irregular neighborhood, a value of 1 in the kernel file indicates that the corresponding input cell will be included in the operation. For the weight neighborhood, the value at each position indicates what the corresponding input cell value is to be multiplied by. Positive, negative, and decimal values can be used.",
          "datatype": "Neighborhood"
        },
        {
          "name": "statistics_type(Optional)",
          "explanation": "Specifies the statistic type to be calculated.MEAN—The mean (average value) of the cells in the neighborhood will be calculated.MAJORITY—The majority (value that occurs most often) of the cells in the neighborhood will be identified.MAXIMUM—The maximum (largest value) of the cells in the neighborhood will be identified.MEDIAN—The median of the cells in the neighborhood will be calculated. Median is equivalent to the 50th percentile.MINIMUM—The minimum (smallest value) of the cells in the neighborhood will be identified.MINORITY—The minority (value that occurs least often) of the cells in the neighborhood will be identified.PERCENTILE—A percentile of the cells in the neighborhood will be calculated. The 90th percentile is calculated by default. You can specify other values (from 0 to 100) using the percentile_value parameter.RANGE—The range (difference between largest and smallest value) of the cells in the neighborhood will be calculated.STD—The standard deviation of the cells in the neighborhood will be calculated.SUM—The sum of the cells in the neighborhood will be calculated.VARIETY—The variety (the number of unique values) of the cells in the neighborhood will be calculated.The default statistic type is MEAN.If the input raster is integer, all the statistics types will be available. If the input raster is floating point, only the MEAN, MAXIMUM, MEDIAN, MINIMUM, PERCENTILE, RANGE, STD, and SUM statistic types will be available.",
          "datatype": "String"
        },
        {
          "name": "ignore_nodata(Optional)",
          "explanation": "Specifies whether NoData values will be ignored by the statistic calculation.DATA—If a NoData value exists within a neighborhood, the NoData value will be ignored. Only cells within the neighborhood that have data values will be used in determining the output value. If the processing cell itself is NoData, the processing cell may receive a value in the output raster, provided at least one cell within the neighborhood has a valid value. This is the default.NODATA—If any cell in a neighborhood has a value of NoData, including the processing cell, the output for the processing cell will be NoData. The presence of a NoData value implies that there is insufficient information to determine the statistic value for the neighborhood.",
          "datatype": "Boolean"
        },
        {
          "name": "percentile_value(Optional)",
          "explanation": "The percentile value that will be calculated. The default is 90, for the 90th percentile.The value can range from 0 to 100. The 0th percentile is essentially equivalent to the minimum statistic, and the 100th percentile is equivalent to the maximum statistic. A value of 50 will produce essentially the same result as the median statistic.This option is only supported if the statistics_type parameter is set to PERCENTILE. If any other statistic type is specified, this parameter will be ignored.",
          "datatype": "Double"
        }
      ],
      "summary": "Calculates for each input cell location a statistic of the values within a specified neighborhood around it. Learn more about how Focal Statistics works",
      "extraction_date": "2025-10-01T15:15:34.799237"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Zonal Statistics",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/zonal-statistics.htm",
      "parameters": [
        {
          "name": "in_zone_data",
          "explanation": "The dataset that defines the zones.The zones can be defined by an integer raster or a feature layer.",
          "datatype": "Raster Layer; Feature Layer"
        },
        {
          "name": "zone_field",
          "explanation": "The field that contains the values that define each zone.It can be an integer or a string field of the zone dataset.",
          "datatype": "Field"
        },
        {
          "name": "in_value_raster",
          "explanation": "The raster that contains the values for which a statistic will be calculated.",
          "datatype": "Raster Layer"
        },
        {
          "name": "statistics_type(Optional)",
          "explanation": "Specifies the statistic type to be calculated.MEAN—The average of all cells in the value raster that belong to the same zone as the output cell will be calculated.This is the default.MAJORITY—The value that occurs most often for all cells in the value raster that belong to the same zone as the output cell will be calculated.MAJORITY_COUNT—The frequency of all cells that contain the majority value in the value raster that belong to the same zone as the output cell will be calculated.MAJORITY_PERCENT—The percentage of cells that contain the majority value in the value raster that belong to the same zone as the output cell will be calculated.MAXIMUM—The largest value of all cells in the value raster that belong to the same zone as the output cell will be calculated.MEDIAN—The median value of all cells in the value raster that belong to the same zone as the output cell will be calculated.MINIMUM—The smallest value of all cells in the value raster that belong to the same zone as the output cell will be calculated.MINORITY—The value that occurs least often for all cells in the value raster that belong to the same zone as the output cell will be calculated.MINORITY_COUNT—The frequency of all cells that contain the minority value in the value raster that belong to the same zone as the output cell will be calculated.MINORITY_PERCENT—The percentage of cells that contain the minority value in the value raster that belong to the same zone as the output cell will be calculated.PERCENTILE—The percentile of all cells in the value raster that belong to the same zone as the output cell will be calculated. The 90th percentile is calculated by default. You can specify other values (from 0 to 100) using the Percentile Value parameter.RANGE—The difference between the largest and smallest value of all cells in the value raster that belong to the same zone as the output cell will be calculated.STD—The standard deviation of all cells in the value raster that belong to the same zone as the output cell will be calculated.SUM—The total value of all cells in the value raster that belong to the same zone as the output cell will be calculated.VARIETY—The number of unique values for all cells in the value raster that belong to the same zone as the output cell will be calculated.",
          "datatype": "String"
        },
        {
          "name": "ignore_nodata(Optional)",
          "explanation": "Specifies whether NoData values in the value input will be ignored in the results of the zone that they fall within.DATA—Within any particular zone, only cells that have a value in the input value raster will be used in determining the output value for that zone. NoData cells in the value raster will be ignored in the statistic calculation. This is the default.NODATA—Within any particular zone, if NoData cells exist in the value raster, they will not be ignored and their existence indicates that there is insufficient information to perform statistical calculations for all the cells in that zone. Consequently, the entire zone will receive the NoData value.",
          "datatype": "Boolean"
        },
        {
          "name": "process_as_multidimensional(Optional)",
          "explanation": "Specifies how the input rasters will be calculated if they are multidimensional.CURRENT_SLICE—Statistics will be calculated from the current slice of the input multidimensional dataset. This is the default.ALL_SLICES—Statistics will be calculated for all dimensions of the input multidimensional dataset.",
          "datatype": "Boolean"
        },
        {
          "name": "percentile_value(Optional)",
          "explanation": "The percentile that will be calculated. The default is 90, indicating the 90th percentile.The values can range from 0 to 100. The 0th percentile is essentially equivalent to the minimum statistic, and the 100th percentile is equivalent to maximum. A value of 50 will produce essentially the same result as the median statistic.\r\nThis parameter is only supported if the statistics_type parameter is set to PERCENTILE.",
          "datatype": "Double"
        },
        {
          "name": "percentile_interpolation_type(Optional)",
          "explanation": "Specifies the method of interpolation that will be used when the percentile value falls between two cell values from the input value raster.AUTO_DETECT—If the input value raster is of integer pixel type, the NEAREST method will be used. If the input value raster is of floating point pixel type, the LINEAR method will be used. This is the default.NEAREST—The nearest available value to the desired percentile is used. In this case, the output pixel type is the same as that of the input value raster.LINEAR—The weighted average of the two surrounding values from the desired percentile is used. In this case, the output pixel type is floating point.",
          "datatype": "String"
        },
        {
          "name": "circular_calculation(Optional)",
          "explanation": "Specifies how the input raster will be processed for circular data.ARITHMETIC—Ordinary linear statistics will be calculated. This is the default. CIRCULAR—The statistics for angles or other cyclic quantities, such as compass direction in degrees, daytimes, and fractional parts of real numbers, will be calculated.",
          "datatype": "Boolean"
        },
        {
          "name": "circular_wrap_value(Optional)",
          "explanation": "The value that will be used to round a linear value to the range of a given circular statistic. Its value must be a positive integer or a floating-point value. The default value is 360 degrees.This parameter is only supported if the circular_calculation parameter is set to CIRCULAR.",
          "datatype": "Double"
        }
      ],
      "summary": "Summarizes the values of a raster within the zones of another dataset. Learn more about how the zonal statistics tools work",
      "extraction_date": "2025-10-01T15:15:37.584195"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Zonal Statistics as Table",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/zonal-statistics-as-table.htm",
      "parameters": [
        {
          "name": "in_zone_data",
          "explanation": "The dataset that defines the zones.The zones can be defined by an integer raster or a feature layer.",
          "datatype": "Raster Layer; Feature Layer"
        },
        {
          "name": "zone_field",
          "explanation": "The field that contains the values that define each zone.It can be an integer or a string field of the zone dataset.",
          "datatype": "Field"
        },
        {
          "name": "in_value_raster",
          "explanation": "The raster that contains the values for which a statistic will be calculated.",
          "datatype": "Raster Layer"
        },
        {
          "name": "out_table",
          "explanation": "The output table that will contain the summary of the values in each zone.The format of the table is determined by the output location and path. By default, the output will be a geodatabase table if in a geodatabase workspace, and a dBASE table if in a file workspace.",
          "datatype": "Table"
        },
        {
          "name": "ignore_nodata(Optional)",
          "explanation": "Specifies whether NoData values in the value input will be ignored in the results of the zone that they fall within.DATA—Within any particular zone, only cells that have a value in the input value raster will be used in determining the output value for that zone. NoData cells in the value raster will be ignored in the statistic calculation. This is the default.NODATA—Within any particular zone, if NoData cells exist in the value raster, they will not be ignored and their existence indicates that there is insufficient information to perform statistical calculations for all the cells in that zone. Consequently, the entire zone will receive the NoData value.",
          "datatype": "Boolean"
        },
        {
          "name": "statistics_type(Optional)",
          "explanation": "Specifies the statistic type to be calculated.ALL—All of the statistics will be calculated. This is the default.MEAN—The average of all cells in the value raster that belong to the same zone as the output cell will be calculated.MAJORITY—The value that occurs most often for all cells in the value raster that belong to the same zone as the output cell will be calculated.MAJORITY_COUNT—The frequency of all cells that contain the majority value in the value raster that belong to the same zone as the output cell will be calculated.MAJORITY_PERCENT—The percentage of cells that contain the majority value in the value raster that belong to the same zone as the output cell will be calculated.MAXIMUM—The largest value of all cells in the value raster that belong to the same zone as the output cell will be calculated.MEDIAN—The median value of all cells in the value raster that belong to the same zone as the output cell will be calculated.MINIMUM—The smallest value of all cells in the value raster that belong to the same zone as the output cell will be calculated.MINORITY—The value that occurs least often for all cells in the value raster that belong to the same zone as the output cell will be calculated.MINORITY_COUNT—The frequency of all cells that contain the minority value in the value raster that belong to the same zone as the output cell will be calculated.MINORITY_PERCENT—The percentage of cells that contain the minority value in the value raster that belong to the same zone as the output cell will be calculated.PERCENTILE—The percentile of all cells in the value raster that belong to the same zone as the output cell will be calculated. The 90th percentile is calculated by default. You can specify other values (from 0 to 100) using the percentile_values parameter.RANGE—The difference between the largest and smallest value of all cells in the value raster that belong to the same zone as the output cell will be calculated.STD—The standard deviation of all cells in the value raster that belong to the same zone as the output cell will be calculated.SUM—The total value of all cells in the value raster that belong to the same zone as the output cell will be calculated.VARIETY—The number of unique values for all cells in the value raster that belong to the same zone as the output cell will be calculated.MIN_MAX—Both the minimum and maximum statistics will be calculated.MEAN_STD—Both the mean and standard deviation statistics will be calculated.MIN_MAX_MEAN—The minimum, maximum, and mean statistics will be calculated.MAJORITY_VALUE_COUNT_PERCENT—The majority value, count, and percentage statistics will be calculated.MINORITY_VALUE_COUNT_PERCENT—The minority value, count, and percentage statistics will be calculated.",
          "datatype": "String"
        },
        {
          "name": "process_as_multidimensional(Optional)",
          "explanation": "Specifies how the input rasters will be calculated if they are multidimensional.\r\nCURRENT_SLICE—Statistics will be calculated from the current slice of the input multidimensional dataset. This is the default.ALL_SLICES—Statistics will be calculated for all dimensions of the input multidimensional dataset.",
          "datatype": "Boolean"
        },
        {
          "name": "percentile_values[percentile_values,...](Optional)",
          "explanation": "The percentile that will be calculated. The default is 90, indicating the 90th percentile.The values can range from 0 to 100. The 0th percentile is essentially equivalent to the minimum statistic, and the 100th percentile is equivalent to maximum. A value of 50 will produce essentially the same result as the median statistic.This parameter is only supported if the statistics_type parameter is set to PERCENTILE or ALL.",
          "datatype": "Double"
        },
        {
          "name": "percentile_interpolation_type(Optional)",
          "explanation": "Specifies the method of interpolation that will be used when the percentile value falls between two cell values from the input value raster.AUTO_DETECT—If the input value raster is of integer pixel type, the NEAREST method will be used. If the input value raster is of floating point pixel type, the LINEAR method will be used. This is the default.NEAREST—The nearest available value to the desired percentile is used.LINEAR—The weighted average of the two surrounding values from the desired percentile is used.",
          "datatype": "String"
        },
        {
          "name": "circular_calculation(Optional)",
          "explanation": "Specifies how the input raster will be processed for circular data.ARITHMETIC—Ordinary linear statistics will be calculated. This is the default. CIRCULAR—The statistics for angles or other cyclic quantities, such as compass direction in degrees, daytimes, and fractional parts of real numbers, will be calculated.",
          "datatype": "Boolean"
        },
        {
          "name": "circular_wrap_value(Optional)",
          "explanation": "The value that will be used to round a linear value to the range of a given circular statistic. Its value must be a positive integer or a floating-point value. The default value is 360 degrees.This parameter is only supported if the circular_calculation parameter is set to CIRCULAR.",
          "datatype": "Double"
        },
        {
          "name": "out_join_layer(Optional)",
          "explanation": "The output layer that will be created by joining the output table to the input zone data.",
          "datatype": "Raster Layer; Feature Layer"
        }
      ],
      "summary": "Summarizes the values of a raster within the zones of another dataset and reports the results as a table. Learn more about how the zonal statistics tools work",
      "extraction_date": "2025-10-01T15:15:40.377441"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Create Color Composite",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/create-color-composite.htm",
      "parameters": [
        {
          "name": "in_raster",
          "explanation": "The input multiband raster data.",
          "datatype": "Raster Dataset; Raster Layer"
        },
        {
          "name": "out_raster",
          "explanation": "The output three-band composite raster.",
          "datatype": "Raster Dataset"
        },
        {
          "name": "method",
          "explanation": "Specifies the\r\nmethod that will be used to extract bands.BAND_NAMES—The band name representing the wavelength\r\ninterval on the electromagnetic spectrum (such as Red, Near Infrared,\r\nor Thermal Infrared) or the polarization (such as VH, VV, HH,\r\nor HV) will be used. This is the default.BAND_IDS— The band number (such as B1, B2, or B3) will be used.",
          "datatype": "String"
        },
        {
          "name": "red_expression",
          "explanation": "The calculation \r\nthat will be assigned to the first band.A band name, band ID, or an algebraic expression using the bands.The supported operators are unary: plus (+), minus (-), times (*), and divide (/).",
          "datatype": "String"
        },
        {
          "name": "green_expression",
          "explanation": "The calculation \r\nthat will be assigned to the second band.A band name, band ID, or an algebraic expression using the bands.The supported operators are unary: plus (+), minus (-), times (*), and divide (/).",
          "datatype": "String"
        },
        {
          "name": "blue_expression",
          "explanation": "The calculation \r\nthat will be assigned to the third band.\r\nA band name, band ID, or an algebraic expression using the bands.The supported operators are unary: plus (+), minus (-), times (*), and divide (/).",
          "datatype": "String"
        }
      ],
      "summary": "Creates a three-band raster dataset from a multiband raster dataset.",
      "extraction_date": "2025-10-01T15:15:45.149806"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Apply Coregistration",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/apply-coregistration.htm",
      "parameters": [
        {
          "name": "in_reference_radar_data",
          "explanation": "The input reference complex radar data.",
          "datatype": "Raster Dataset; Raster Layer"
        },
        {
          "name": "in_secondary_radar_data",
          "explanation": "The input secondary complex radar data.",
          "datatype": "Raster Dataset; Raster Layer"
        },
        {
          "name": "out_secondary_radar_data",
          "explanation": "The output secondary radar data coregistered to the reference radar data.",
          "datatype": "Raster Dataset"
        },
        {
          "name": "in_dem_raster",
          "explanation": "The DEM raster that will be used to estimate the local illuminated area.",
          "datatype": "Mosaic Layer; Raster Layer"
        },
        {
          "name": "geoid(Optional)",
          "explanation": "Specifies whether the vertical reference system of the input DEM will be transformed to ellipsoidal height. Most elevation datasets are referenced to sea level orthometric height, so a correction is required in these cases to convert to ellipsoidal height.GEOID—A geoid correction will be made to convert orthometric height to ellipsoidal height (based on EGM96 geoid).  This is the default.NONE—No geoid correction will be made. Use this option only if the DEM is provided in ellipsoidal height.",
          "datatype": "Boolean"
        },
        {
          "name": "polarization_bands[polarization_bands,...](Optional)",
          "explanation": "The polarization bands that will be corrected.  The first band is  selected by default.",
          "datatype": "String"
        }
      ],
      "summary": "Resamples  the secondary single look complex (SLC) data to the reference SLC grid using a digital elevation model (DEM) and orbit state vector metadata. For Terrain Observation by Progressive Scan (TOPS) mode radar data, the tool also deramps and demodulates the secondary SLC before resampling. Once resampling is performed, the secondary radar data is reramped and remodulated.",
      "extraction_date": "2025-10-01T15:15:47.541064"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Apply Geometric Terrain Correction",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/apply-geometric-terrain-correction.htm",
      "parameters": [
        {
          "name": "in_radar_data",
          "explanation": "The input radar data.",
          "datatype": "Raster Dataset; Raster Layer"
        },
        {
          "name": "out_radar_data",
          "explanation": "The corrected geometric terrain radar data.",
          "datatype": "Raster Dataset"
        },
        {
          "name": "polarization_bands[polarization_bands,...](Optional)",
          "explanation": "The polarization bands that will be corrected.  The first band is  selected by default.",
          "datatype": "String"
        },
        {
          "name": "in_dem_raster(Optional)",
          "explanation": "The input DEM.If no DEM is specified or in areas that are not covered by a specified DEM, an approximated DEM, interpolated from metadata tie points, will be created. Use the tie-point approach for full ocean radar scenes only; specify a DEM whenever land features are included in the radar scene.",
          "datatype": "Raster Dataset; Raster Layer; Mosaic Layer"
        },
        {
          "name": "geoid(Optional)",
          "explanation": "Specifies whether the vertical reference system of the input DEM will be transformed to ellipsoidal height. Most elevation datasets are referenced to sea level orthometric height, so a correction is required in these cases to convert to ellipsoidal height.GEOID—A geoid correction will be made to convert orthometric height to ellipsoidal height (based on EGM96 geoid).  This is the default.NONE—No geoid correction will be made. Use this option only if the DEM is provided in ellipsoidal height.",
          "datatype": "Boolean"
        }
      ],
      "summary": "Orthorectifies the input synthetic aperture radar (SAR) data using a range-Doppler backgeocoding algorithm. The range-Doppler backgeocoding approach computes the radar range and azimuth indices for every digital elevation model (DEM) grid point using the orbit state vectors.  If no DEM is provided, the tool uses the tie points included in the metadata to perform the range-Doppler terrain correction.",
      "extraction_date": "2025-10-01T15:15:49.887557"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Apply Orbit Correction",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/apply-orbit-correction.htm",
      "parameters": [
        {
          "name": "in_radar_data",
          "explanation": "The input radar data.",
          "datatype": "Raster Dataset; Raster Layer"
        },
        {
          "name": "in_orbit_file",
          "explanation": "The input orbit file.",
          "datatype": "File"
        },
        {
          "name": "folder(Optional)",
          "explanation": "The alternate folder location that will be searched for the downloaded orbit state vector files. The default folder is the input radar data .SAFE folder.",
          "datatype": "Folder"
        }
      ],
      "summary": "Updates the orbital information in the Sentinel-1 synthetic aperture radar (SAR) data using a more accurate orbit state vector (OSV) file. Orbit files can be downloaded from external sources using the Download Orbit File  tool.",
      "extraction_date": "2025-10-01T15:15:52.346519"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Apply Radiometric Calibration",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/apply-radiometric-calibration.htm",
      "parameters": [
        {
          "name": "in_radar_data",
          "explanation": "The input radar data.",
          "datatype": "Raster Dataset; Raster Layer"
        },
        {
          "name": "out_radar_data",
          "explanation": "The calibrated radar data.",
          "datatype": "Raster Dataset"
        },
        {
          "name": "polarization_bands[polarization_bands,...](Optional)",
          "explanation": "The polarization bands that will be corrected.  The first band is  selected by default.",
          "datatype": "String"
        },
        {
          "name": "calibration_type(Optional)",
          "explanation": "Specifies the type of calibration that will be applied.BETA_NOUGHT—The radar reflectivity will be calibrated to backscatter for a unit area on the slant range. This is the default.SIGMA_NOUGHT— The backscatter returned will be calibrated to the antenna from a unit area on the ground with the plane locally tangent to the ellipsoid. This is known as the radar cross section.  Sigma nought values vary due to incidence angle, wavelength, polarization, terrain, and surface scattering properties.GAMMA_NOUGHT—The backscatter returned will be calibrated to the antenna from a unit area aligned with the plane perpendicular to the slant range. This normalizes gamma nought using the incidence angle relative to the ellipsoid. Gamma nought values vary due to wavelength, polarization, terrain, and surface scattering properties.",
          "datatype": "String"
        }
      ],
      "summary": "Converts the input synthetic aperture radar (SAR) reflectivity into physical units of normalized backscatter by normalizing the reflectivity using a reference plane. Calibrating SAR data is necessary to obtain meaningful backscatter that can be related to the physical properties of features in the image.",
      "extraction_date": "2025-10-01T15:15:54.830094"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Apply Radiometric Terrain Flattening",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/apply-radiometric-terrain-flattening.htm",
      "parameters": [
        {
          "name": "in_radar_data",
          "explanation": "The input radar data.The data that will have radiometric terrain flattening applied. The data must be radiometrically calibrated to beta nought.",
          "datatype": "Raster Dataset; Raster Layer"
        },
        {
          "name": "out_radar_data",
          "explanation": "The radiometrically terrain-flattened radar data.",
          "datatype": "Raster Dataset"
        },
        {
          "name": "in_dem_raster",
          "explanation": "The input DEM.The DEM that will be used  to estimate the local illuminated area and the local incidence angle.",
          "datatype": "Mosaic Layer; Raster Layer"
        },
        {
          "name": "geoid(Optional)",
          "explanation": "Specifies whether the vertical reference system of the input DEM will be transformed to ellipsoidal height. Most elevation datasets are referenced to sea level orthometric height, so a correction is required in these cases to convert to ellipsoidal height.GEOID—A geoid correction will be made to convert orthometric height to ellipsoidal height (based on EGM96 geoid).  This is the default.NONE—No geoid correction will be made. Use this option only if the DEM is provided in ellipsoidal height.",
          "datatype": "Boolean"
        },
        {
          "name": "polarization_bands[polarization_bands,...](Optional)",
          "explanation": "The polarization bands that will be radiometrically terrain flattened. The first band is  selected by default.",
          "datatype": "String"
        },
        {
          "name": "calibration_type(Optional)",
          "explanation": "Specifies whether the output will be terrain flattened using sigma nought or gamma nought.GAMMA_NOUGHT—  The beta nought backscatter will be corrected using an accurate computation of an area using a DEM. This is the default.SIGMA_NOUGHT— The beta nought backscatter will be corrected using the unit area of a plane that is locally tangent to the DEM.",
          "datatype": "String"
        },
        {
          "name": "out_scattering_area(Optional)",
          "explanation": "The scattering area radar\r\ndataset.",
          "datatype": "Raster Dataset"
        },
        {
          "name": "out_geometric_distortion(Optional)",
          "explanation": "The 4-band geometric distortion radar dataset. The first band is the terrain slope, the second band is look angle, the third band is the foreshortening ratio, and the fourth band is the local incidence angle.",
          "datatype": "Raster Dataset"
        },
        {
          "name": "out_geometric_distortion_mask(Optional)",
          "explanation": "The 1-band geometric distortion mask radar dataset. The pixels are classified using six unique values, one for each distortion type:Undetermined —Value of 0Foreshortening —Value of 1Lengthening —Value of 2Shadow —Value of 3Layover —Value of 4Layover and shadow —Value of 5",
          "datatype": "Raster Dataset"
        }
      ],
      "summary": "Corrects the input synthetic aperture radar (SAR) data for radiometric distortions due to topography. Due to the side-looking nature of SAR sensors,\r\nfeatures facing the sensor appear artificially brighter and\r\nfeatures facing away from the sensor appear artificially\r\ndarker. Radiometric terrain flattening normalizes the backscatter\r\nvalues so that value variations will be due to surface scattering\r\nproperties. Radiometric terrain flattening is necessary to\r\nobtain meaningful backscatter that can be\r\nrelated directly to the surface scattering properties of features\r\nin a SAR image over any terrain.",
      "extraction_date": "2025-10-01T15:15:57.563124"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Compute Coherence",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/compute-coherence.htm",
      "parameters": [
        {
          "name": "in_reference_radar_data",
          "explanation": "The input reference complex radar data.",
          "datatype": "Raster Dataset; Raster Layer"
        },
        {
          "name": "in_secondary_radar_data",
          "explanation": "The input secondary complex radar data.",
          "datatype": "Raster Dataset; Raster Layer"
        },
        {
          "name": "out_radar_data",
          "explanation": "The output coherence radar data.",
          "datatype": "Raster Dataset"
        },
        {
          "name": "polarization_bands[polarization_bands,...](Optional)",
          "explanation": "The polarization bands that will be corrected.  The first band is  selected by default.",
          "datatype": "String"
        },
        {
          "name": "range_window_size(Optional)",
          "explanation": "The range window size in pixels. The default value is 10.",
          "datatype": "Long"
        },
        {
          "name": "azimuth_window_size(Optional)",
          "explanation": "The azimuth window size in pixels. The default value is the minimum number of pixels required to create an approximate square window. For example, if the range_window_size parameter value is 10, the default will be 3.",
          "datatype": "Long"
        }
      ],
      "summary": "Computes the similarity between the reference and secondary input complex radar data. The output is a coherence raster with a value range of 0 to 1 in which 0 indicates no coherence and 1 indicates perfect coherence. A value of 0.3 or above is considered a good coherence value.",
      "extraction_date": "2025-10-01T15:16:00.090125"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Convert SAR Units",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/convert-sar-units.htm",
      "parameters": [
        {
          "name": "in_radar_data",
          "explanation": "The input radar data.",
          "datatype": "Raster Dataset; Raster Layer"
        },
        {
          "name": "out_radar_data",
          "explanation": "The converted radar dataset.",
          "datatype": "Raster Dataset"
        },
        {
          "name": "conversion_type(Optional)",
          "explanation": "Specifies the type of backscatter conversion that will\r\nbe applied.LINEAR_TO_DB—The unitless values will be converted to dB values. This is the default.DB_TO_LINEAR—The dB values will be converted to unitless values.AMPLITUDE_TO_INTENSITY—The amplitude values  will be converted to intensity values by squaring the amplitude.INTENSITY_TO_AMPLITUDE—The intensity values  will be converted to amplitude values by applying the square root to the intensity.COMPLEX_TO_INTENSITY— The complex values will be converted to intensity values by adding the square of the real and imaginary components.",
          "datatype": "String"
        }
      ],
      "summary": "Converts the scaling of the input synthetic aperture radar (SAR) data. Conversions can be performed  between amplitude and intensity, between linear and decibels (dB), and between complex and intensity.",
      "extraction_date": "2025-10-01T15:16:02.441502"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Deburst",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/deburst.htm",
      "parameters": [
        {
          "name": "in_radar_data",
          "explanation": "The input radar data.",
          "datatype": "Raster Dataset; Raster Layer"
        },
        {
          "name": "out_radar_data",
          "explanation": "The debursted radar data.",
          "datatype": "Raster Dataset"
        },
        {
          "name": "polarization_bands[polarization_bands,...](Optional)",
          "explanation": "The polarization bands that will be corrected.  The first band is  selected by default.",
          "datatype": "String"
        }
      ],
      "summary": "Merges the multiple bursts from the input Sentinel-1 Single Look Complex (SLC) synthetic aperture radar (SAR) data and outputs a single, seamless subswath raster.",
      "extraction_date": "2025-10-01T15:16:04.834326"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Despeckle",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/despeckle.htm",
      "parameters": [
        {
          "name": "in_radar_data",
          "explanation": "The input radar data.",
          "datatype": "Raster Dataset; Raster Layer"
        },
        {
          "name": "out_radar_data",
          "explanation": "The despeckled radar data.",
          "datatype": "Raster Dataset"
        },
        {
          "name": "polarization_bands[polarization_bands,...](Optional)",
          "explanation": "The polarization bands that will be filtered. The first band is  selected by default.",
          "datatype": "String"
        },
        {
          "name": "filter_type(Optional)",
          "explanation": "Specifies the type of smoothing algorithm or filter that will be applied.LEE—A spatial filter will be applied to each pixel in an image to reduce the speckle noise. This option filters the data based on local statistics calculated within a square window. This filter is useful for smoothing\r\nspeckled data that has an additive or multiplicative\r\ncomponent.ENHANCED_LEE—A spatial filter that preserves the sharpness and detail of the image will be applied to reduce the speckle noise. This option is an enhanced version of the Lee filter. This\r\nfilter is useful for reducing speckle while preserving\r\ntexture information.REFINED_LEE—A spatial filter will be applied to selected pixels, based on local statistics, to reduce the speckle noise. This filter uses a nonsquare filter window to match the direction of edges. It is useful for reducing speckle while preserving edges. This is the default.FROST—An exponentially damped circularly symmetric filter that uses local statistics within individual filter windows will be applied to reduce the speckle noise. This does not affect image features at the edges. This filter is useful for reducing speckle while preserving edges.KUAN—A spatial filter will be applied to each pixel in an image to reduce the speckle noise.  This filters the data based on local statistics of the centered pixel value that is calculated using the neighboring pixels. This filter is useful for reducing speckle while preserving edges.GAMMA_MAP—A Bayesian analysis\r\nand Gamma distribution filter will be applied to reduce the speckle noise. This filter is useful for reducing speckle while preserving\r\nedges.",
          "datatype": "String"
        },
        {
          "name": "filter_size(Optional)",
          "explanation": "Specifies the size of the pixel window\r\nthat will be used to filter noise.3x3—A 3-by-3 filter  size will be used. This is the default.5x5—A 5-by-5 filter  size will be used.7x7—A 7-by-7 filter  size will be used.9x9—A 9-by-9 filter  size will be used.11x11—An 11-by-11 filter  size will be used.This parameter is only valid when the filter_type parameter is set to LEE, ENHANCED_LEE, FROST, KUAN, or GAMMA_MAP.",
          "datatype": "String"
        },
        {
          "name": "noise_model(Optional)",
          "explanation": "This parameter is only valid when the filter_type parameter is set to LEE.",
          "datatype": "String"
        },
        {
          "name": "noise_variance(Optional)",
          "explanation": "The noise variance of the radar image. The default is 0.25.\r\nThis parameter is only valid when the filter_type parameter is set to LEE and the noise_model parameter is set to ADDITIVE_NOISE or ADDITIVE_AND_MULTIPLICATIVE_NOISE.",
          "datatype": "Double"
        },
        {
          "name": "add_noise_mean(Optional)",
          "explanation": "The mean value of additive noise. A larger noise mean value will produce less smoothing, while a smaller value results in more smoothing.\r\nThe default value is 0.This parameter is only valid when the filter_type parameter is set to LEE and the noise_model parameter is set to ADDITIVE_NOISE or ADDITIVE_AND_MULTIPLICATIVE_NOISE.",
          "datatype": "Double"
        },
        {
          "name": "mult_noise_mean(Optional)",
          "explanation": "The mean value of multiplicative noise. A larger noise mean value will produce less smoothing, while a smaller value results in more smoothing.\r\nThe default value is 1.This parameter is only valid when the filter_type parameter is set to LEE and the noise_model parameter is set to MULTIPLICATIVE_NOISE or ADDITIVE_AND_MULTIPLICATIVE_NOISE.",
          "datatype": "Double"
        },
        {
          "name": "number_of_looks(Optional)",
          "explanation": "The number of looks value of the image, which controls image smoothing and estimates noise variance. A smaller value results in more smoothing, while a larger value retains more image features. The default value is 1.This parameter is only valid when the filter_type parameter is set to ENHANCED_LEE, KUAN, or GAMMA_MAP or when the filter_type parameter is set to LEE and the noise_model parameter is set to MULTIPLICATIVE_NOISE.",
          "datatype": "Long"
        },
        {
          "name": "damp_factor(Optional)",
          "explanation": "The exponential damping level of smoothing that will be applied. A damping value greater than 1 will result in better edge preservation but less smoothing. Values less than 1 will result in more smoothing. A value of 0 will produce results similar to a low-pass filter. The default is 1.",
          "datatype": "Long"
        }
      ],
      "summary": "Corrects the input synthetic aperture radar (SAR) data for speckle,  which is a result of coherent illumination that resembles a grainy or salt and pepper effect. This tool filters out noise while retaining edges and sharp features in the SAR image. The available filters are Lee, Enhanced Lee, Refined Lee, Frost, Kuan, and Gamma MAP.",
      "extraction_date": "2025-10-01T15:16:07.525525"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Download Orbit File",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/download-orbit-file.htm",
      "parameters": [
        {
          "name": "in_radar_data",
          "explanation": "The input radar data.",
          "datatype": "Raster Dataset; Raster Layer"
        },
        {
          "name": "orbit_type(Optional)",
          "explanation": "Specifies the OSV type that will be downloaded. SENTINEL_RESTITUTED—Approximate OSV data will be downloaded. This is available several hours\r\nafter data acquisition.SENTINEL_PRECISE—Refined OSV\r\ndata will be downloaded. This\r\nis available\r\n20 days after data acquisition. This is the default.",
          "datatype": "String"
        },
        {
          "name": "username(Optional)",
          "explanation": "The Copernicus Data Space Ecosystem login credential username.  This parameter is only valid when the input data has Sentinel restituted or Sentinel precise orbit types.",
          "datatype": "String"
        },
        {
          "name": "password(Optional)",
          "explanation": "The Copernicus Data Space Ecosystem login credential password.This parameter is only valid when the input data has Sentinel restituted or Sentinel precise orbit types.",
          "datatype": "Encrypted String"
        },
        {
          "name": "cloud_storage(Optional)",
          "explanation": "The Copernicus Data Space Ecosystem cloud storage connection file.",
          "datatype": "File"
        },
        {
          "name": "folder(Optional)",
          "explanation": "An alternate folder location where the downloaded orbit state vector file will be stored.\r\nThe default folder is the input radar data .SAFE folder.",
          "datatype": "Folder"
        }
      ],
      "summary": "Downloads the updated orbit  files for Sentinel-1 synthetic aperture radar (SAR) data. This tool uses the orbit type to make a call to the orbit website. Using the SAR metadata, it identifies the appropriate orbit state vector (OSV) file and downloads it to the input SAR data directory. Three types of OSVs are available for a Sentinel-1 product: predicted, restituted, and precise. Predicted OSVs are provided with the Sentinel-1 Level 1 ground range detected (GRD)\r\nand single look complex (SLC)\r\n auxiliary products. Restituted OSVs are available through the European Space Agency (ESA) within 3 hours of image acquisition. Precise OSVs are available through ESA within 3 weeks of image acquisition. Sentinel-1 OSV files are downloaded from the Copernicus Data Space Ecosystem.",
      "extraction_date": "2025-10-01T15:16:10.213979"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Generate Radiometric Terrain Corrected Data",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/generate-radiometric-terrain-corrected-data.htm",
      "parameters": [
        {
          "name": "in_radar_data",
          "explanation": "The input radar data.This data needs to be Level 1 radar data.",
          "datatype": "Raster Dataset; Raster Layer"
        },
        {
          "name": "out_radar_data",
          "explanation": "The radiometric terrain-corrected radar dataset.",
          "datatype": "Raster Dataset"
        },
        {
          "name": "in_dem_raster",
          "explanation": "The input DEM.The DEM that will be used  to estimate the local illuminated area and the local incidence angle.",
          "datatype": "Raster Dataset; Raster Layer"
        },
        {
          "name": "geoid(Optional)",
          "explanation": "Specifies whether the vertical reference system of the input DEM will be transformed to ellipsoidal height. Most elevation datasets are referenced to sea level orthometric height, so a correction is required in these cases to convert to ellipsoidal height.GEOID—A geoid correction will be made to convert orthometric height to ellipsoidal height (based on EGM96 geoid).  This is the default.NONE—No geoid correction will be made. Use this option only if the DEM is provided in ellipsoidal height.",
          "datatype": "Boolean"
        },
        {
          "name": "polarization_bands[polarization_bands,...](Optional)",
          "explanation": "The polarization bands that will be filtered. The first band is  selected by default.",
          "datatype": "String"
        },
        {
          "name": "output_units(Optional)",
          "explanation": "Specifies the units that will be used for the RTC outputs.LINEAR—The units of the output will be  in linear backscatter. This is the default.DECIBEL—The units of the output will be  in decibels.",
          "datatype": "String"
        },
        {
          "name": "processing_level[processing_level,...](Optional)",
          "explanation": "Specifies the radar products that will be retained. The available processing levels depend on the input radar data and its corresponding processing workflow.By default, no option will be used, and no radar products will be retained.TNR— The output from the Remove Thermal Noise tool will be retained.CalB0—The output from the  Apply Radiometric Calibration tool will be retained and calibrated to Beta nought.DBST—The output from the Deburst tool will be retained.ML—The output from the Multilook tool will be retained.RTFG0—The output from the Apply Radiometric Terrain Flattening tool will be retained and calibrated to Gamma noughtDSPK—The output from the Despeckle tool will be retained.GTC—The output from the Apply Geometric Terrain Correction tool will be retained.",
          "datatype": "String"
        },
        {
          "name": "out_folder(Optional)",
          "explanation": "The output folder where the output radar products will be retained.",
          "datatype": "Workspace"
        }
      ],
      "summary": "Transforms Level 1 synthetic aperture radar (SAR) data to a radiometric terrain-corrected (RTC) dataset. Use the RTC data for analysis and visualization. This tool creates a SAR output that removes unwanted noise and distortions by applying the appropriate RTC workflow according to the input SAR sensor, mode, and product type.",
      "extraction_date": "2025-10-01T15:16:12.831692"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Multilook",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/multilook.htm",
      "parameters": [
        {
          "name": "in_radar_data",
          "explanation": "The input radar data.",
          "datatype": "Raster Dataset; Raster Layer"
        },
        {
          "name": "out_radar_data",
          "explanation": "The output multilook radar data.",
          "datatype": "Raster Dataset"
        },
        {
          "name": "polarization_bands[polarization_bands,...](Optional)",
          "explanation": "The polarization bands that will be corrected.  The first band is  selected by default.",
          "datatype": "String"
        },
        {
          "name": "range_looks(Optional)",
          "explanation": "The integer number of looks in the range direction. If no value is provided, the minimum number of looks required to create an approximate square pixel will be used.",
          "datatype": "Long"
        },
        {
          "name": "azimuth_looks(Optional)",
          "explanation": "The integer number of looks in the azimuth direction. If no value is provided, the minimum number of looks required to create an approximate square pixel will be used.",
          "datatype": "Long"
        }
      ],
      "summary": "Averages the input synthetic aperture radar (SAR) data  by looks in range and azimuth to approximate square pixels, mitigates speckle, and reduces SAR tool processing time.",
      "extraction_date": "2025-10-01T15:16:15.273789"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Remove Thermal Noise",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/remove-thermal-noise.htm",
      "parameters": [
        {
          "name": "in_radar_data",
          "explanation": "The input radar data.",
          "datatype": "Raster Dataset; Raster Layer"
        },
        {
          "name": "out_radar_data",
          "explanation": "The thermal noise-corrected radar data.",
          "datatype": "Raster Dataset"
        },
        {
          "name": "polarization_bands[polarization_bands,...](Optional)",
          "explanation": "The polarization bands that will be corrected.  The first band is  selected by default.",
          "datatype": "String"
        }
      ],
      "summary": "Corrects backscatter disturbances caused by thermal noise in the input synthetic aperture radar (SAR) data, resulting in a more seamless image.",
      "extraction_date": "2025-10-01T15:16:17.650354"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Compute SAR Indices",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/compute-sar-indices.htm",
      "parameters": [
        {
          "name": "in_radar_data",
          "explanation": "The input radar data.",
          "datatype": "Raster Dataset; Raster Layer"
        },
        {
          "name": "out_raster",
          "explanation": "The output SAR index raster.",
          "datatype": "Raster Dataset"
        },
        {
          "name": "index(Optional)",
          "explanation": "Specifies the SAR index that will be computed.RVI—The Radar Vegetation Index will be used. RVI is the ratio of cross-polarized backscatter to the total backscatter from all polarizations. The values range between 0 and 1. RVI values near 0 indicate barren landscapes, while larger values indicate vegetated landscapes. This is the default.RFDI— The Radar Forest Degradation Index will be used. RFDI  is the normalized difference between co- and cross-polarized backscatter. Lower RFDI values (less than 0.3) indicate a denser forest. Moderate RFDI values (between 0.4 and 0.6) correspond to degraded forests.  Higher RFDI values (greater than 0.6) indicate deforested landscapes. CSI—The Canopy Structure Index will be used. CSI is the normalized difference of co-polarized backscatter (HH, VV). The values range between -1 and +1 in which canopies dominated with vertical structures will have CSI values near -1, while those dominated with horizontal structures will have CSI values near 1. This option is only supported when the input radar data contains HH and VV bands.",
          "datatype": "String"
        },
        {
          "name": "polarization_bands(Optional)",
          "explanation": "Specifies the polarization bands that will be used in the index computation.This parameter is only supported when the in_radar_data parameter value is a quad-polarized SAR dataset and the index parameter value is RVI or RFDI. HH_HV—The horizontal-horizontal and horizontal-vertical bands will be used in the index computation (dual-polarization). This is the default.VV_VH—The vertical-vertical and vertical-horizontal  bands will be used in the index computation (dual-polarization).HH_HV_VH_VV—The horizontal-horizontal, horizontal-vertical, vertical-horizontal, and vertical-vertical bands will be used in the index computation (quad-polarization).",
          "datatype": "String"
        }
      ],
      "summary": "Computes various SAR indices for synthetic aperture radar (SAR) data, such as \r\nRadar Vegetation Index (RVI), Radar Forest Degradation Index (RFDI), and Canopy Structure Index (CSI). The formulas used for these indices depend on the polarizations available in the input radar dataset.",
      "extraction_date": "2025-10-01T15:16:20.119674"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Detect Bright Ocean Objects",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/detect-bright-ocean-objects.htm",
      "parameters": [
        {
          "name": "in_radar_data",
          "explanation": "The input radar data.",
          "datatype": "Raster Dataset; Raster Layer"
        },
        {
          "name": "out_feature_class",
          "explanation": "The output feature class of detected bright ocean objects.",
          "datatype": "Feature Class"
        },
        {
          "name": "out_type(Optional)",
          "explanation": "Specifies the type of boundary that will be used for the output feature class.BOUNDS— The minimum bounding box of the detected object will be used. This is the default.PERIMETER— An outline of the perimeter of the detected object will be used.",
          "datatype": "String"
        },
        {
          "name": "min_object_width(Optional)",
          "explanation": "The minimum width of an object to be detected. The width must be a positive value. The default value is 10 meters.",
          "datatype": "Linear Unit"
        },
        {
          "name": "max_object_width(Optional)",
          "explanation": "The maximum width of an object to be detected. The width must be a positive value. The default value is 100 meters.",
          "datatype": "Linear Unit"
        },
        {
          "name": "min_object_length(Optional)",
          "explanation": "The minimum length of an object to be detected. The length must be a positive value. The default value is 50 meters.",
          "datatype": "Linear Unit"
        },
        {
          "name": "max_object_length(Optional)",
          "explanation": "The maximum length of an object to be detected. The length must be a positive value. The default value is 500 meters.",
          "datatype": "Linear Unit"
        },
        {
          "name": "mask_features(Optional)",
          "explanation": "A land or water polygon feature. This polygon will be used to create a mask.",
          "datatype": "Feature Layer"
        },
        {
          "name": "feature_type(Optional)",
          "explanation": "Specifies the type of polygon the mask_features parameter value represents. This parameter is required if the mask_features  parameter is specified.LAND—The mask input is a land polygon. An inverted mask will be created using this input.  WATER—The mask input is a water polygon. A mask will be created using this input.",
          "datatype": "String"
        },
        {
          "name": "in_dem_raster(Optional)",
          "explanation": "The input DEM.If the input radar data is not orthorectified, this DEM will be used to orthorectify it.If no value is provided for the mask_features parameter, this DEM will also be used to create a land mask.",
          "datatype": "Mosaic Layer; Raster Layer"
        },
        {
          "name": "geoid(Optional)",
          "explanation": "Specifies whether the vertical reference system of the input DEM will be transformed to ellipsoidal height. Most elevation datasets are referenced to sea level orthometric height, so a correction is required in these cases to convert to ellipsoidal height.GEOID—A geoid correction will be made to convert orthometric height to ellipsoidal height (based on EGM96 geoid).  This is the default.NONE—No geoid correction will be made. Use this option only if the DEM is provided in ellipsoidal height.",
          "datatype": "Boolean"
        },
        {
          "name": "mask_tolerance(Optional)",
          "explanation": "The buffer distance surrounding the mask created from either the mask_features  parameter or the in_dem_raster parameter. The distance cannot be negative. The default value is 100 meters.",
          "datatype": "Linear Unit"
        }
      ],
      "summary": "Detects potential bright human-made objects—such as ships, oil rigs, and windmills—while masking out the synthetic aperture radar (SAR) data outside the region of interest. The tool clusters pixels and filters the clusters by the minimum and maximum width and length parameters, and outputs the results to a feature class. The output feature class can be specified as  a bounding box or a perimeter around the polygon for the detected objects.",
      "extraction_date": "2025-10-01T15:16:22.681666"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Detect Dark Ocean Areas",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/detect-dark-ocean-areas.htm",
      "parameters": [
        {
          "name": "in_radar_data",
          "explanation": "The input radar data.",
          "datatype": "Raster Dataset; Raster Layer"
        },
        {
          "name": "out_raster",
          "explanation": "The output binary raster of the detected dark ocean areas. A value of 1 corresponds to a detected dark area.",
          "datatype": "Raster Dataset"
        },
        {
          "name": "min_area(Optional)",
          "explanation": "The minimum area to be detected.The size cannot be negative. The default value is 10000 square meters.",
          "datatype": "Areal Unit"
        },
        {
          "name": "mask_features(Optional)",
          "explanation": "A land or water polygon feature. This polygon will be used to create a mask.",
          "datatype": "Feature Layer"
        },
        {
          "name": "feature_type(Optional)",
          "explanation": "Specifies the type of polygon the mask_features parameter value represents. This parameter is required if the mask_features  parameter is specified.LAND—The mask input is a land polygon. An inverted mask will be created using this input.  WATER—The mask input is a water polygon. A mask will be created using this input.",
          "datatype": "String"
        },
        {
          "name": "in_dem_raster(Optional)",
          "explanation": "The input DEM.If the input radar data is not orthorectified, this DEM will be used to orthorectify it.If no value is provided for the mask_features parameter, this DEM will also be used to create a land mask.",
          "datatype": "Mosaic Layer; Raster Layer"
        },
        {
          "name": "geoid(Optional)",
          "explanation": "Specifies whether the vertical reference system of the input DEM will be transformed to ellipsoidal height. Most elevation datasets are referenced to sea level orthometric height, so a correction is required in these cases to convert to ellipsoidal height.GEOID—A geoid correction will be made to convert orthometric height to ellipsoidal height (based on EGM96 geoid).  This is the default.NONE—No geoid correction will be made. Use this option only if the DEM is provided in ellipsoidal height.",
          "datatype": "Boolean"
        },
        {
          "name": "mask_tolerance(Optional)",
          "explanation": "The buffer distance surrounding the mask created from either the mask_features  parameter or the in_dem_raster parameter. The distance cannot be negative. The default value is 100 meters.",
          "datatype": "Linear Unit"
        }
      ],
      "summary": "Identifies potential dark pixels belonging to oil spills or algae, and clusters these pixels, while masking out the synthetic aperture radar (SAR) data outside the region of interest. The tool filters clusters using the Minimum Area parameter, and creates the result as a binary raster. A value of 1 corresponds to dark areas detected and is symbolized in a random color. A value of 0 indicates that no dark areas were detected and is symbolized with full transparency. Both orthorectified and nonorthorectified radar data are valid inputs. Nonorthorectified radar data results in improved azimuth artifact filtering, since the data is in radar coordinates.",
      "extraction_date": "2025-10-01T15:16:25.237059"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Extract Water",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/extract-water.htm",
      "parameters": [
        {
          "name": "in_radar_data",
          "explanation": "The input radar data.",
          "datatype": "Raster Dataset; Raster Layer"
        },
        {
          "name": "out_feature_class",
          "explanation": "The output polygon feature class that depicts water and land polygons.",
          "datatype": "Feature Class"
        },
        {
          "name": "min_area(Optional)",
          "explanation": "The minimum area to extract as a water body. The default value is 50,000 square meters.",
          "datatype": "Areal Unit"
        },
        {
          "name": "in_dem_raster(Optional)",
          "explanation": "The input DEM.If the input radar data is not orthorectified, this DEM will be used to orthorectify it.This DEM will also be used to optimize the polygon construction.",
          "datatype": "Mosaic Layer; Raster Layer"
        },
        {
          "name": "geoid(Optional)",
          "explanation": "Specifies whether the vertical reference system of the input DEM will be transformed to ellipsoidal height. Most elevation datasets are referenced to sea level orthometric height, so a correction is required in these cases to convert to ellipsoidal height.GEOID—A geoid correction will be made to convert orthometric height to ellipsoidal height (based on EGM96 geoid).  This is the default.NONE—No geoid correction will be made. Use this option only if the DEM is provided in ellipsoidal height.",
          "datatype": "Boolean"
        }
      ],
      "summary": "Finds water bodies using input synthetic aperture radar (SAR) data and a digital elevation model (DEM). The tool uses the input radar backscatter to determine whether pixels should be classified as water; then creates polygons for water areas. The tool will also create polygons for areas that are not water, which will be considered land areas.",
      "extraction_date": "2025-10-01T15:16:27.618370"
    },
    {
      "toolset": "image-analyst",
      "tool_name": "Create Binary Mask",
      "help_url": "https://pro.arcgis.com/en/pro-app/3.5/tool-reference/image-analyst/create-binary-mask.htm",
      "parameters": [
        {
          "name": "in_raster",
          "explanation": "The input raster dataset. \r\nIf the input is  multiband, the first band will be used by default.",
          "datatype": "Mosaic Layer; Raster Layer; Image Service; String; Raster Dataset; Mosaic Dataset"
        },
        {
          "name": "out_raster",
          "explanation": "The output binary raster dataset. Supported formats are TIFF, \r\nCRF, and PNG.",
          "datatype": "Raster Dataset"
        },
        {
          "name": "background_value(Optional)",
          "explanation": "The background value for the output raster. The default value is 0.",
          "datatype": "Double"
        },
        {
          "name": "flood_fill(Optional)",
          "explanation": "Specifies how \r\nbackground pixel values will be determined.FLOOD_FILL—Background pixel values will be determined by the flood fill operation, which fills the connected pixels from the image boundary to the mask boundary. Pixels inside the mask will not be converted to background regardless of their value.NO_FLOOD_FILL—Background pixel values will be determined by the specified background value. This is the default.",
          "datatype": "Boolean"
        },
        {
          "name": "expand_background(Optional)",
          "explanation": "The number of pixels that will be used to expand or shrink the background. Negative values will shrink the background.",
          "datatype": "Long"
        },
        {
          "name": "expand_mask(Optional)",
          "explanation": "The number of pixels that will be used to expand or shrink the mask. Negative values will shrink the mask.",
          "datatype": "Long"
        },
        {
          "name": "min_region_size(Optional)",
          "explanation": "The number of connected pixels that will be used to define a mask region. Mask regions that are smaller than this size will be classified as background.",
          "datatype": "Long"
        },
        {
          "name": "background_nodata(Optional)",
          "explanation": "Specifies whether the background value will be set to NoData.\r\nBACKGROUND_NODATA—The background value will be set to NoData.BACKGROUND_DATA—The background value will not be set to NoData. This is the default.",
          "datatype": "Boolean"
        }
      ],
      "summary": "Converts an input raster dataset to a binary raster. Pixels are labeled as either mask or background based on user-defined values.",
      "extraction_date": "2025-10-01T15:16:31.937124"
    }
  ]
}